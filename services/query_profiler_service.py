from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import os
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Deque, Dict, List, Optional

try:
    # Structured logging events (fail-open)
    from observability import emit_event  # type: ignore
except Exception:  # pragma: no cover
    def emit_event(_event: str, severity: str = "info", **_fields: Any) -> None:  # type: ignore
        return None

logger = logging.getLogger(__name__)


class QueryStage(Enum):
    """×©×œ×‘×™ ×‘×™×¦×•×¢ ×©××™×œ×ª×” ×‘-MongoDB"""

    COLLSCAN = "COLLSCAN"  # ×¡×¨×™×§×ª collection ××œ××”
    IXSCAN = "IXSCAN"  # ×¡×¨×™×§×ª ××™× ×“×§×¡
    FETCH = "FETCH"  # ×©×œ×™×¤×ª ××¡××›×™×
    SORT = "SORT"  # ××™×•×Ÿ
    PROJECTION = "PROJECTION"  # projection
    LIMIT = "LIMIT"  # ×”×’×‘×œ×ª ×ª×•×¦××•×ª
    SKIP = "SKIP"  # ×“×™×œ×•×’ ×¢×œ ×ª×•×¦××•×ª
    SHARD_MERGE = "SHARD_MERGE"  # ××™×–×•×’ shards


class SeverityLevel(Enum):
    """×¨××ª ×—×•××¨×” ×©×œ ×‘×¢×™×™×ª ×‘×™×¦×•×¢×™×"""

    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


@dataclass
class QueryStats:
    """×¡×˜×˜×™×¡×˜×™×§×•×ª ×‘×™×¦×•×¢ ×©×œ ×©××™×œ×ª×”"""

    execution_time_ms: float
    docs_examined: int
    docs_returned: int
    keys_examined: int
    index_used: Optional[str] = None
    is_covered_query: bool = False
    memory_usage_bytes: int = 0

    @property
    def efficiency_ratio(self) -> float:
        """×™×—×¡ ×™×¢×™×œ×•×ª - docs_returned / docs_examined"""
        if self.docs_examined == 0:
            return 1.0
        return self.docs_returned / self.docs_examined


@dataclass
class ExplainStage:
    """×©×œ×‘ ×‘×•×“×“ ×‘-explain plan"""

    stage: QueryStage
    input_stage: Optional["ExplainStage"] = None
    docs_examined: int = 0
    keys_examined: int = 0
    execution_time_ms: float = 0
    index_name: Optional[str] = None
    direction: str = "forward"
    filter_condition: Optional[Dict[str, Any]] = None
    children: List["ExplainStage"] = field(default_factory=list)


@dataclass
class ExplainPlan:
    """×ª×•×›× ×™×ª ×‘×™×¦×•×¢ ××œ××” ×©×œ ×©××™×œ×ª×”"""

    query_id: str
    collection: str
    query_shape: Dict[str, Any]
    winning_plan: ExplainStage
    rejected_plans: List[ExplainStage] = field(default_factory=list)
    stats: Optional[QueryStats] = None
    server_info: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.utcnow)


@dataclass
class OptimizationRecommendation:
    """×”××œ×¦×ª ××•×¤×˜×™××™×–×¦×™×” ×‘×•×“×“×ª"""

    id: str
    title: str
    description: str
    severity: SeverityLevel
    category: str  # "index", "query", "schema", "connection"
    suggested_action: str
    estimated_improvement: str
    code_example: Optional[str] = None
    documentation_link: Optional[str] = None


@dataclass
class SlowQueryRecord:
    """×¨×©×•××ª ×©××™×œ×ª×” ××™×˜×™×ª"""

    query_id: str
    collection: str
    operation: str  # "find", "aggregate", "update", etc.
    query_shape: Dict[str, Any]
    execution_time_ms: float
    timestamp: datetime
    client_info: Optional[Dict[str, Any]] = None
    explain_plan: Optional[ExplainPlan] = None
    recommendations: List[OptimizationRecommendation] = field(default_factory=list)


class AggregationStage(Enum):
    """×©×œ×‘×™ ××’×¨×’×¦×™×” × ×¤×•×¦×™×"""

    COLLSCAN = "COLLSCAN"
    IXSCAN = "IXSCAN"
    FETCH = "FETCH"
    SORT = "SORT"
    MATCH = "$match"
    GROUP = "$group"
    LOOKUP = "$lookup"
    UNWIND = "$unwind"
    PROJECT = "$project"
    LIMIT = "$limit"
    SKIP = "$skip"
    SORT_KEY_GENERATOR = "SORT_KEY_GENERATOR"


@dataclass
class AggregationExplainStage:
    """×©×œ×‘ ×‘××’×¨×’×¦×™×” ×¢× ××™×“×¢ ××¤×•×¨×˜"""

    stage_name: str
    execution_time_ms: float = 0
    docs_examined: int = 0
    n_returned: int = 0

    # ××™×“×¢ ×¡×¤×¦×™×¤×™ ×œ×©×œ×‘
    uses_disk: bool = False  # ×”×× ×”×©×œ×‘ ××©×ª××© ×‘×“×™×¡×§ (×œ××©×œ $sort ×’×“×•×œ)
    memory_usage_bytes: int = 0
    index_used: Optional[str] = None

    # ×¢×‘×•×¨ $lookup
    lookup_collection: Optional[str] = None
    lookup_strategy: Optional[str] = None  # "nestedLoopJoin" vs "indexedLoopJoin"


@dataclass
class AggregationExplainPlan:
    """×ª×•×›× ×™×ª ×‘×™×¦×•×¢ ××œ××” ×œ××’×¨×’×¦×™×”"""

    query_id: str
    collection: str
    pipeline_shape: List[Dict[str, Any]]
    stages: List[AggregationExplainStage]
    total_execution_time_ms: float
    server_info: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.utcnow)


class RateLimiter:
    """×”×’×‘×œ×ª ×§×¦×‘ ×‘×§×©×•×ª ×œ×¤×¨×•×¤×™×™×œ×¨"""

    def __init__(self, requests_per_minute: int = 60):
        self.requests_per_minute = max(1, int(requests_per_minute))
        self._request_counts: Dict[str, List[datetime]] = defaultdict(list)

    def is_allowed(self, client_id: str) -> bool:
        now = datetime.utcnow()
        minute_ago = now - timedelta(minutes=1)

        # × ×™×§×•×™ ×‘×§×©×•×ª ×™×©× ×•×ª
        self._request_counts[client_id] = [t for t in self._request_counts[client_id] if t > minute_ago]

        # ×‘×“×™×§×ª ××’×‘×œ×”
        if len(self._request_counts[client_id]) >= self.requests_per_minute:
            return False

        self._request_counts[client_id].append(now)
        return True


def _env_bool(name: str, default: bool) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    return str(raw).strip().lower() in {"1", "true", "yes", "y", "on"}


def _env_int(name: str, default: int) -> int:
    raw = os.getenv(name)
    if raw is None or str(raw).strip() == "":
        return default
    try:
        return int(float(raw))
    except Exception:
        return default


class QueryProfilerService:
    """
    ×©×™×¨×•×ª ×œ× ×™×ª×•×— ×‘×™×¦×•×¢×™ ×©××™×œ×ª×•×ª MongoDB.

    ××¡×¤×§:
    - ××™×¡×•×£ ×©××™×œ×ª×•×ª ××™×˜×™×•×ª ×‘×–××Ÿ ×××ª
    - ×™×¦×™×¨×ª explain plans
    - × ×™×ª×•×— ×•×”××œ×¦×•×ª ××•×¤×˜×™××™×–×¦×™×”
    """

    # ×¡×£ ×‘×¨×™×¨×ª ××—×“×œ ×œ×©××™×œ×ª×” ××™×˜×™×ª (×‘××™×œ×™×©× ×™×•×ª)
    DEFAULT_SLOW_THRESHOLD_MS = 100

    # ××¡×¤×¨ ××§×¡×™××œ×™ ×©×œ ×©××™×œ×ª×•×ª ××™×˜×™×•×ª ×œ×©××•×¨ ×‘×–×™×›×¨×•×Ÿ
    MAX_SLOW_QUERIES_BUFFER = 1000

    def __init__(self, db_manager: Any, slow_threshold_ms: int = DEFAULT_SLOW_THRESHOLD_MS):
        self.db_manager = db_manager
        self.slow_threshold_ms = int(slow_threshold_ms)

        max_buffer = _env_int("PROFILER_MAX_BUFFER_SIZE", self.MAX_SLOW_QUERIES_BUFFER)
        self._slow_queries: Deque[SlowQueryRecord] = deque(maxlen=max(1, int(max_buffer)))
        self._query_patterns: Dict[str, int] = {}

        # Metrics (best-effort)
        self._metrics_enabled = _env_bool("PROFILER_METRICS_ENABLED", True)
        if self._metrics_enabled:
            try:
                from monitoring.profiler_metrics import (  # type: ignore
                    ACTIVE_PROFILER_BUFFER_SIZE,
                    COLLSCAN_DETECTED,
                    QUERY_DURATION,
                    SLOW_QUERIES_TOTAL,
                )
            except Exception:  # pragma: no cover
                SLOW_QUERIES_TOTAL = None  # type: ignore
                QUERY_DURATION = None  # type: ignore
                ACTIVE_PROFILER_BUFFER_SIZE = None  # type: ignore
                COLLSCAN_DETECTED = None  # type: ignore
            self._SLOW_QUERIES_TOTAL = SLOW_QUERIES_TOTAL
            self._QUERY_DURATION = QUERY_DURATION
            self._ACTIVE_PROFILER_BUFFER_SIZE = ACTIVE_PROFILER_BUFFER_SIZE
            self._COLLSCAN_DETECTED = COLLSCAN_DETECTED
        else:
            self._SLOW_QUERIES_TOTAL = None
            self._QUERY_DURATION = None
            self._ACTIVE_PROFILER_BUFFER_SIZE = None
            self._COLLSCAN_DETECTED = None

    def _generate_query_id(self, collection: str, query_shape: Dict[str, Any]) -> str:
        """×™×¦×™×¨×ª ××–×”×” ×™×™×—×•×“×™ ×œ×©××™×œ×ª×”"""
        content = f"{collection}:{json.dumps(query_shape, sort_keys=True)}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def _is_broken_query_shape(self, query: Any) -> bool:
        """
        ×‘×“×™×§×” ×× query_shape ××›×™×œ × ×¨××•×œ ×©×‘×•×¨ ××’×¨×¡×” ×™×©× ×”.

        ×’×¨×¡××•×ª ×™×©× ×•×ª ×”×™×• ××§×¦×¨×•×ª ××¢×¨×›×™× ×œ-["<N items>"] ×‘××§×•× ×œ×©××•×¨ ×¢×œ ××•×¨×š ×”××¢×¨×š.
        ×–×” ×©×•×‘×¨ ××ª explain() ×›×™ ××•×¤×¨×˜×•×¨×™× ×›××• $eq ××¦×¤×™× ×œ××¡×¤×¨ ××¨×’×•×× ×˜×™× ××“×•×™×§.
        """
        if isinstance(query, dict):
            for v in query.values():
                if self._is_broken_query_shape(v):
                    return True
        elif isinstance(query, list):
            for item in query:
                # ×–×™×”×•×™ ×¤×œ×™×™×¡×”×•×œ×“×¨ ×©×‘×•×¨: "<N items>" (N ×”×•× ××¡×¤×¨)
                if isinstance(item, str) and item.startswith("<") and item.endswith(" items>"):
                    return True
                if self._is_broken_query_shape(item):
                    return True
        return False

    def _normalize_query_shape(self, query: Dict[str, Any]) -> Dict[str, Any]:
        """
        × ×¨××•×œ ×¦×•×¨×ª ×”×©××™×œ×ª×” - ×”×—×œ×¤×ª ×¢×¨×›×™× ×‘×¤×œ×™×™×¡×”×•×œ×“×¨×™×.
        ×××¤×©×¨ ×–×™×”×•×™ ×“×¤×•×¡×™ ×©××™×œ×ª×•×ª ×“×•××™×.

        ğŸ”’ ××‘×˜×—×”: ×¤×•× ×§×¦×™×” ×–×• ××•× ×¢×ª ×“×œ×™×¤×ª ××™×“×¢ ××™×©×™ (PII) ×œ×“×©×‘×•×¨×“/×œ×•×’×™×
        ×¢×œ ×™×“×™ ×”×—×œ×¤×ª ×›×œ ×”×¢×¨×›×™× ×‘×¤×œ×™×™×¡×”×•×œ×“×¨×™×.

        ×—×©×•×‘: ××˜×¤×œ×ª ×’× ×‘××¢×¨×›×™× ××§×•× × ×™× (×œ××©×œ $in, $or)!

        âš ï¸ ××§×¨×™× ××™×•×—×“×™×:
        - $sort: ×¢×¨×›×™ 1/-1 × ×©××¨×™× ×›×¤×™ ×©×”× (×—×™×•× ×™×™× ×œ××‘× ×” ×”-query)
        - ××¢×¨×›×™× ×‘××•×¤×¨×˜×•×¨×™×: ××•×¨×š × ×©××¨, ××™×‘×¨×™× ×× ×•×¨××œ×™× ×¨×§×•×¨×¡×™×‘×™×ª
        """

        # ××¤×ª×—×•×ª ×©×‘×”× ×œ× ×× ×¨××œ×™× ×¢×¨×›×™× ××¡×¤×¨×™×™× (1/-1) â€“ ×§×¨×™×˜×™×™× ×œ×ª×—×‘×™×¨ MongoDB
        SORT_LIKE_KEYS = {"$sort", "$orderby"}

        def normalize_value(value: Any, parent_key: Optional[str] = None) -> Any:
            """
            × ×¨××•×œ ×¢×¨×š ×œ×©× "query shape" ×œ×œ× ×“×œ×™×¤×ª PII.

            âš ï¸ ×—×©×•×‘: ×œ× ××©× ×™× ××‘× ×”/××•×¨×š ×©×œ ××¢×¨×›×™×, ×›×“×™ ×œ× ×œ×©×‘×•×¨ ×‘×™×˜×•×™×™ $expr
            ×•××•×¤×¨×˜×•×¨×™× ×©××¦×¤×™× ×œ××¡×¤×¨ ××¨×’×•×× ×˜×™× ××“×•×™×§ (×œ××©×œ $eq).

            Args:
                value: ×”×¢×¨×š ×œ× ×¨××•×œ
                parent_key: ×”××¤×ª×— ×”×”×•×¨×” (×œ×–×™×”×•×™ ×”×§×©×¨×™× ×›××• $sort)
            """
            if isinstance(value, dict):
                result: Dict[str, Any] = {}
                for k, v in value.items():
                    str_k = str(k)
                    # ×× ×–×” $sort â€“ × ×¢×‘×™×¨ ××ª ×”-key ×›×”×§×©×¨ ×œ×™×œ×“×™×
                    if str_k in SORT_LIKE_KEYS:
                        result[str_k] = normalize_value(v, parent_key=str_k)
                    else:
                        result[str_k] = normalize_value(v, parent_key=parent_key)
                return result

            if isinstance(value, list):
                if not value:
                    return []

                # ×× ×›×œ ×”××™×‘×¨×™× ×¤×¨×™××™×˜×™×‘×™×™× â€“ ××¤×©×¨ ×œ× ×¨××œ ××”×¨ ×‘×œ×™ ×¨×§×•×¨×¡×™×” ×¢××•×§×”,
                # ×ª×•×š ×©××™×¨×” ×¢×œ ××•×¨×š ×”××¢×¨×š.
                if all(isinstance(v, (str, int, float, bool, type(None))) for v in value):
                    out: List[Any] = []
                    for v in value:
                        if v is None:
                            out.append("<null>")
                        elif isinstance(v, str) and v.startswith("$"):
                            # ×”×©××¨×ª field-paths / operator tokens (×œ××©×œ "$user_id", "$eq") ×›×¤×™ ×©×”×
                            out.append(v)
                        elif parent_key in SORT_LIKE_KEYS and isinstance(v, (int, float)) and v in (1, -1, 1.0, -1.0):
                            # ×‘-$sort: ×©×•××¨×™× ×¢×œ 1/-1
                            out.append(int(v))
                        else:
                            out.append("<value>")
                    return out

                # ××¢×¨×š ××•×¨×›×‘ (dicts / lists): × ×¨××•×œ ×¨×§×•×¨×¡×™×‘×™ *×œ×›×œ* ×”××™×‘×¨×™× (×©×•××¨ ××•×¨×š)
                return [normalize_value(v, parent_key=parent_key) for v in value]

            if value is None:
                return "<null>"

            # ×©××™×¨×ª ××—×¨×•×–×•×ª ×©××ª×—×™×œ×•×ª ×‘-$ (field paths / operator markers) â€“ ×–×” ×œ× PII
            if isinstance(value, str):
                return value if value.startswith("$") else "<value>"

            # ×‘-$sort: ×©×•××¨×™× ×¢×œ 1/-1 (×›×™×•×•×Ÿ ××™×•×Ÿ)
            if parent_key in SORT_LIKE_KEYS and isinstance(value, (int, float)) and value in (1, -1, 1.0, -1.0):
                return int(value)

            if isinstance(value, (int, float, bool, datetime, bytes)):
                return "<value>"

            # ObjectId, Decimal128, UUID ×•×›×•' â€” best-effort
            return "<value>"

        # × ×§×•×“×ª ×›× ×™×¡×”: ×›×œ ××¤×ª×— ×‘×¨××” ×”×¢×œ×™×•× ×” ×× ×•×¨××œ ×¢× ×”-key ×©×œ×• ×›×”×§×©×¨
        result: Dict[str, Any] = {}
        for k, v in (query or {}).items():
            str_k = str(k)
            if str_k in SORT_LIKE_KEYS:
                result[str_k] = normalize_value(v, parent_key=str_k)
            else:
                result[str_k] = normalize_value(v, parent_key=None)
        return result

    def record_slow_query_sync(
        self,
        collection: str,
        operation: str,
        query: Dict[str, Any],
        execution_time_ms: float,
        client_info: Optional[Dict[str, Any]] = None,
    ) -> SlowQueryRecord:
        """×¨×™×©×•× ×©××™×œ×ª×” ××™×˜×™×ª (×¡×™× ×›×¨×•× ×™) - × ×•×— ×œ×©×™××•×© ×××–×™×Ÿ PyMongo."""
        query_shape = self._normalize_query_shape(query or {})
        query_id = self._generate_query_id(collection, query_shape)

        record = SlowQueryRecord(
            query_id=query_id,
            collection=collection,
            operation=operation,
            query_shape=query_shape,
            execution_time_ms=float(execution_time_ms),
            timestamp=datetime.utcnow(),
            client_info=client_info,
        )

        self._slow_queries.append(record)

        # ×¢×“×›×•×Ÿ ××•× ×” ×“×¤×•×¡×™ ×©××™×œ×ª×•×ª
        pattern_key = f"{collection}:{operation}:{json.dumps(query_shape, sort_keys=True)}"
        self._query_patterns[pattern_key] = self._query_patterns.get(pattern_key, 0) + 1

        # Metrics (best-effort)
        try:
            if self._SLOW_QUERIES_TOTAL is not None:
                self._SLOW_QUERIES_TOTAL.labels(collection=record.collection, operation=record.operation).inc()
            if self._QUERY_DURATION is not None:
                self._QUERY_DURATION.labels(collection=record.collection, operation=record.operation).observe(
                    float(record.execution_time_ms) / 1000.0
                )
            if self._ACTIVE_PROFILER_BUFFER_SIZE is not None:
                self._ACTIVE_PROFILER_BUFFER_SIZE.set(len(self._slow_queries))
        except Exception:
            pass

        # Observability event (best-effort) â€“ ×œ× ×©×•×œ×— ××ª ×”×©××™×œ×ª×” ×”×’×•×œ××™×ª ××œ× query_shape ×‘×œ×‘×“
        try:
            emit_event(
                "slow_query_detected",
                severity="warning",
                query_id=record.query_id,
                collection=record.collection,
                operation=record.operation,
                execution_time_ms=record.execution_time_ms,
                query_shape=record.query_shape,
            )
        except Exception:
            pass

        logger.warning(
            "Slow query recorded: %s.%s took %.2fms (threshold: %sms)",
            collection,
            operation,
            float(execution_time_ms),
            self.slow_threshold_ms,
        )

        return record

    async def record_slow_query(
        self,
        collection: str,
        operation: str,
        query: Dict[str, Any],
        execution_time_ms: float,
        client_info: Optional[Dict[str, Any]] = None,
    ) -> SlowQueryRecord:
        """
        ×¨×™×©×•× ×©××™×œ×ª×” ××™×˜×™×ª.
        × ×§×¨× ××•×˜×•××˜×™×ª ×¢×œ ×™×“×™ ×”-CommandListener.
        """
        return self.record_slow_query_sync(collection, operation, query, execution_time_ms, client_info)

    async def get_slow_queries(
        self,
        limit: int = 50,
        collection_filter: Optional[str] = None,
        min_execution_time_ms: Optional[float] = None,
        since: Optional[datetime] = None,
    ) -> List[SlowQueryRecord]:
        """×§×‘×œ×ª ×¨×©×™××ª ×©××™×œ×ª×•×ª ××™×˜×™×•×ª ×¢× ××¤×©×¨×•×™×•×ª ×¡×™× ×•×Ÿ."""
        queries = list(self._slow_queries)

        # ×¡×™× ×•×Ÿ ×œ×¤×™ collection
        if collection_filter:
            queries = [q for q in queries if q.collection == collection_filter]

        # ×¡×™× ×•×Ÿ ×œ×¤×™ ×–××Ÿ ×‘×™×¦×•×¢ ××™× ×™××œ×™
        if min_execution_time_ms is not None:
            queries = [q for q in queries if q.execution_time_ms >= float(min_execution_time_ms)]

        # ×¡×™× ×•×Ÿ ×œ×¤×™ ×–××Ÿ
        if since:
            queries = [q for q in queries if q.timestamp >= since]

        # ××™×•×Ÿ ×œ×¤×™ ×–××Ÿ ×‘×™×¦×•×¢ (×”×›×™ ××™×˜×™×•×ª ×§×•×“×)
        queries.sort(key=lambda q: q.execution_time_ms, reverse=True)

        return queries[: max(1, int(limit))]

    async def get_explain_plan(
        self,
        collection: str,
        query: Dict[str, Any],
        verbosity: str = "queryPlanner",  # âš ï¸ ×‘×¨×™×¨×ª ××—×“×œ ×‘×˜×•×—×” - ×œ× ××¨×™×¦×” ××ª ×”×©××™×œ×ª×”!
    ) -> ExplainPlan:
        """
        ×§×‘×œ×ª explain plan ××¤×•×¨×˜ ×œ×©××™×œ×ª×”.

        âš ï¸ ××–×”×¨×”: executionStats ×•-allPlansExecution ××¨×™×¦×™× ××ª ×”×©××™×œ×ª×” ×‘×¤×•×¢×œ!
        """
        # ×‘×“×™×§×” ×× ×”-query ×”×•× query_shape ×©×‘×•×¨ ××’×¨×¡×” ×™×©× ×”
        if self._is_broken_query_shape(query):
            raise ValueError(
                "Query shape contains broken array normalization from old version. "
                "Arrays like '<N items>' cannot be used with explain(). "
                "Please use the original query or re-record this slow query."
            )

        def _run_explain() -> Dict[str, Any]:
            db = getattr(self.db_manager, "db", None)
            if db is None:
                raise RuntimeError("No MongoDB database available")
            coll = db[collection]

            cursor = coll.find(query)
            try:
                # × ×¡×™×•×Ÿ ×œ×”×¨×™×¥ ×¢× ×¨××ª ×”×¤×™×¨×•×˜ ×”××‘×•×§×©×ª
                return cursor.explain(verbosity)
            except TypeError:
                # Fallback ×œ×’×¨×¡××•×ª ×™×©× ×•×ª ×©×œ pymongo ×©×œ× ××§×‘×œ×•×ª ××¨×’×•×× ×˜×™×
                logger.warning(
                    "Profiler: PyMongo Cursor.explain() does not support verbosity=%r; "
                    "falling back to default explain() without execution stats.",
                    verbosity,
                    exc_info=True,
                )
                return cursor.explain()

        explain_result = await asyncio.to_thread(_run_explain)
        return self._parse_explain_result(collection, query, explain_result)

    def _parse_explain_result(self, collection: str, query: Dict[str, Any], explain_result: Dict[str, Any]) -> ExplainPlan:
        """×¤×¨×¡×•×¨ ×ª×•×¦××ª explain ×œ×™×¦×™×¨×ª ExplainPlan"""
        query_planner = explain_result.get("queryPlanner", {}) if isinstance(explain_result, dict) else {}
        execution_stats = explain_result.get("executionStats", {}) if isinstance(explain_result, dict) else {}

        winning_plan_raw = query_planner.get("winningPlan", {}) if isinstance(query_planner, dict) else {}
        winning_plan = self._parse_stage(winning_plan_raw if isinstance(winning_plan_raw, dict) else {})

        rejected_plans: List[ExplainStage] = []
        for plan in (query_planner.get("rejectedPlans", []) if isinstance(query_planner, dict) else []) or []:
            if isinstance(plan, dict):
                rejected_plans.append(self._parse_stage(plan))

        stats = None
        if execution_stats and isinstance(execution_stats, dict):
            stats = QueryStats(
                execution_time_ms=float(execution_stats.get("executionTimeMillis", 0) or 0),
                docs_examined=int(execution_stats.get("totalDocsExamined", 0) or 0),
                docs_returned=int(execution_stats.get("nReturned", 0) or 0),
                keys_examined=int(execution_stats.get("totalKeysExamined", 0) or 0),
                index_used=self._extract_index_name(winning_plan_raw if isinstance(winning_plan_raw, dict) else {}),
                is_covered_query=self._is_covered_query(execution_stats),
            )

        query_shape = self._normalize_query_shape(query)
        query_id = self._generate_query_id(collection, query_shape)

        return ExplainPlan(
            query_id=query_id,
            collection=collection,
            query_shape=query_shape,
            winning_plan=winning_plan,
            rejected_plans=rejected_plans,
            stats=stats,
            server_info=explain_result.get("serverInfo", {}) if isinstance(explain_result, dict) else {},
        )

    def _parse_stage(self, stage_data: Dict[str, Any]) -> ExplainStage:
        """×¤×¨×¡×•×¨ ×©×œ×‘ ×‘×•×“×“ ×‘-explain plan"""
        stage_name = str(stage_data.get("stage", "UNKNOWN") or "UNKNOWN")
        try:
            stage_type = QueryStage(stage_name)
        except ValueError:
            stage_type = QueryStage.FETCH

        input_stage = None
        if "inputStage" in stage_data and isinstance(stage_data.get("inputStage"), dict):
            input_stage = self._parse_stage(stage_data["inputStage"])

        children: List[ExplainStage] = []
        if "inputStages" in stage_data and isinstance(stage_data.get("inputStages"), list):
            for child_stage in stage_data["inputStages"]:
                if isinstance(child_stage, dict):
                    children.append(self._parse_stage(child_stage))

        return ExplainStage(
            stage=stage_type,
            input_stage=input_stage,
            index_name=stage_data.get("indexName"),
            direction=str(stage_data.get("direction", "forward") or "forward"),
            filter_condition=stage_data.get("filter") if isinstance(stage_data.get("filter"), dict) else None,
            children=children,
        )

    def _extract_index_name(self, plan: Dict[str, Any]) -> Optional[str]:
        """×—×™×œ×•×¥ ×©× ×”××™× ×“×§×¡ ××ª×•×›× ×™×ª ×”×‘×™×¦×•×¢"""
        if "indexName" in plan:
            try:
                return str(plan["indexName"])
            except Exception:
                return None
        if "inputStage" in plan and isinstance(plan.get("inputStage"), dict):
            return self._extract_index_name(plan["inputStage"])
        return None

    def _is_covered_query(self, execution_stats: Dict[str, Any]) -> bool:
        """×‘×“×™×§×” ×”×× ×”×©××™×œ×ª×” ×”×™× covered query"""
        docs_examined = int(execution_stats.get("totalDocsExamined", 0) or 0)
        keys_examined = int(execution_stats.get("totalKeysExamined", 0) or 0)
        n_returned = int(execution_stats.get("nReturned", 0) or 0)
        return docs_examined == 0 and keys_examined >= n_returned and n_returned > 0

    async def analyze_and_recommend(self, explain_plan: ExplainPlan) -> List[OptimizationRecommendation]:
        """× ×™×ª×•×— explain plan ×•×™×¦×™×¨×ª ×”××œ×¦×•×ª ××•×¤×˜×™××™×–×¦×™×”."""
        recommendations: List[OptimizationRecommendation] = []

        if self._has_collscan(explain_plan.winning_plan):
            try:
                if self._COLLSCAN_DETECTED is not None:
                    self._COLLSCAN_DETECTED.labels(collection=explain_plan.collection).inc()
            except Exception:
                pass
            recommendations.append(self._create_collscan_recommendation(explain_plan))

        if explain_plan.stats and explain_plan.stats.efficiency_ratio < 0.1:
            recommendations.append(self._create_efficiency_recommendation(explain_plan))

        if self._has_in_memory_sort(explain_plan.winning_plan):
            recommendations.append(self._create_sort_recommendation(explain_plan))

        if explain_plan.stats and not explain_plan.stats.is_covered_query:
            if self._could_be_covered_query(explain_plan):
                recommendations.append(self._create_covered_query_recommendation(explain_plan))

        pattern_count = self._get_pattern_frequency(explain_plan)
        if pattern_count > 10:
            recommendations.append(self._create_frequent_query_recommendation(explain_plan, pattern_count))

        return recommendations

    async def generate_recommendations(self, explain_plan: ExplainPlan) -> List[OptimizationRecommendation]:
        """
        ××œ×’×•×¨×™×ª× ×™×¦×™×¨×ª ×”××œ×¦×•×ª:

        1. × ×™×ª×•×— ×©×œ×‘×™ ×”×‘×™×¦×•×¢ (stages)
        2. ×‘×“×™×§×ª ×™×—×¡×™ ×™×¢×™×œ×•×ª
        3. ×–×™×”×•×™ ×“×¤×•×¡×™× ×‘×¢×™×™×ª×™×™×
        4. ×™×¦×™×¨×ª ×”××œ×¦×•×ª ×¢× ×¢×“×™×¤×•×™×•×ª
        """
        recommendations = await self.analyze_and_recommend(explain_plan)
        severity_order = {SeverityLevel.CRITICAL: 0, SeverityLevel.WARNING: 1, SeverityLevel.INFO: 2}
        return sorted(recommendations, key=lambda r: severity_order.get(r.severity, 999))

    def _has_collscan(self, stage: ExplainStage) -> bool:
        """×‘×“×™×§×” ×”×× ×™×© COLLSCAN ×‘×ª×•×›× ×™×ª"""
        if stage.stage == QueryStage.COLLSCAN:
            return True
        if stage.input_stage and self._has_collscan(stage.input_stage):
            return True
        for child in stage.children:
            if self._has_collscan(child):
                return True
        return False

    def _has_in_memory_sort(self, stage: ExplainStage) -> bool:
        """×‘×“×™×§×” ×”×× ×™×© ××™×•×Ÿ ×‘×–×™×›×¨×•×Ÿ"""
        if stage.stage == QueryStage.SORT:
            return True
        if stage.input_stage:
            return self._has_in_memory_sort(stage.input_stage)
        return False

    def _could_be_covered_query(self, explain_plan: ExplainPlan) -> bool:
        """×‘×“×™×§×” ×”×× ×”×©××™×œ×ª×” ×™×›×•×œ×” ×œ×”×™×•×ª covered query"""
        return explain_plan.stats is not None and explain_plan.stats.index_used is not None

    def _get_pattern_frequency(self, explain_plan: ExplainPlan) -> int:
        """×§×‘×œ×ª ×ª×“×™×¨×•×ª ×“×¤×•×¡ ×”×©××™×œ×ª×”"""
        pattern_key = f"{explain_plan.collection}:find:{json.dumps(explain_plan.query_shape, sort_keys=True)}"
        return int(self._query_patterns.get(pattern_key, 0))

    def _create_collscan_recommendation(self, explain_plan: ExplainPlan) -> OptimizationRecommendation:
        """×™×¦×™×¨×ª ×”××œ×¦×” ×œ×˜×™×¤×•×œ ×‘-COLLSCAN"""
        fields = list((explain_plan.query_shape or {}).keys())
        index_suggestion = ", ".join(f'"{f}": 1' for f in fields[:3])
        return OptimizationRecommendation(
            id=f"collscan_{explain_plan.query_id}",
            title="ğŸ”´ COLLSCAN ×–×•×”×” - × ×“×¨×© ××™× ×“×§×¡",
            description=(
                f"×”×©××™×œ×ª×” ×¢×œ collection '{explain_plan.collection}' ××‘×¦×¢×ª ×¡×¨×™×§×” ××œ××”. "
                f"×–×” ×¢×œ×•×œ ×œ×”×™×•×ª ××™×˜×™ ×××•×“ ×¢×œ collections ×’×“×•×œ×™×."
            ),
            severity=SeverityLevel.CRITICAL,
            category="index",
            suggested_action="×¦×•×¨ ××™× ×“×§×¡ ××ª××™× ×œ×©××™×œ×ª×”",
            estimated_improvement="×™×›×•×œ ×œ×©×¤×¨ ×¤×™ 10-100 ×‘×”×ª×× ×œ×’×•×“×œ ×”-collection",
            code_example=f"""db.{explain_plan.collection}.createIndex({{ {index_suggestion} }})""",
            documentation_link="https://www.mongodb.com/docs/manual/indexes/",
        )

    def _create_efficiency_recommendation(self, explain_plan: ExplainPlan) -> OptimizationRecommendation:
        """×™×¦×™×¨×ª ×”××œ×¦×” ×œ×™×—×¡ ×™×¢×™×œ×•×ª × ××•×š"""
        stats = explain_plan.stats
        assert stats is not None
        return OptimizationRecommendation(
            id=f"efficiency_{explain_plan.query_id}",
            title="ğŸŸ¡ ×™×—×¡ ×™×¢×™×œ×•×ª × ××•×š",
            description=(
                f"×”×©××™×œ×ª×” ×¡×•×¨×§×ª {stats.docs_examined:,} ××¡××›×™× ××š ××—×–×™×¨×” ×¨×§ {stats.docs_returned:,}. "
                f"×™×—×¡ ×™×¢×™×œ×•×ª: {stats.efficiency_ratio:.1%}"
            ),
            severity=SeverityLevel.WARNING,
            category="query",
            suggested_action="×‘×“×•×§ ××ª ×”××™× ×“×§×¡×™× ×”×§×™×™××™× ××• ×¦××¦× ××ª ×”× ×ª×•× ×™× ×”××•×—×–×¨×™×",
            estimated_improvement=f"×¦××¦×•× ×¡×¨×™×§×” ×-{stats.docs_examined:,} ×œ-~{stats.docs_returned:,} ××¡××›×™×",
        )

    def _create_sort_recommendation(self, explain_plan: ExplainPlan) -> OptimizationRecommendation:
        """×™×¦×™×¨×ª ×”××œ×¦×” ×œ××™×•×Ÿ ×‘×–×™×›×¨×•×Ÿ"""
        return OptimizationRecommendation(
            id=f"sort_{explain_plan.query_id}",
            title="ğŸŸ  ××™×•×Ÿ ×‘×–×™×›×¨×•×Ÿ",
            description=(
                "×”×©××™×œ×ª×” ××‘×¦×¢×ª ××™×•×Ÿ ×‘×–×™×›×¨×•×Ÿ ×‘××§×•× ×œ×”×©×ª××© ×‘××™× ×“×§×¡. "
                "×–×” ×¢×œ×•×œ ×œ×”×™×•×ª ××™×˜×™ ×•×œ×¦×¨×•×š ×–×™×›×¨×•×Ÿ ×¨×‘."
            ),
            severity=SeverityLevel.WARNING,
            category="index",
            suggested_action="×¦×•×¨ ××™× ×“×§×¡ ×©×›×•×œ×œ ××ª ×©×“×” ×”××™×•×Ÿ",
            estimated_improvement="×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ ×•×©×™×¤×•×¨ ××”×™×¨×•×ª",
        )

    def _create_covered_query_recommendation(self, explain_plan: ExplainPlan) -> OptimizationRecommendation:
        """×™×¦×™×¨×ª ×”××œ×¦×” ×œ-covered query"""
        return OptimizationRecommendation(
            id=f"covered_{explain_plan.query_id}",
            title="ğŸŸ¢ ××¤×©×¨×•×ª ×œ-Covered Query",
            description=(
                "×”×©××™×œ×ª×” ××©×ª××©×ª ×‘××™× ×“×§×¡ ××‘×œ ×¢×“×™×™×Ÿ × ×™×’×©×ª ×œ××¡××›×™×. "
                "× ×™×ª×Ÿ ×œ×©×¤×¨ ×¢×œ ×™×“×™ ×”×•×¡×¤×ª ×©×“×•×ª ×”-projection ×œ××™× ×“×§×¡."
            ),
            severity=SeverityLevel.INFO,
            category="index",
            suggested_action="×”×•×¡×£ ××ª ×©×“×•×ª ×”-projection ×œ××™× ×“×§×¡ ××• ×”×’×‘×œ ××ª ×”×©×“×•×ª ×”××•×—×–×¨×™×",
            estimated_improvement="×¢×“ 50% ×©×™×¤×•×¨ ×‘×’×™×©×” ×œ× ×ª×•× ×™×",
        )

    def _create_frequent_query_recommendation(self, explain_plan: ExplainPlan, count: int) -> OptimizationRecommendation:
        """×™×¦×™×¨×ª ×”××œ×¦×” ×œ×©××™×œ×ª×•×ª ×ª×›×•×¤×•×ª"""
        return OptimizationRecommendation(
            id=f"frequent_{explain_plan.query_id}",
            title="ğŸ“Š ×“×¤×•×¡ ×©××™×œ×ª×” ×ª×›×•×¤×”",
            description=f"×©××™×œ×ª×” ×–×• ×”×•×¤×™×¢×” {count} ×¤×¢××™×. ×©×§×•×œ ××•×¤×˜×™××™×–×¦×™×” ××• caching.",
            severity=SeverityLevel.INFO,
            category="query",
            suggested_action="×©×§×•×œ caching ×‘×¨××ª ×”××¤×œ×™×§×¦×™×” ××• ××•×¤×˜×™××™×–×¦×™×” × ×•×¡×¤×ª",
            estimated_improvement="×”×¤×—×ª×ª ×¢×•××¡ ×¢×œ ×‘×¡×™×¡ ×”× ×ª×•× ×™×",
        )

    async def get_collection_stats(self, collection: str) -> Dict[str, Any]:
        """×§×‘×œ×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª collection ×œ×¦×•×¨×š ×”××œ×¦×•×ª"""

        def _get_stats() -> Dict[str, Any]:
            db = getattr(self.db_manager, "db", None)
            if db is None:
                raise RuntimeError("No MongoDB database available")
            stats = db.command("collStats", collection)
            indexes = list(db[collection].list_indexes())
            return {
                "size_bytes": stats.get("size", 0),
                "count": stats.get("count", 0),
                "avg_obj_size": stats.get("avgObjSize", 0),
                "index_count": len(indexes),
                "indexes": [idx.get("name") for idx in indexes if isinstance(idx, dict) and idx.get("name")],
                "total_index_size": stats.get("totalIndexSize", 0),
            }

        return await asyncio.to_thread(_get_stats)

    def get_summary(self) -> Dict[str, Any]:
        """×§×‘×œ×ª ×¡×™×›×•× ××¦×‘ ×”×¤×¨×•×¤×™×™×œ×¨"""
        queries = list(self._slow_queries)
        if not queries:
            return {
                "total_slow_queries": 0,
                "collections_affected": [],
                "avg_execution_time_ms": 0,
                "max_execution_time_ms": 0,
                "unique_patterns": 0,
            }

        collections = set(q.collection for q in queries)
        avg_time = sum(q.execution_time_ms for q in queries) / len(queries)
        max_time = max(q.execution_time_ms for q in queries)

        return {
            "total_slow_queries": len(queries),
            "collections_affected": list(collections),
            "avg_execution_time_ms": round(avg_time, 2),
            "max_execution_time_ms": round(max_time, 2),
            "unique_patterns": len(self._query_patterns),
            "threshold_ms": self.slow_threshold_ms,
        }

    # --- Aggregations ---
    def _fix_pipeline_for_explain(self, pipeline: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ×ª×™×§×•×Ÿ ×¢×¨×›×™ placeholder ×‘-pipeline ×œ×¤× ×™ ×©×œ×™×—×” ×œ-explain.

        ×›××©×¨ ×”-pipeline ×¢×‘×¨ × ×¨××•×œ (×œ×”×’× ×ª PII), ×¢×¨×›×™× ××¡×¤×¨×™×™× ×”×•×—×œ×¤×• ×‘-"<value>".
        MongoDB explain ×™×™×›×©×œ ×× ×™×§×‘×œ "$limit": "<value>" ×‘××§×•× ××¡×¤×¨.

        ×¤×•× ×§×¦×™×” ×–×• ××—×œ×™×¤×” ××ª ×”-placeholders ×‘×¢×¨×›×™ ×‘×¨×™×¨×ª ××—×“×œ ×ª×§×™× ×™×.
        """
        fixed: List[Dict[str, Any]] = []
        for stage in (pipeline or []):
            if not isinstance(stage, dict):
                fixed.append(stage)
                continue

            new_stage = {}
            for key, value in stage.items():
                # ×ª×™×§×•×Ÿ $limit - ×—×™×™×‘ ×œ×”×™×•×ª ××¡×¤×¨ ×©×œ× ×—×™×•×‘×™
                if key == "$limit":
                    if not isinstance(value, (int, float)) or value <= 0:
                        new_stage[key] = 10  # ×¢×¨×š ×“××™ ×œ×‘×“×™×§×”
                    else:
                        new_stage[key] = int(value)
                # ×ª×™×§×•×Ÿ $skip - ×—×™×™×‘ ×œ×”×™×•×ª ××¡×¤×¨ ×©×œ× ×œ×-×©×œ×™×œ×™
                elif key == "$skip":
                    if not isinstance(value, (int, float)) or value < 0:
                        new_stage[key] = 0
                    else:
                        new_stage[key] = int(value)
                # ×ª×™×§×•×Ÿ $sample - size ×—×™×™×‘ ×œ×”×™×•×ª ××¡×¤×¨ ×©×œ× ×—×™×•×‘×™
                elif key == "$sample" and isinstance(value, dict):
                    sample_val = value.copy()
                    if "size" in sample_val and not isinstance(sample_val["size"], (int, float)):
                        sample_val["size"] = 10
                    new_stage[key] = sample_val
                else:
                    new_stage[key] = value

            fixed.append(new_stage)
        return fixed

    async def get_aggregation_explain(
        self,
        collection: str,
        pipeline: List[Dict[str, Any]],
        verbosity: str = "queryPlanner",  # ×‘×¨×™×¨×ª ××—×“×œ ×‘×˜×•×—×”!
    ) -> AggregationExplainPlan:
        """
        ×§×‘×œ×ª explain plan ×œ××’×¨×’×¦×™×”.
        """
        # ×‘×“×™×§×” ×× ×”-pipeline ××›×™×œ query_shape ×©×‘×•×¨ ××’×¨×¡×” ×™×©× ×”
        for stage in (pipeline or []):
            if self._is_broken_query_shape(stage):
                raise ValueError(
                    "Pipeline contains broken array normalization from old version. "
                    "Arrays like '<N items>' cannot be used with explain(). "
                    "Please use the original pipeline or re-record this slow query."
                )

        # ×ª×™×§×•×Ÿ ×¢×¨×›×™ placeholder ×œ×¤× ×™ ×©×œ×™×—×” ×œ-MongoDB
        fixed_pipeline = self._fix_pipeline_for_explain(pipeline)

        def _run_explain() -> Dict[str, Any]:
            db = getattr(self.db_manager, "db", None)
            if db is None:
                raise RuntimeError("No MongoDB database available")
            # MongoDB command API ×œ-aggregate explain
            return db.command(
                "aggregate",
                collection,
                pipeline=fixed_pipeline,
                explain=True,
                cursor={},
            )

        explain_result = await asyncio.to_thread(_run_explain)
        return self._parse_aggregation_explain(collection, pipeline, explain_result)

    def _parse_aggregation_explain(
        self,
        collection: str,
        pipeline: List[Dict[str, Any]],
        explain_result: Dict[str, Any],
    ) -> AggregationExplainPlan:
        """×¤×¨×¡×•×¨ explain ×©×œ ××’×¨×’×¦×™×”"""
        stages: List[AggregationExplainStage] = []

        explain_stages = explain_result.get("stages", []) if isinstance(explain_result, dict) else []

        if not explain_stages:
            query_planner = explain_result.get("queryPlanner", {}) if isinstance(explain_result, dict) else {}
            if query_planner:
                stages.append(
                    AggregationExplainStage(
                        stage_name="OPTIMIZED_PIPELINE",
                        index_used=self._extract_index_from_planner(query_planner),
                    )
                )
        else:
            for stage_data in explain_stages:
                if isinstance(stage_data, dict):
                    stages.append(self._parse_aggregation_stage(stage_data))

        total_time = sum(float(s.execution_time_ms) for s in stages)

        pipeline_shape = self._normalize_pipeline_shape(pipeline)
        query_id = self._generate_query_id(collection, {"pipeline": pipeline_shape})

        return AggregationExplainPlan(
            query_id=query_id,
            collection=collection,
            pipeline_shape=pipeline_shape,
            stages=stages,
            total_execution_time_ms=total_time,
            server_info=explain_result.get("serverInfo", {}) if isinstance(explain_result, dict) else {},
        )

    def _extract_index_from_planner(self, query_planner: Dict[str, Any]) -> Optional[str]:
        try:
            winning_plan = query_planner.get("winningPlan", {})
            if isinstance(winning_plan, dict):
                return self._extract_index_name(winning_plan)
        except Exception:
            pass
        return None

    def _parse_aggregation_stage(self, stage_data: Dict[str, Any]) -> AggregationExplainStage:
        """×¤×¨×¡×•×¨ ×©×œ×‘ ××’×¨×’×¦×™×” ×‘×•×“×“"""
        stage_name = next((k for k in stage_data.keys() if str(k).startswith("$")), "UNKNOWN")
        stage_info = stage_data.get(stage_name, {}) if isinstance(stage_data.get(stage_name), dict) else {}

        execution_time = float(stage_data.get("executionTimeMillis", 0) or 0)
        docs_examined = int(stage_data.get("docsExamined", 0) or 0)
        n_returned = int(stage_data.get("nReturned", 0) or 0)
        uses_disk = bool(stage_data.get("usedDisk", False))
        memory_usage = int(stage_data.get("memUsage", 0) or 0)
        index_used = None
        lookup_collection = None
        lookup_strategy = None

        if stage_name == "$lookup":
            lookup_collection = stage_info.get("from")
            if "indexesUsed" in stage_data:
                indexes_used = stage_data.get("indexesUsed") or []
                if isinstance(indexes_used, list) and indexes_used:
                    index_used = indexes_used[0]
                lookup_strategy = "indexedLoopJoin"
            else:
                lookup_strategy = "nestedLoopJoin"

        if stage_name == "$match":
            input_stage = stage_data.get("inputStage", {}) if isinstance(stage_data.get("inputStage"), dict) else {}
            if input_stage.get("stage") == "IXSCAN":
                index_used = input_stage.get("indexName")

        return AggregationExplainStage(
            stage_name=str(stage_name),
            execution_time_ms=execution_time,
            docs_examined=docs_examined,
            n_returned=n_returned,
            uses_disk=uses_disk,
            memory_usage_bytes=memory_usage,
            index_used=index_used,
            lookup_collection=lookup_collection,
            lookup_strategy=lookup_strategy,
        )

    def _normalize_pipeline_shape(self, pipeline: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        × ×¨××•×œ ×¦×•×¨×ª ×”-pipeline - ×”×—×œ×¤×ª ×¢×¨×›×™× ×‘×¤×œ×™×™×¡×”×•×œ×“×¨×™×.

        âš ï¸ ××§×¨×™× ××™×•×—×“×™×:
        - $sort / $orderby: ×¢×¨×›×™× ××•×—×–×¨×™× ×›××• ×©×”× (1/-1 ×—×™×™×‘×™× ×œ×”×™×©××¨!)
        - ×©××¨ ×”×©×œ×‘×™×: × ×¨××•×œ ×¨×’×™×œ
        """
        # ××¤×ª×—×•×ª ×©×‘×”× ×œ× ×× ×¨××œ×™× ×¢×¨×›×™× ××¡×¤×¨×™×™× (1/-1) â€“ ×§×¨×™×˜×™×™× ×œ×ª×—×‘×™×¨ MongoDB
        SORT_LIKE_KEYS = {"$sort", "$orderby"}

        normalized: List[Dict[str, Any]] = []
        for stage in pipeline or []:
            if not isinstance(stage, dict):
                continue
            normalized_stage: Dict[str, Any] = {}
            for key, value in stage.items():
                if key in SORT_LIKE_KEYS:
                    # âš ï¸ ×§×¨×™×˜×™: ×œ× ×× ×¨××œ×™× $sort - ×¢×¨×›×™ 1/-1 ×—×™×™×‘×™× ×œ×”×™×©××¨!
                    normalized_stage[key] = value
                elif isinstance(value, dict):
                    normalized_stage[key] = self._normalize_query_shape(value)
                else:
                    normalized_stage[key] = "<value>"
            normalized.append(normalized_stage)
        return normalized

    async def analyze_aggregation_and_recommend(self, explain: AggregationExplainPlan) -> List[OptimizationRecommendation]:
        """×”××œ×¦×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ××’×¨×’×¦×™×•×ª"""
        recommendations: List[OptimizationRecommendation] = []

        for i, stage in enumerate(explain.stages):
            if stage.stage_name == "$lookup" and stage.lookup_strategy == "nestedLoopJoin":
                recommendations.append(
                    OptimizationRecommendation(
                        id=f"lookup_no_index_{explain.query_id}_{i}",
                        title=f"ğŸ”´ $lookup ×œ×œ× ××™× ×“×§×¡ ×¢×œ '{stage.lookup_collection}'",
                        description=(
                            f"×”-$lookup ××‘×¦×¢ nested loop join ×©×”×•× ××™×˜×™ ×××•×“. "
                            f"×¦×•×¨ ××™× ×“×§×¡ ×¢×œ ×”×©×“×” ×”××§×•×©×¨ ×‘-collection '{stage.lookup_collection}'."
                        ),
                        severity=SeverityLevel.CRITICAL,
                        category="index",
                        suggested_action=f"×¦×•×¨ ××™× ×“×§×¡ ×¢×œ ×©×“×” ×”-foreign field ×‘-{stage.lookup_collection}",
                        estimated_improvement="×™×›×•×œ ×œ×©×¤×¨ ×¤×™ 10-100",
                        code_example=f"db.{stage.lookup_collection}.createIndex({{ <foreignField>: 1 }})",
                    )
                )

            if stage.stage_name == "$sort" and stage.uses_disk:
                recommendations.append(
                    OptimizationRecommendation(
                        id=f"sort_disk_{explain.query_id}_{i}",
                        title="ğŸŸ  $sort ××©×ª××© ×‘×“×™×¡×§",
                        description=(
                            "×¤×¢×•×œ×ª ×”××™×•×Ÿ ×—×¨×’×” ×××’×‘×œ×ª ×”×–×™×›×¨×•×Ÿ (100MB) ×•×”×©×ª××©×” ×‘×“×™×¡×§. "
                            "×–×” ×××˜ ××©××¢×•×ª×™×ª ××ª ×”×©××™×œ×ª×”."
                        ),
                        severity=SeverityLevel.WARNING,
                        category="index",
                        suggested_action="×”×•×¡×£ $match ×œ×¤× ×™ ×”-$sort ×œ×”×§×˜× ×ª ×›××•×ª ×”× ×ª×•× ×™×, ××• ×¦×•×¨ ××™× ×“×§×¡ ×¢×œ ×©×“×” ×”××™×•×Ÿ",
                        estimated_improvement="×× ×™×¢×ª I/O ×œ×“×™×¡×§",
                    )
                )

            if stage.stage_name == "$unwind":
                recommendations.append(
                    OptimizationRecommendation(
                        id=f"unwind_warning_{explain.query_id}_{i}",
                        title="âš ï¸ ×©×™××•×© ×‘-$unwind",
                        description=(
                            "$unwind ×™×›×•×œ ×œ×”×›×¤×™×œ ××ª ××¡×¤×¨ ×”××¡××›×™× ×¤×™ ×’×•×“×œ ×”××¢×¨×š. "
                            "×•×“× ×©××ª×” ××¡× ×Ÿ ×œ×¤× ×™ ×”-$unwind."
                        ),
                        severity=SeverityLevel.INFO,
                        category="query",
                        suggested_action="×”×•×¡×£ $match ×œ×¤× ×™ $unwind ×œ×”×’×‘×œ×ª ×›××•×ª ×”××¡××›×™×",
                        estimated_improvement="×ª×œ×•×™ ×‘×’×•×“×œ ×”××¢×¨×›×™×",
                    )
                )

            if stage.stage_name == "$match" and i > 0:
                has_early_match = any(s.stage_name == "$match" for s in explain.stages[:i])
                if not has_early_match:
                    recommendations.append(
                        OptimizationRecommendation(
                            id=f"match_order_{explain.query_id}_{i}",
                            title="ğŸŸ¡ $match ×œ× ×‘×”×ª×—×œ×ª ×”-Pipeline",
                            description="×›×“××™ ×œ×©×™× $match ×›××” ×©×™×•×ª×¨ ××•×§×“× ×‘-pipeline ×›×“×™ ×œ×¡× ×Ÿ ××¡××›×™× ××•×§×“×.",
                            severity=SeverityLevel.WARNING,
                            category="query",
                            suggested_action="×”×¢×‘×¨ ××ª ×”-$match ×œ×”×ª×—×œ×ª ×”-pipeline ×× ××¤×©×¨",
                            estimated_improvement="×”×¤×—×ª×ª ×›××•×ª ×”× ×ª×•× ×™× ×‘×©×œ×‘×™× ×”×‘××™×",
                        )
                    )

        severity_order = {SeverityLevel.CRITICAL: 0, SeverityLevel.WARNING: 1, SeverityLevel.INFO: 2}
        return sorted(recommendations, key=lambda r: severity_order.get(r.severity, 999))


class PersistentQueryProfilerService(QueryProfilerService):
    """
    ×’×¨×¡×” ××©×•×¤×¨×ª ×©×œ ×”×¤×¨×•×¤×™×™×œ×¨ ×¢× ×©××™×¨×” ×‘-MongoDB.
    ××ª××™××” ×œ-Production.
    """

    COLLECTION_NAME = "slow_queries_log"

    async def record_slow_query(
        self,
        collection: str,
        operation: str,
        query: Dict[str, Any],
        execution_time_ms: float,
        client_info: Optional[Dict[str, Any]] = None,
    ) -> SlowQueryRecord:
        record = await super().record_slow_query(collection, operation, query, execution_time_ms, client_info)
        await self._persist_record(record)
        return record

    async def _persist_record(self, record: SlowQueryRecord) -> None:
        doc = {
            "query_id": record.query_id,
            "collection": record.collection,
            "operation": record.operation,
            "query_shape": record.query_shape,
            "execution_time_ms": record.execution_time_ms,
            "timestamp": record.timestamp,
            "client_info": record.client_info,
        }

        def _insert() -> None:
            db = getattr(self.db_manager, "db", None)
            if db is None:
                return None
            db[self.COLLECTION_NAME].insert_one(doc)
            return None

        await asyncio.to_thread(_insert)

    async def get_slow_queries(
        self,
        limit: int = 50,
        collection_filter: Optional[str] = None,
        min_execution_time_ms: Optional[float] = None,
        since: Optional[datetime] = None,
    ) -> List[SlowQueryRecord]:
        query: Dict[str, Any] = {}
        if collection_filter:
            query["collection"] = collection_filter
        if min_execution_time_ms is not None:
            query["execution_time_ms"] = {"$gte": float(min_execution_time_ms)}
        if since:
            query["timestamp"] = {"$gte": since}

        limit_n = max(1, int(limit))

        def _fetch() -> List[Dict[str, Any]]:
            db = getattr(self.db_manager, "db", None)
            if db is None:
                return []
            cursor = db[self.COLLECTION_NAME].find(query, sort=[("execution_time_ms", -1)], limit=limit_n)
            return list(cursor)

        docs = await asyncio.to_thread(_fetch)

        out: List[SlowQueryRecord] = []
        for doc in docs:
            if not isinstance(doc, dict):
                continue
            out.append(
                SlowQueryRecord(
                    query_id=str(doc.get("query_id") or ""),
                    collection=str(doc.get("collection") or ""),
                    operation=str(doc.get("operation") or ""),
                    query_shape=doc.get("query_shape") if isinstance(doc.get("query_shape"), dict) else {},
                    execution_time_ms=float(doc.get("execution_time_ms", 0) or 0),
                    timestamp=doc.get("timestamp") if isinstance(doc.get("timestamp"), datetime) else datetime.utcnow(),
                    client_info=doc.get("client_info") if isinstance(doc.get("client_info"), dict) else None,
                )
            )
        return out

    async def get_pattern_statistics(self, days: int = 7) -> List[Dict[str, Any]]:
        since = datetime.utcnow() - timedelta(days=max(1, int(days)))
        pipeline = [
            {"$match": {"timestamp": {"$gte": since}}},
            {
                "$group": {
                    "_id": {"query_id": "$query_id", "collection": "$collection", "operation": "$operation"},
                    "count": {"$sum": 1},
                    "avg_time_ms": {"$avg": "$execution_time_ms"},
                    "max_time_ms": {"$max": "$execution_time_ms"},
                    "min_time_ms": {"$min": "$execution_time_ms"},
                    "last_seen": {"$max": "$timestamp"},
                    "query_shape": {"$first": "$query_shape"},
                }
            },
            {"$sort": {"count": -1}},
            {"$limit": 50},
        ]

        def _aggregate() -> List[Dict[str, Any]]:
            db = getattr(self.db_manager, "db", None)
            if db is None:
                return []
            return list(db[self.COLLECTION_NAME].aggregate(pipeline))

        return await asyncio.to_thread(_aggregate)

    def get_summary(self) -> Dict[str, Any]:
        """×¡×™×›×•× ××”×™×¨ â€“ best-effort: × ×¡×” ××”-DB, ××—×¨×ª fallback ×œ×–×™×›×¨×•×Ÿ."""
        try:
            db = getattr(self.db_manager, "db", None)
            if db is None:
                return super().get_summary()
            # ×—×™×©×•×‘ lightweight ×¢×œ ×—×œ×•×Ÿ ×§×¦×¨ (24h) ×›×“×™ ×œ×”×™×× ×¢ ××¢×•××¡
            since = datetime.utcnow() - timedelta(hours=24)
            query = {"timestamp": {"$gte": since}}
            total = int(db[self.COLLECTION_NAME].count_documents(query))
            if total <= 0:
                return {
                    "total_slow_queries": 0,
                    "collections_affected": [],
                    "avg_execution_time_ms": 0,
                    "max_execution_time_ms": 0,
                    "unique_patterns": 0,
                    "threshold_ms": self.slow_threshold_ms,
                }
            agg = list(
                db[self.COLLECTION_NAME].aggregate(
                    [
                        {"$match": query},
                        {
                            "$group": {
                                "_id": None,
                                "avg_ms": {"$avg": "$execution_time_ms"},
                                "max_ms": {"$max": "$execution_time_ms"},
                                "collections": {"$addToSet": "$collection"},
                                "unique_patterns": {"$addToSet": "$query_id"},
                            }
                        },
                    ]
                )
            )
            doc = agg[0] if agg else {}
            collections = doc.get("collections") if isinstance(doc.get("collections"), list) else []
            patterns = doc.get("unique_patterns") if isinstance(doc.get("unique_patterns"), list) else []
            return {
                "total_slow_queries": total,
                "collections_affected": collections,
                "avg_execution_time_ms": round(float(doc.get("avg_ms", 0) or 0), 2),
                "max_execution_time_ms": round(float(doc.get("max_ms", 0) or 0), 2),
                "unique_patterns": len(patterns),
                "threshold_ms": self.slow_threshold_ms,
            }
        except Exception:
            return super().get_summary()

