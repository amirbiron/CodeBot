# ğŸš€ ××“×¨×™×š ××™××•×©: Smart Semantic Search (AI Embeddings)

> **××˜×¨×ª ×”××¡××š:** ××“×¨×™×š ×¦×¢×“-××—×¨-×¦×¢×“ ×œ××™××•×© ×—×™×¤×•×© ×¡×× ×˜×™ ×—×›× ×‘-CodeBot, ×ª×•×× ×œ×§×•×“ ×”×§×™×™×.  
> **×§×”×œ ×™×¢×“:** ××¤×ª×—×™× ×©×¢×•×‘×“×™× ×¢×œ ×”×¤×¨×•×™×§×˜.  
> **×ª××¨×™×š:** ×“×¦××‘×¨ 2025

---

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×

1. [×¡×§×™×¨×” ×›×œ×œ×™×ª](#-×¡×§×™×¨×”-×›×œ×œ×™×ª)
2. [×©×œ×‘ 1: ×”×’×“×¨×ª ×ª×©×ª×™×ª](#-×©×œ×‘-1-×”×’×“×¨×ª-×ª×©×ª×™×ª)
3. [×©×œ×‘ 2: ×™×¦×™×¨×ª ×©×™×¨×•×ª Embeddings](#-×©×œ×‘-2-×™×¦×™×¨×ª-×©×™×¨×•×ª-embeddings)
4. [×©×œ×‘ 3: ×¢×“×›×•×Ÿ ×”××•×“×œ×™× ×‘-Database](#-×©×œ×‘-3-×¢×“×›×•×Ÿ-×”××•×“×œ×™×-×‘-database)
5. [×©×œ×‘ 4: ×™×¦×™×¨×ª Vector Index ×‘-MongoDB](#-×©×œ×‘-4-×™×¦×™×¨×ª-vector-index-×‘-mongodb)
6. [×©×œ×‘ 5: ××™× ×“×•×§×¡ ×§×‘×¦×™× ×§×™×™××™×](#-×©×œ×‘-5-××™× ×“×•×§×¡-×§×‘×¦×™×-×§×™×™××™×)
7. [×©×œ×‘ 6: ×¢×“×›×•×Ÿ ×× ×•×¢ ×”×—×™×¤×•×©](#-×©×œ×‘-6-×¢×“×›×•×Ÿ-×× ×•×¢-×”×—×™×¤×•×©)
8. [×©×œ×‘ 7: ×¢×“×›×•×Ÿ ×”-API](#-×©×œ×‘-7-×¢×“×›×•×Ÿ-×”-api)
9. [×©×œ×‘ 8: ×¢×“×›×•×Ÿ ×”-Frontend](#-×©×œ×‘-8-×¢×“×›×•×Ÿ-×”-frontend)
10. [×©×™×¤×•×¨×™× ××•××œ×¦×™× (Nice to Have)](#-×©×™×¤×•×¨×™×-××•××œ×¦×™×-nice-to-have)
11. [×‘×“×™×§×•×ª](#-×‘×“×™×§×•×ª)
12. [× ×¡×¤×—×™×](#-× ×¡×¤×—×™×)

---

## ğŸ¯ ×¡×§×™×¨×” ×›×œ×œ×™×ª

### ××” × ×‘× ×”?

××¢×¨×›×ª ×—×™×¤×•×© ×—×›××” ×©×××¤×©×¨×ª ×œ××¦×•× ×§×‘×¦×™× **×œ×¤×™ ××©××¢×•×ª** ×•×œ× ×¨×§ ×œ×¤×™ ××™×œ×™× ××“×•×™×§×•×ª.

### ×“×•×’××”

| ×—×™×¤×•×© ×”××©×ª××© | ×ª×•×¦××” ×©×ª×™××¦× |
|--------------|--------------|
| `"×ª×™×§×•×Ÿ ×”×‘×”×•×‘ ×‘×›×¤×ª×•×¨"` | `theme.css` ×¢× `/* prevent white flash on click */` |
| `"validate email"` | ×§×•×‘×¥ ×¢× ×¤×•× ×§×¦×™×” `is_valid_email_address()` |
| `"handle errors"` | ×§×•×‘×¥ ×¢× `try/except` ××• `catch` |

### ××¨×›×™×˜×§×˜×•×¨×” ×‘×¨××” ×’×‘×•×”×”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query  â”‚â”€â”€â”€â–¶â”‚ Embedding API  â”‚â”€â”€â”€â–¶â”‚ Vector (1536d)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (OpenAI)      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Results     â”‚â—€â”€â”€â”€â”‚ MongoDB Atlas  â”‚â—€â”€â”€â”€â”‚ $vectorSearch   â”‚
â”‚  (ranked)    â”‚    â”‚ Vector Search  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ ×©×œ×‘ 1: ×”×’×“×¨×ª ×ª×©×ª×™×ª

### 1.1 ×”×•×¡×¤×ª ××©×ª× ×™ ×¡×‘×™×‘×”

×”×•×¡×£ ×œ×§×•×‘×¥ `.env` (××• ×œ-secrets):

```bash
# Embeddings API
OPENAI_API_KEY=sk-...your-key...
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536  # or 512 for smaller index

# Feature flags
SEMANTIC_SEARCH_ENABLED=true
SEMANTIC_SEARCH_INDEX_ON_SAVE=true
```

### 1.2 ×¢×“×›×•×Ÿ `config.py`

××¦× ××ª ×”×§×•×‘×¥ `config.py` ×•×”×•×¡×£ ××ª ×”×”×’×“×¨×•×ª ×”×‘××•×ª (×œ×™×“ ×”×”×’×“×¨×•×ª ×”×§×™×™××•×ª):

```python
# === Semantic Search ===
OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
EMBEDDING_MODEL: str = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
EMBEDDING_DIMENSIONS: int = int(os.getenv("EMBEDDING_DIMENSIONS", "1536"))
SEMANTIC_SEARCH_ENABLED: bool = os.getenv("SEMANTIC_SEARCH_ENABLED", "false").lower() == "true"
SEMANTIC_SEARCH_INDEX_ON_SAVE: bool = os.getenv("SEMANTIC_SEARCH_INDEX_ON_SAVE", "false").lower() == "true"
# ××¡×¤×¨ ×ª×•×•×™× ××§×¡×™××œ×™ ×œ×©×œ×™×—×” ×œ-Embedding API (×—×™×¡×›×•×Ÿ ×‘×¢×œ×•×™×•×ª)
EMBEDDING_MAX_CHARS: int = int(os.getenv("EMBEDDING_MAX_CHARS", "2000"))
```

### 1.3 ×”×•×¡×¤×ª dependencies

×”×•×¡×£ ×œ-`requirements/base.txt`:

```
openai>=1.0.0
tiktoken>=0.5.0  # ×œ×¡×¤×™×¨×ª tokens (××•×¤×¦×™×•× ×œ×™)
```

---

## ğŸ¤– ×©×œ×‘ 2: ×™×¦×™×¨×ª ×©×™×¨×•×ª Embeddings

### 2.1 ×™×¦×™×¨×ª `services/embedding_service.py`

×¦×•×¨ ×§×•×‘×¥ ×—×“×©:

```python
"""
×©×™×¨×•×ª ×™×¦×™×¨×ª Embeddings ×¢×‘×•×¨ ×—×™×¤×•×© ×¡×× ×˜×™.
Embedding Service for Semantic Search.
"""

from __future__ import annotations

import hashlib
import logging
import os
from typing import Any, Dict, List, Optional

import httpx

from config import config

logger = logging.getLogger(__name__)

# ===== Constants =====
_OPENAI_API_URL = "https://api.openai.com/v1/embeddings"
_DEFAULT_MODEL = "text-embedding-3-small"
_DEFAULT_DIMENSIONS = 1536

# Observability imports (safe fallbacks)
try:
    from observability import emit_event
except Exception:
    def emit_event(event: str, severity: str = "info", **fields):
        return None

try:
    from metrics import track_performance
except Exception:
    from contextlib import contextmanager
    @contextmanager
    def track_performance(operation: str, labels=None):
        yield


class EmbeddingError(RuntimeError):
    """×©×’×™××” ×‘×™×¦×™×¨×ª embedding."""
    pass


def _get_api_key() -> str:
    """×§×‘×œ×ª API key ××”×§×•× ×¤×™×’ ××• ×-ENV."""
    return getattr(config, "OPENAI_API_KEY", "") or os.getenv("OPENAI_API_KEY", "")


def _get_model() -> str:
    """×§×‘×œ×ª ×©× ×”××•×“×œ."""
    return getattr(config, "EMBEDDING_MODEL", _DEFAULT_MODEL) or _DEFAULT_MODEL


def _get_dimensions() -> int:
    """×§×‘×œ×ª ××¡×¤×¨ ×”×××“×™×."""
    return int(getattr(config, "EMBEDDING_DIMENSIONS", _DEFAULT_DIMENSIONS) or _DEFAULT_DIMENSIONS)


def _truncate_text(text: str, max_chars: int) -> str:
    """×§×™×¦×•×¨ ×˜×§×¡×˜ ×œ××§×¡×™××•× ×ª×•×•×™×."""
    if not text:
        return ""
    max_chars = max(100, int(max_chars))
    if len(text) <= max_chars:
        return text
    # ×—×™×ª×•×š ×—×›×: × ×¡×” ×œ×—×ª×•×š ×‘×¡×•×£ ××©×¤×˜ ××• ×©×•×¨×”
    truncated = text[:max_chars]
    # ××¦× ××ª ×”× ×§×•×“×” ××• ×”×©×•×¨×” ×”×—×“×©×” ×”××—×¨×•× ×”
    for sep in ["\n\n", "\n", ". ", ".\n"]:
        last_sep = truncated.rfind(sep)
        if last_sep > max_chars * 0.7:  # ×œ×¤×—×•×ª 70% ××”×˜×§×¡×˜
            return truncated[:last_sep + len(sep)].strip()
    return truncated.strip()


def _prepare_text_for_embedding(
    code: str,
    file_name: str = "",
    description: str = "",
    tags: Optional[List[str]] = None,
    programming_language: str = "",
) -> str:
    """
    ×”×›× ×ª ×˜×§×¡×˜ ×œ×©×œ×™×—×” ×œ-Embedding API.
    ××©×œ×‘ ××˜×-×“××˜×” ×¢× ×”×§×•×“ ×œ×§×‘×œ×ª embedding ×¢×©×™×¨ ×™×•×ª×¨.
    """
    parts: List[str] = []
    
    # ×”×•×¡×£ ××˜×-×“××˜×” (××©×§×œ ×’×‘×•×” ×™×•×ª×¨ ×œ×¤×¨×™×˜×™× ××œ×•)
    if file_name:
        parts.append(f"File: {file_name}")
    if programming_language:
        parts.append(f"Language: {programming_language}")
    if description:
        parts.append(f"Description: {description}")
    if tags:
        safe_tags = [str(t).strip() for t in tags if t and not str(t).startswith("repo:")]
        if safe_tags:
            parts.append(f"Tags: {', '.join(safe_tags)}")
    
    # ×”×•×¡×£ ××ª ×”×§×•×“ ×¢×¦××•
    if code:
        # ××•×¤×˜×™××™×–×¦×™×”: ×¢×‘×•×¨ ×§×•×“ ××¨×•×š, ×”×ª××§×“ ×‘×”×¢×¨×•×ª ×•×‘×”×ª×—×œ×”
        max_code_chars = int(getattr(config, "EMBEDDING_MAX_CHARS", 2000) or 2000)
        code_truncated = _truncate_text(code, max_code_chars)
        parts.append(f"Code:\n{code_truncated}")
    
    return "\n".join(parts)


async def generate_embedding(
    text: str,
    *,
    timeout: float = 10.0,
) -> List[float]:
    """
    ×™×¦×™×¨×ª embedding vector ×¢×‘×•×¨ ×˜×§×¡×˜.
    
    Args:
        text: ×”×˜×§×¡×˜ ×œ×™×¦×™×¨×ª embedding.
        timeout: ×–××Ÿ ××§×¡×™××œ×™ ×œ×‘×§×©×” ×‘×©× ×™×•×ª.
    
    Returns:
        ×¨×©×™××ª floats (×•×§×˜×•×¨) ×‘××•×¨×š EMBEDDING_DIMENSIONS.
    
    Raises:
        EmbeddingError: ×‘××§×¨×” ×©×œ ×©×’×™××”.
    """
    api_key = _get_api_key()
    if not api_key:
        raise EmbeddingError("openai_api_key_missing")
    
    if not text or not text.strip():
        raise EmbeddingError("empty_text")
    
    model = _get_model()
    dimensions = _get_dimensions()
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    
    payload = {
        "model": model,
        "input": text.strip(),
        "dimensions": dimensions,
    }
    
    try:
        emit_event("embedding_request_start", severity="debug", model=model)
    except Exception:
        pass
    
    with track_performance("embedding_api_call", labels={"model": model}):
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.post(_OPENAI_API_URL, headers=headers, json=payload)
                response.raise_for_status()
                data = response.json()
        except httpx.TimeoutException as exc:
            logger.warning("embedding_api_timeout", extra={"model": model})
            raise EmbeddingError("embedding_timeout") from exc
        except httpx.HTTPStatusError as exc:
            logger.warning(
                "embedding_api_http_error",
                extra={"status_code": exc.response.status_code, "model": model}
            )
            raise EmbeddingError(f"embedding_http_error_{exc.response.status_code}") from exc
        except httpx.RequestError as exc:
            logger.warning("embedding_api_request_error", extra={"error": str(exc)})
            raise EmbeddingError("embedding_request_error") from exc
    
    # ×—×™×œ×•×¥ ×”×•×•×§×˜×•×¨ ××”×ª×’×•×‘×”
    try:
        embedding = data["data"][0]["embedding"]
        if not isinstance(embedding, list) or len(embedding) != dimensions:
            raise EmbeddingError("embedding_invalid_response")
        
        try:
            emit_event(
                "embedding_request_done",
                severity="debug",
                model=model,
                dimensions=len(embedding),
            )
        except Exception:
            pass
        
        return embedding
    except (KeyError, IndexError, TypeError) as exc:
        raise EmbeddingError("embedding_parse_error") from exc


async def generate_embedding_for_file(
    code: str,
    file_name: str = "",
    description: str = "",
    tags: Optional[List[str]] = None,
    programming_language: str = "",
    **kwargs,
) -> List[float]:
    """
    ×™×¦×™×¨×ª embedding ×¢×‘×•×¨ ×§×•×‘×¥ ×§×•×“ (convenience function).
    
    ××©×œ×‘ ××ª ×›×œ ×”××˜×-×“××˜×” ×¢× ×”×§×•×“ ×œ×§×‘×œ×ª embedding ××“×•×™×§ ×™×•×ª×¨.
    """
    text = _prepare_text_for_embedding(
        code=code,
        file_name=file_name,
        description=description,
        tags=tags,
        programming_language=programming_language,
    )
    return await generate_embedding(text, **kwargs)


def generate_embedding_sync(text: str, *, timeout: float = 10.0) -> List[float]:
    """
    ×’×¨×¡×” ×¡×™× ×›×¨×•× ×™×ª ×œ×™×¦×™×¨×ª embedding (×œ×©×™××•×© ×‘-background jobs).
    """
    import asyncio
    
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # ×× ×›×‘×¨ ×™×© event loop ×¨×¥, ×¦×•×¨ ×—×“×©
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as pool:
                future = pool.submit(asyncio.run, generate_embedding(text, timeout=timeout))
                return future.result(timeout=timeout + 5)
        else:
            return loop.run_until_complete(generate_embedding(text, timeout=timeout))
    except Exception:
        # Fallback ×œ×¤×©×˜×•×ª
        return asyncio.run(generate_embedding(text, timeout=timeout))


# ===== Cache helpers =====
def _hash_text(text: str) -> str:
    """×™×¦×™×¨×ª hash ×§×¦×¨ ×œ×˜×§×¡×˜ (×œ×¦×•×¨×›×™ cache)."""
    return hashlib.sha256(text.encode("utf-8", errors="ignore")).hexdigest()[:32]
```

### 2.2 ×¢×“×›×•×Ÿ `services/__init__.py`

×”×•×¡×£ ×™×™×‘×•× ×œ×©×™×¨×•×ª ×”×—×“×©:

```python
# ×‘×ª×—×ª×™×ª ×”×§×•×‘×¥, ×”×•×¡×£:
try:
    from .embedding_service import (
        generate_embedding,
        generate_embedding_for_file,
        generate_embedding_sync,
        EmbeddingError,
    )
except ImportError:
    pass  # Optional dependency
```

---

## ğŸ“Š ×©×œ×‘ 3: ×¢×“×›×•×Ÿ ×”××•×“×œ×™× ×‘-Database

### 3.1 ×¢×“×›×•×Ÿ `database/models.py`

××¦× ××ª ×”-dataclass `CodeSnippet` ×•×”×•×¡×£ ××ª ×”×©×“×•×ª ×”×‘××™×:

```python
@dataclass
class CodeSnippet:
    """×™×™×¦×•×’ ×§×˜×¢ ×§×•×“ ×”× ×©××¨ ×‘××¡×“ ×”× ×ª×•× ×™×."""
    user_id: int
    file_name: str
    code: str
    programming_language: str
    # ×©×“×•×ª ××•×¢×“×¤×™×
    is_favorite: bool = False
    favorited_at: Optional[datetime] = None
    description: str = ""
    tags: Optional[List[str]] = None
    version: int = 1
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    is_active: bool = True
    # ×©×“×•×ª ×¡×œ ××™×—×–×•×¨
    deleted_at: Optional[datetime] = None
    deleted_expires_at: Optional[datetime] = None
    
    # ===== ×©×“×•×ª ×—×“×©×™× ×œ×—×™×¤×•×© ×¡×× ×˜×™ =====
    embedding: Optional[List[float]] = None  # ×•×§×˜×•×¨ ×”-embedding (1536 floats)
    embedding_model: Optional[str] = None    # ×©× ×”××•×“×œ ×©×™×¦×¨ ××ª ×”-embedding
    embedding_updated_at: Optional[datetime] = None  # ××ª×™ ×¢×•×“×›×Ÿ ×”-embedding
    needs_embedding_update: bool = True  # ×”×× ×¦×¨×™×š ×œ×¢×“×›×Ÿ ××ª ×”-embedding
    
    def __post_init__(self) -> None:
        if self.tags is None:
            self.tags = []
        if self.created_at is None:
            self.created_at = datetime.now(timezone.utc)
        if self.updated_at is None:
            self.updated_at = datetime.now(timezone.utc)
```

### 3.2 ×¢×“×›×•×Ÿ `HEAVY_FIELDS_EXCLUDE_PROJECTION` ×‘-`database/repository.py`

××¦× ××ª ×”×”×’×“×¨×” ×©×œ `_HEAVY_FIELDS_EXCLUDE_PROJECTION` ×•×¢×“×›×Ÿ:

```python
_HEAVY_FIELDS_EXCLUDE_PROJECTION: Dict[str, int] = {
    "code": 0,        # CodeSnippet
    "content": 0,     # LargeFile
    "raw_data": 0,    # future-proof
    "raw_content": 0, # future-proof
    "embedding": 0,   # â¬…ï¸ ×—×“×©! ×œ× ×œ×”×—×–×™×¨ ××ª ×”×•×§×˜×•×¨ ×‘×¨×©×™××•×ª
}
```

### 3.3 ×¢×“×›×•×Ÿ `save_code_snippet` ×‘-`database/repository.py`

××¦× ××ª ×”×¤×•× ×§×¦×™×” `save_code_snippet` ×•×”×•×¡×£ ××ª ×”×”×™×’×™×•×Ÿ ×œ×¡×™××•×Ÿ ×¦×•×¨×š ×‘×¢×“×›×•×Ÿ embedding:

> âš ï¸ **×—×©×•×‘:** ×™×© ×œ×‘×“×•×§ ×©×™× ×•×™ ×‘×›×œ ×”×©×“×•×ª ×©××¨×›×™×‘×™× ××ª ×”-Embedding (×œ× ×¨×§ `code`!).  
> ×”-Embedding × ×‘× ×” ×: `code` + `description` + `tags` + `programming_language`.  
> ×× × ×‘×“×•×§ ×¨×§ ××ª `code`, ×©×™× ×•×™ ×‘×ª×™××•×¨ ××• ×‘×ª×’×™×•×ª ×œ× ×™×¢×“×›×Ÿ ××ª ×”-Embedding ×•×”×—×™×¤×•×© ×œ× ×™××¦× ××ª ×”×§×•×‘×¥ ×œ×¤×™ ×”××™×“×¢ ×”×—×“×©.

```python
@_instrument_db("db.save_code_snippet")
def save_code_snippet(self, snippet: CodeSnippet) -> bool:
    try:
        # Normalize code before persisting
        try:
            if config.NORMALIZE_CODE_ON_SAVE:
                snippet.code = normalize_code(snippet.code)
        except Exception:
            pass
        
        existing = self.get_latest_version(snippet.user_id, snippet.file_name)
        if existing:
            snippet.version = existing['version'] + 1
            # ... (×§×•×“ ×§×™×™× ×œ×©××™×¨×ª ××•×¢×“×¤×™×)
            
            # ===== ×‘×“×™×§×” ×× ×¦×¨×™×š ×œ×¢×“×›×Ÿ embedding =====
            # ×—×©×•×‘: ×‘×•×“×§×™× ×©×™× ×•×™ ×‘×›×œ ×”×©×“×•×ª ×©××¨×›×™×‘×™× ××ª ×”-Embedding!
            # ×”-Embedding × ×‘× ×” ×: code + description + tags + programming_language
            
            old_code = existing.get('code', '')
            old_description = existing.get('description', '')
            old_tags = existing.get('tags') or []
            old_language = existing.get('programming_language', '')
            
            # ×”×©×•×•××” ×‘×˜×•×—×” ×©×œ tags (×¨×©×™××•×ª)
            def _normalize_tags(tags):
                if not tags:
                    return []
                return sorted([str(t).strip().lower() for t in tags if t])
            
            embedding_content_changed = (
                old_code != snippet.code or
                old_description != (snippet.description or '') or
                _normalize_tags(old_tags) != _normalize_tags(snippet.tags) or
                old_language != (snippet.programming_language or '')
            )
            
            if embedding_content_changed:
                snippet.needs_embedding_update = True
                snippet.embedding = None  # × ×§×” embedding ×™×©×Ÿ
            else:
                # ×©××•×¨ ×¢×œ ×”-embedding ×”×§×™×™× ×× ×”×ª×•×›×Ÿ ×œ× ×”×©×ª× ×”
                snippet.embedding = existing.get('embedding')
                snippet.embedding_model = existing.get('embedding_model')
                snippet.embedding_updated_at = existing.get('embedding_updated_at')
                snippet.needs_embedding_update = existing.get('needs_embedding_update', True)
        
        snippet.updated_at = datetime.now(timezone.utc)
        # ... (×”××©×š ×”×§×•×“ ×”×§×™×™×)
```

---

## ğŸ” ×©×œ×‘ 4: ×™×¦×™×¨×ª Vector Index ×‘-MongoDB

### 4.1 ×™×¦×™×¨×ª ×”××™× ×“×§×¡ ×‘-MongoDB Atlas

**×—×©×•×‘:** Vector Search ×–××™×Ÿ ×¨×§ ×‘-MongoDB Atlas (×œ× ×‘-Community Edition).

×”×™×›× ×¡ ×œ-MongoDB Atlas Console ×•×¦×•×¨ Search Index ×—×“×©:

1. ×œ×š ×œ-**Database** â†’ **Search** â†’ **Create Search Index**
2. ×‘×—×¨ **JSON Editor**
3. ×”×–×Ÿ ××ª ×”×”×’×“×¨×” ×”×‘××”:

```json
{
  "name": "code_snippets_vector_index",
  "type": "vectorSearch",
  "definition": {
    "fields": [
      {
        "type": "vector",
        "path": "embedding",
        "numDimensions": 1536,
        "similarity": "cosine"
      },
      {
        "type": "filter",
        "path": "user_id"
      },
      {
        "type": "filter",
        "path": "is_active"
      },
      {
        "type": "filter",
        "path": "programming_language"
      }
    ]
  }
}
```

### 4.2 ×™×¦×™×¨×ª ×¡×§×¨×™×¤×˜ ×œ×™×¦×™×¨×ª ×”××™× ×“×§×¡ (××•×¤×¦×™×•× ×œ×™)

×¦×•×¨ ×§×•×‘×¥ `scripts/create_vector_index.py`:

```python
"""
×¡×§×¨×™×¤×˜ ×œ×™×¦×™×¨×ª Vector Index ×‘-MongoDB Atlas.
"""

import os
import sys

# ×”×•×¡×£ ××ª ×”-root ×œ× ×ª×™×‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pymongo import MongoClient
from config import config

def create_vector_index():
    """×™×¦×™×¨×ª Vector Search Index."""
    client = MongoClient(config.MONGODB_URI)
    db = client[config.MONGODB_DB_NAME]
    collection = db.code_snippets
    
    # ×”×’×“×¨×ª ×”××™× ×“×§×¡
    index_definition = {
        "name": "code_snippets_vector_index",
        "type": "vectorSearch",
        "definition": {
            "fields": [
                {
                    "type": "vector",
                    "path": "embedding",
                    "numDimensions": int(os.getenv("EMBEDDING_DIMENSIONS", "1536")),
                    "similarity": "cosine"
                },
                {"type": "filter", "path": "user_id"},
                {"type": "filter", "path": "is_active"},
                {"type": "filter", "path": "programming_language"},
            ]
        }
    }
    
    print("Creating vector index...")
    print("Note: This must be done via Atlas UI or Atlas Admin API")
    print("Index definition:")
    import json
    print(json.dumps(index_definition, indent=2))

if __name__ == "__main__":
    create_vector_index()
```

---

## ğŸ“¥ ×©×œ×‘ 5: ××™× ×“×•×§×¡ ×§×‘×¦×™× ×§×™×™××™×

### 5.1 ×™×¦×™×¨×ª Background Job ×œ××™× ×“×•×§×¡

×¦×•×¨ ×§×•×‘×¥ `scripts/index_embeddings.py`:

```python
"""
×¡×§×¨×™×¤×˜ ×œ××™× ×“×•×§×¡ embeddings ×¢×‘×•×¨ ×§×‘×¦×™× ×§×™×™××™×.
Batch Indexing Script for Existing Files.
"""

from __future__ import annotations

import asyncio
import logging
import os
import sys
import time
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

# ×”×•×¡×£ ××ª ×”-root ×œ× ×ª×™×‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import config

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)

# Constants
BATCH_SIZE = 50  # ××¡×¤×¨ ×§×‘×¦×™× ×œ×¢×™×‘×•×“ ×‘×›×œ batch
RATE_LIMIT_DELAY = 0.1  # ×”×©×”×™×” ×‘×™×Ÿ ×‘×§×©×•×ª (×©× ×™×•×ª)
MAX_RETRIES = 3


async def process_batch(
    files: List[Dict[str, Any]],
    repository,
    stats: Dict[str, int],
) -> None:
    """×¢×™×‘×•×“ batch ×©×œ ×§×‘×¦×™×."""
    from services.embedding_service import (
        generate_embedding_for_file,
        EmbeddingError,
    )
    
    for file_data in files:
        file_id = str(file_data.get("_id", ""))
        file_name = file_data.get("file_name", "unknown")
        user_id = file_data.get("user_id")
        
        try:
            # ×”×›×Ÿ ××ª ×”×˜×§×¡×˜
            code = file_data.get("code", "")
            if not code:
                stats["skipped_empty"] += 1
                continue
            
            # ×™×¦×™×¨×ª embedding
            embedding = await generate_embedding_for_file(
                code=code,
                file_name=file_name,
                description=file_data.get("description", ""),
                tags=file_data.get("tags"),
                programming_language=file_data.get("programming_language", ""),
            )
            
            # ×¢×“×›×•×Ÿ ×‘-DB
            now = datetime.now(timezone.utc)
            update_result = repository.manager.collection.update_one(
                {"_id": file_data["_id"]},
                {
                    "$set": {
                        "embedding": embedding,
                        "embedding_model": config.EMBEDDING_MODEL,
                        "embedding_updated_at": now,
                        "needs_embedding_update": False,
                    }
                }
            )
            
            if update_result.modified_count > 0:
                stats["indexed"] += 1
                logger.debug(f"Indexed: {file_name}")
            else:
                stats["unchanged"] += 1
            
            # Rate limiting
            await asyncio.sleep(RATE_LIMIT_DELAY)
            
        except EmbeddingError as e:
            stats["errors"] += 1
            logger.warning(f"Embedding error for {file_name}: {e}")
        except Exception as e:
            stats["errors"] += 1
            logger.error(f"Unexpected error for {file_name}: {e}")


async def index_all_files(dry_run: bool = False) -> Dict[str, int]:
    """××™× ×“×•×§×¡ ×›×œ ×”×§×‘×¦×™× ×©×¦×¨×™×›×™× embedding."""
    from database import db
    
    stats = {
        "total": 0,
        "indexed": 0,
        "skipped_empty": 0,
        "unchanged": 0,
        "errors": 0,
    }
    
    logger.info("Starting embedding indexing...")
    
    # ×©××™×œ×ª×” ×œ×›×œ ×”×§×‘×¦×™× ×©×¦×¨×™×›×™× embedding
    query = {
        "$or": [
            {"embedding": {"$exists": False}},
            {"embedding": None},
            {"needs_embedding_update": True},
        ],
        "is_active": {"$ne": False},
    }
    
    # ×¡×¤×™×¨×”
    try:
        total = db.manager.collection.count_documents(query)
        stats["total"] = total
        logger.info(f"Found {total} files to index")
    except Exception as e:
        logger.error(f"Failed to count documents: {e}")
        return stats
    
    if dry_run:
        logger.info("Dry run - not making changes")
        return stats
    
    # ×¢×™×‘×•×“ ×‘-batches
    cursor = db.manager.collection.find(
        query,
        # Include code for embedding, but exclude heavy fields we don't need
        projection={
            "_id": 1,
            "user_id": 1,
            "file_name": 1,
            "code": 1,
            "description": 1,
            "tags": 1,
            "programming_language": 1,
        }
    ).batch_size(BATCH_SIZE)
    
    batch: List[Dict[str, Any]] = []
    batch_num = 0
    
    for doc in cursor:
        batch.append(doc)
        
        if len(batch) >= BATCH_SIZE:
            batch_num += 1
            logger.info(f"Processing batch {batch_num} ({len(batch)} files)...")
            await process_batch(batch, db, stats)
            batch = []
    
    # ×¢×™×‘×•×“ batch ××—×¨×•×Ÿ
    if batch:
        batch_num += 1
        logger.info(f"Processing final batch {batch_num} ({len(batch)} files)...")
        await process_batch(batch, db, stats)
    
    logger.info(
        f"Indexing complete. "
        f"Total: {stats['total']}, "
        f"Indexed: {stats['indexed']}, "
        f"Errors: {stats['errors']}"
    )
    
    return stats


def main():
    """Entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Index embeddings for existing files")
    parser.add_argument("--dry-run", action="store_true", help="Don't make changes")
    args = parser.parse_args()
    
    # ×‘×“×™×§×ª API key
    if not os.getenv("OPENAI_API_KEY") and not getattr(config, "OPENAI_API_KEY", ""):
        logger.error("OPENAI_API_KEY not set!")
        sys.exit(1)
    
    # ×”×¨×¦×”
    stats = asyncio.run(index_all_files(dry_run=args.dry_run))
    
    # Exit code ×œ×¤×™ ×”×¦×œ×—×”
    if stats["errors"] > stats["indexed"]:
        sys.exit(1)


if __name__ == "__main__":
    main()
```

### 5.2 ×”×•×¡×¤×ª Hook ×œ××™× ×“×•×§×¡ ××•×˜×•××˜×™ ×‘×©××™×¨×”

×¢×“×›×Ÿ ××ª `save_code_snippet` ×‘-`database/repository.py` ×œ×”×•×¡×¤×ª ××™× ×“×•×§×¡ ××¡×™× ×›×¨×•× ×™:

```python
# ×‘×¡×•×£ ×”×¤×•× ×§×¦×™×” save_code_snippet, ××—×¨×™ ×”-insert ×”×¦×œ×™×—:

if result.inserted_id:
    # ... (×§×•×“ cache invalidation ×§×™×™×)
    
    # ===== ××™× ×“×•×§×¡ embedding ××¡×™× ×›×¨×•× ×™ =====
    if getattr(config, "SEMANTIC_SEARCH_INDEX_ON_SAVE", False):
        try:
            self._schedule_embedding_update(snippet)
        except Exception:
            pass  # ×œ× × ×›×©×™×œ ×©××™×¨×” ×‘×’×œ×œ embedding
    
    return True

def _schedule_embedding_update(self, snippet: CodeSnippet) -> None:
    """×ª×–××•×Ÿ ×¢×“×›×•×Ÿ embedding ×‘×¨×§×¢."""
    import threading
    
    def _worker():
        try:
            from services.embedding_service import generate_embedding_sync
            
            text = f"File: {snippet.file_name}\n"
            if snippet.programming_language:
                text += f"Language: {snippet.programming_language}\n"
            if snippet.description:
                text += f"Description: {snippet.description}\n"
            text += f"Code:\n{snippet.code[:2000]}"
            
            embedding = generate_embedding_sync(text)
            
            # ×¢×“×›×•×Ÿ ×‘-DB
            from datetime import datetime, timezone
            now = datetime.now(timezone.utc)
            
            self.manager.collection.update_one(
                {"user_id": snippet.user_id, "file_name": snippet.file_name},
                {
                    "$set": {
                        "embedding": embedding,
                        "embedding_model": getattr(config, "EMBEDDING_MODEL", "text-embedding-3-small"),
                        "embedding_updated_at": now,
                        "needs_embedding_update": False,
                    }
                },
                sort=[("version", -1)],
            )
        except Exception as e:
            logger.debug(f"Background embedding update failed: {e}")
    
    threading.Thread(target=_worker, daemon=True).start()
```

---

## ğŸ” ×©×œ×‘ 6: ×¢×“×›×•×Ÿ ×× ×•×¢ ×”×—×™×¤×•×©

### 6.1 ×”×•×¡×¤×ª ×—×™×¤×•×© ×¡×× ×˜×™ ×œ-`search_engine.py`

××¦× ××ª ×”×§×•×‘×¥ `search_engine.py` ×•×”×•×¡×£ ××ª ×”××ª×•×“×” `_semantic_search`:

```python
# ×”×•×¡×£ import ×‘×¨××© ×”×§×•×‘×¥:
from typing import Any, Dict, List, Optional, Set, Tuple, cast

# ××¦× ××ª ×”×¤×•× ×§×¦×™×” search ×•×¢×“×›×Ÿ ××ª ×”-elif:
# (×‘×ª×•×š ×”××ª×•×“×” search ×©×œ AdvancedSearchEngine)

elif search_type == SearchType.SEMANTIC:
    candidates = self._semantic_search(query, user_id, limit)

# ×”×•×¡×£ ××ª ×”××ª×•×“×” ×”×—×“×©×”:
def _semantic_search(
    self,
    query: str,
    user_id: int,
    limit: int = 50,
) -> List[SearchResult]:
    """×—×™×¤×•×© ×¡×× ×˜×™ ×‘×××¦×¢×•×ª embeddings."""
    from config import config
    
    # ×‘×“×™×§×” ×©×”×¤×™×¦'×¨ ××•×¤×¢×œ
    if not getattr(config, "SEMANTIC_SEARCH_ENABLED", False):
        logger.warning("Semantic search is disabled, falling back to text search")
        index = self.get_index(user_id)
        return self._text_search(query, index, user_id)
    
    try:
        # ×™×¦×™×¨×ª embedding ×œ×©××™×œ×ª×”
        import asyncio
        from services.embedding_service import generate_embedding
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as pool:
                    future = pool.submit(asyncio.run, generate_embedding(query))
                    query_embedding = future.result(timeout=15)
            else:
                query_embedding = loop.run_until_complete(generate_embedding(query))
        except Exception:
            query_embedding = asyncio.run(generate_embedding(query))
        
        # ×”×¨×¦×ª Vector Search ×‘-MongoDB
        pipeline = [
            {
                "$vectorSearch": {
                    "index": "code_snippets_vector_index",
                    "path": "embedding",
                    "queryVector": query_embedding,
                    "numCandidates": limit * 10,  # ×™×•×ª×¨ candidates ×œ×“×™×•×§ ×˜×•×‘ ×™×•×ª×¨
                    "limit": limit,
                    "filter": {
                        "user_id": user_id,
                        "is_active": {"$ne": False},
                    }
                }
            },
            {
                "$project": {
                    "_id": 1,
                    "file_name": 1,
                    "code": 1,
                    "programming_language": 1,
                    "tags": 1,
                    "description": 1,
                    "created_at": 1,
                    "updated_at": 1,
                    "version": 1,
                    "score": {"$meta": "vectorSearchScore"},
                }
            }
        ]
        
        results_raw = list(db.manager.collection.aggregate(pipeline))
        
        # ×”××¨×” ×œ-SearchResult
        results: List[SearchResult] = []
        for doc in results_raw:
            score = float(doc.get("score", 0))
            result = self._create_search_result(doc, query, score)
            results.append(result)
        
        try:
            emit_event(
                "semantic_search_done",
                severity="info",
                user_id=int(user_id),
                results_count=int(len(results)),
            )
        except Exception:
            pass
        
        return results
        
    except Exception as e:
        logger.error(f"Semantic search failed: {e}, falling back to fuzzy search")
        try:
            emit_event("semantic_search_error", severity="error", error=str(e))
        except Exception:
            pass
        
        # Fallback ×œ×—×™×¤×•×© fuzzy
        index = self.get_index(user_id)
        return self._fuzzy_search(query, index, user_id)
```

### 6.2 ×”×•×¡×¤×ª ×¤×•× ×§×¦×™×™×ª ×¢×–×¨ ×œ×—×™×¤×•×© ×¡×× ×˜×™ ×‘-repository

×”×•×¡×£ ×œ×§×•×‘×¥ `database/repository.py`:

```python
def semantic_search(
    self,
    user_id: int,
    query_embedding: List[float],
    limit: int = 20,
    programming_language: Optional[str] = None,
) -> List[Dict]:
    """
    ×—×™×¤×•×© ×¡×× ×˜×™ ×‘×××¦×¢×•×ª Vector Search.
    
    Args:
        user_id: ××–×”×” ×”××©×ª××©
        query_embedding: ×•×§×˜×•×¨ ×”-embedding ×©×œ ×”×©××™×œ×ª×”
        limit: ××¡×¤×¨ ×ª×•×¦××•×ª ××§×¡×™××œ×™
        programming_language: ×¡×™× ×•×Ÿ ×œ×¤×™ ×©×¤×” (××•×¤×¦×™×•× ×œ×™)
    
    Returns:
        ×¨×©×™××ª ××¡××›×™× ×××•×™×™× ×™× ×œ×¤×™ similarity score
    """
    try:
        # ×‘× ×™×™×ª ×”×¤×™×œ×˜×¨
        filter_conditions: Dict[str, Any] = {
            "user_id": user_id,
            "is_active": {"$ne": False},
        }
        if programming_language:
            filter_conditions["programming_language"] = programming_language
        
        pipeline = [
            {
                "$vectorSearch": {
                    "index": "code_snippets_vector_index",
                    "path": "embedding",
                    "queryVector": query_embedding,
                    "numCandidates": limit * 10,
                    "limit": limit,
                    "filter": filter_conditions,
                }
            },
            {
                "$project": {
                    "_id": 1,
                    "file_name": 1,
                    "programming_language": 1,
                    "tags": 1,
                    "description": 1,
                    "updated_at": 1,
                    "version": 1,
                    "score": {"$meta": "vectorSearchScore"},
                    # ×œ× ××—×–×™×¨×™× code ×•-embedding - ×©×“×•×ª ×›×‘×“×™×
                }
            }
        ]
        
        with track_performance("db_semantic_search"):
            results = list(self.manager.collection.aggregate(pipeline, allowDiskUse=True))
        
        return results
        
    except Exception as e:
        emit_event("db_semantic_search_error", severity="error", error=str(e))
        return []
```

---

## ğŸŒ ×©×œ×‘ 7: ×¢×“×›×•×Ÿ ×”-API

### 7.1 ×™×¦×™×¨×ª Blueprint ×—×“×© ××• ×¢×“×›×•×Ÿ ×§×™×™×

×¦×•×¨ ×§×•×‘×¥ `webapp/search_api.py`:

```python
"""
API endpoints ×œ×—×™×¤×•×© (×›×•×œ×œ ×¡×× ×˜×™).
Search API Blueprint.
"""

from __future__ import annotations

import logging
from flask import Blueprint, jsonify, request, session
from typing import Any, Dict, List, Optional

from config import config

logger = logging.getLogger(__name__)

search_bp = Blueprint('search_api', __name__, url_prefix='/api/search')


def _parse_int(val: Optional[str], default: int, lo: int, hi: int) -> int:
    """Parse integer with bounds."""
    try:
        v = int(val) if val not in (None, "") else default
        return max(lo, min(hi, v))
    except Exception:
        return default


@search_bp.route('', methods=['GET'])
def search_files():
    """
    ×—×™×¤×•×© ×§×‘×¦×™×.
    
    Query params:
        q: ×©××™×œ×ª×ª ×”×—×™×¤×•×©
        type: ×¡×•×’ ×”×—×™×¤×•×© (text, fuzzy, regex, semantic) - ×‘×¨×™×¨×ª ××—×“×œ: text
        language: ×¡×™× ×•×Ÿ ×œ×¤×™ ×©×¤×ª ×ª×›× ×•×ª
        limit: ××¡×¤×¨ ×ª×•×¦××•×ª ××§×¡×™××œ×™ (×‘×¨×™×¨×ª ××—×“×œ: 20)
    
    Returns:
        JSON ×¢× ×ª×•×¦××•×ª ×”×—×™×¤×•×©
    """
    try:
        user_id = int(session.get('user_id') or 0)
    except Exception:
        user_id = 0
    
    if user_id <= 0:
        return jsonify({"ok": False, "error": "unauthorized"}), 401
    
    # ×¤×¨××˜×¨×™×
    query = request.args.get('q', '').strip()
    search_type = request.args.get('type', 'text').lower()
    language = request.args.get('language')
    limit = _parse_int(request.args.get('limit'), 20, 1, 100)
    
    if not query:
        return jsonify({"ok": True, "items": [], "total": 0})
    
    try:
        from search_engine import search_engine, SearchType, SearchFilter
        
        # ××™×¤×•×™ ×¡×•×’ ×—×™×¤×•×©
        type_map = {
            'text': SearchType.TEXT,
            'fuzzy': SearchType.FUZZY,
            'regex': SearchType.REGEX,
            'semantic': SearchType.SEMANTIC,
            'function': SearchType.FUNCTION,
            'content': SearchType.CONTENT,
        }
        
        st = type_map.get(search_type, SearchType.TEXT)
        
        # ×‘×“×™×§×” ×©×¡×× ×˜×™ ××•×¤×¢×œ
        if st == SearchType.SEMANTIC and not getattr(config, "SEMANTIC_SEARCH_ENABLED", False):
            # Fallback ×œ×—×™×¤×•×© ×¨×’×™×œ
            st = SearchType.FUZZY
        
        # ×”×›× ×ª ×¤×™×œ×˜×¨×™×
        filters = None
        if language:
            filters = SearchFilter(languages=[language])
        
        # ×‘×™×¦×•×¢ ×”×—×™×¤×•×©
        results = search_engine.search(
            user_id=user_id,
            query=query,
            search_type=st,
            filters=filters,
            limit=limit,
        )
        
        # ×”××¨×” ×œ-JSON-serializable
        items = []
        for r in results:
            items.append({
                "file_name": r.file_name,
                "programming_language": r.programming_language,
                "tags": r.tags,
                "updated_at": r.updated_at.isoformat() if r.updated_at else None,
                "score": round(r.relevance_score, 4),
                "snippet_preview": r.snippet_preview[:200] if r.snippet_preview else None,
                "is_semantic": st == SearchType.SEMANTIC,
            })
        
        return jsonify({
            "ok": True,
            "items": items,
            "total": len(items),
            "search_type": search_type,
            "semantic_enabled": getattr(config, "SEMANTIC_SEARCH_ENABLED", False),
        })
        
    except Exception as e:
        logger.error(f"Search API error: {e}", exc_info=True)
        return jsonify({"ok": False, "error": "search_failed"}), 500


@search_bp.route('/suggest', methods=['GET'])
def search_suggestions():
    """×”×¦×¢×•×ª ×œ×”×©×œ××ª ×—×™×¤×•×©."""
    try:
        user_id = int(session.get('user_id') or 0)
    except Exception:
        user_id = 0
    
    if user_id <= 0:
        return jsonify({"ok": False, "suggestions": []})
    
    partial = request.args.get('q', '').strip()
    limit = _parse_int(request.args.get('limit'), 10, 1, 20)
    
    if len(partial) < 2:
        return jsonify({"ok": True, "suggestions": []})
    
    try:
        from search_engine import search_engine
        suggestions = search_engine.suggest_completions(user_id, partial, limit)
        return jsonify({"ok": True, "suggestions": suggestions})
    except Exception as e:
        logger.error(f"Suggestions error: {e}")
        return jsonify({"ok": True, "suggestions": []})


@search_bp.route('/status', methods=['GET'])
def search_status():
    """×¡×˜×˜×•×¡ ××¢×¨×›×ª ×”×—×™×¤×•×© ×”×¡×× ×˜×™."""
    try:
        user_id = int(session.get('user_id') or 0)
    except Exception:
        user_id = 0
    
    if user_id <= 0:
        return jsonify({"ok": False, "error": "unauthorized"}), 401
    
    from database import db
    
    # ×¡×¤×™×¨×ª ×§×‘×¦×™× ×¢×/×‘×œ×™ embedding
    try:
        total = db.manager.collection.count_documents({
            "user_id": user_id,
            "is_active": {"$ne": False},
        })
        indexed = db.manager.collection.count_documents({
            "user_id": user_id,
            "is_active": {"$ne": False},
            "embedding": {"$exists": True, "$ne": None},
        })
    except Exception:
        total, indexed = 0, 0
    
    return jsonify({
        "ok": True,
        "semantic_enabled": getattr(config, "SEMANTIC_SEARCH_ENABLED", False),
        "total_files": total,
        "indexed_files": indexed,
        "indexing_progress": round(indexed / total * 100, 1) if total > 0 else 0,
    })
```

### 7.2 ×¨×™×©×•× ×”-Blueprint ×‘-`webapp/app.py`

××¦× ××ª ×”×§×˜×¢ ×©××¨×©× blueprints ×•×”×•×¡×£:

```python
# ×”×•×¡×£ ××ª ×”-import:
from webapp.search_api import search_bp

# ×•×¨×©×•× ××ª ×”-Blueprint:
app.register_blueprint(search_bp)
```

---

## ğŸ¨ ×©×œ×‘ 8: ×¢×“×›×•×Ÿ ×”-Frontend

### 8.1 ×”×•×¡×¤×ª Toggle ×œ×—×™×¤×•×© ×¡×× ×˜×™

×¢×“×›×Ÿ ××ª ×ª×‘× ×™×ª ×”×—×™×¤×•×© (×œ××©×œ `webapp/templates/files.html` ××• `dashboard.html`):

```html
<!-- ×‘×ª×•×š ×˜×•×¤×¡ ×”×—×™×¤×•×© -->
<div class="search-container">
    <input type="text" 
           id="search-input" 
           class="search-input" 
           placeholder="×—×™×¤×•×© ×§×‘×¦×™×..."
           autocomplete="off">
    
    <!-- Toggle ×—×™×¤×•×© ×¡×× ×˜×™ -->
    <div class="semantic-toggle" id="semantic-toggle">
        <label class="toggle-label">
            <input type="checkbox" 
                   id="semantic-checkbox" 
                   {% if semantic_enabled %}{% else %}disabled{% endif %}>
            <span class="toggle-slider"></span>
            <span class="toggle-text">ğŸ¤– ×—×™×¤×•×© ×—×›×</span>
        </label>
        {% if not semantic_enabled %}
        <span class="toggle-hint">(×œ× ×–××™×Ÿ)</span>
        {% endif %}
    </div>
    
    <button type="submit" class="search-button">ğŸ”</button>
</div>

<!-- CSS -->
<style>
.semantic-toggle {
    display: flex;
    align-items: center;
    gap: 8px;
    margin-left: 12px;
}

.toggle-label {
    display: flex;
    align-items: center;
    cursor: pointer;
    gap: 6px;
}

.toggle-label input {
    display: none;
}

.toggle-slider {
    width: 36px;
    height: 20px;
    background: #ccc;
    border-radius: 10px;
    position: relative;
    transition: background 0.3s;
}

.toggle-slider::after {
    content: '';
    width: 16px;
    height: 16px;
    background: white;
    border-radius: 50%;
    position: absolute;
    top: 2px;
    left: 2px;
    transition: transform 0.3s;
}

.toggle-label input:checked + .toggle-slider {
    background: #4CAF50;
}

.toggle-label input:checked + .toggle-slider::after {
    transform: translateX(16px);
}

.toggle-text {
    font-size: 0.9em;
    color: #666;
}

.toggle-hint {
    font-size: 0.75em;
    color: #999;
}

/* ×¡×™××•×Ÿ ×ª×•×¦××•×ª ×¡×× ×˜×™×•×ª */
.search-result.semantic::before {
    content: 'ğŸ§ ';
    margin-left: 4px;
}
</style>
```

### 8.2 JavaScript ×œ×—×™×¤×•×©

```javascript
// search.js ××• ×‘×ª×•×š ×”×ª×‘× ×™×ª

document.addEventListener('DOMContentLoaded', function() {
    const searchInput = document.getElementById('search-input');
    const semanticCheckbox = document.getElementById('semantic-checkbox');
    const resultsContainer = document.getElementById('search-results');
    
    let searchTimeout = null;
    
    // ×—×™×¤×•×© ×¢× debounce
    searchInput.addEventListener('input', function() {
        clearTimeout(searchTimeout);
        searchTimeout = setTimeout(() => performSearch(), 300);
    });
    
    async function performSearch() {
        const query = searchInput.value.trim();
        if (!query) {
            resultsContainer.innerHTML = '';
            return;
        }
        
        const searchType = semanticCheckbox?.checked ? 'semantic' : 'text';
        
        try {
            const response = await fetch(
                `/api/search?q=${encodeURIComponent(query)}&type=${searchType}&limit=20`
            );
            const data = await response.json();
            
            if (data.ok) {
                renderResults(data.items, searchType === 'semantic');
            }
        } catch (error) {
            console.error('Search error:', error);
        }
    }
    
    function renderResults(items, isSemantic) {
        if (!items.length) {
            resultsContainer.innerHTML = '<p class="no-results">×œ× × ××¦××• ×ª×•×¦××•×ª</p>';
            return;
        }
        
        const html = items.map(item => `
            <div class="search-result ${isSemantic ? 'semantic' : ''}">
                <a href="/file/${encodeURIComponent(item.file_name)}">
                    ${escapeHtml(item.file_name)}
                </a>
                <span class="result-meta">
                    ${item.programming_language || ''}
                    ${item.score ? `(${(item.score * 100).toFixed(0)}%)` : ''}
                </span>
                ${item.snippet_preview ? `
                    <div class="snippet-preview">${escapeHtml(item.snippet_preview)}</div>
                ` : ''}
            </div>
        `).join('');
        
        resultsContainer.innerHTML = html;
    }
    
    function escapeHtml(str) {
        const div = document.createElement('div');
        div.textContent = str;
        return div.innerHTML;
    }
});
```

---

## ğŸ’¡ ×©×™×¤×•×¨×™× ××•××œ×¦×™× (Nice to Have)

### 9.1 Hybrid Search (×—×™×¤×•×© ×”×™×‘×¨×™×“×™)

×‘××§×•× Toggle ×‘×™×Ÿ ×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×œ×¡×× ×˜×™, × ×™×ª×Ÿ ×œ×”×¨×™×¥ ××ª ×©× ×™×”× ×•×œ×©×œ×‘ ×ª×•×¦××•×ª:

```python
def _hybrid_search(
    self,
    query: str,
    user_id: int,
    limit: int = 50,
    semantic_weight: float = 0.6,  # ××©×§×œ ×œ×ª×•×¦××•×ª ×¡×× ×˜×™×•×ª
) -> List[SearchResult]:
    """
    ×—×™×¤×•×© ×”×™×‘×¨×™×“×™: ×©×™×œ×•×‘ ×ª×•×¦××•×ª ×˜×§×¡×˜×•××œ×™×•×ª ×•×¡×× ×˜×™×•×ª.
    ××—×–×™×¨ ×ª×•×¦××•×ª ×˜×•×‘×•×ª ×™×•×ª×¨ ××©×ª×™ ×”×©×™×˜×•×ª ×‘× ×¤×¨×“.
    """
    index = self.get_index(user_id)
    
    # ×”×¨×¦×ª ×©× ×™ ×¡×•×’×™ ×”×—×™×¤×•×©
    text_results = self._text_search(query, index, user_id)
    semantic_results = self._semantic_search(query, user_id, limit * 2)
    
    # × ×¨××•×œ ×¦×™×•× ×™× ×œ×˜×•×•×— 0-1
    def _normalize_scores(results: List[SearchResult]) -> Dict[str, float]:
        if not results:
            return {}
        scores = [r.relevance_score for r in results]
        min_score, max_score = min(scores), max(scores)
        score_range = max_score - min_score if max_score > min_score else 1.0
        return {
            r.file_name: (r.relevance_score - min_score) / score_range
            for r in results
        }
    
    text_scores = _normalize_scores(text_results)
    semantic_scores = _normalize_scores(semantic_results)
    
    # ×©×™×œ×•×‘ ×¦×™×•× ×™× ×¢× ××©×§×œ×•×ª
    text_weight = 1.0 - semantic_weight
    combined_scores: Dict[str, float] = {}
    all_files = set(text_scores.keys()) | set(semantic_scores.keys())
    
    for file_name in all_files:
        text_score = text_scores.get(file_name, 0.0)
        semantic_score = semantic_scores.get(file_name, 0.0)
        combined_scores[file_name] = (
            text_weight * text_score + 
            semantic_weight * semantic_score
        )
    
    # ××™×•×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ ××©×•×œ×‘
    sorted_files = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
    
    # ×‘× ×™×™×ª ×ª×•×¦××•×ª
    results_map = {r.file_name: r for r in text_results + semantic_results}
    final_results = []
    for file_name, score in sorted_files[:limit]:
        if file_name in results_map:
            result = results_map[file_name]
            result.relevance_score = score
            final_results.append(result)
    
    return final_results
```

**×™×ª×¨×•× ×•×ª:**
- ××•×¦× ×ª×•×¦××•×ª ×’× ×œ×¤×™ ××™×œ×™× ××“×•×™×§×•×ª ×•×’× ×œ×¤×™ ××©××¢×•×ª
- ××¤×—×™×ª False Negatives
- ×—×•×•×™×ª ××©×ª××© ×˜×•×‘×” ×™×•×ª×¨

### 9.2 ×—×™×ª×•×š ×œ×¤×™ Tokens (TikToken)

×‘××§×•× ×œ×—×ª×•×š ×œ×¤×™ ×ª×•×•×™× (×©×¢×œ×•×œ ×œ×”×™×•×ª ××’×¨×¡×™×‘×™ ××“×™ ×‘×¢×‘×¨×™×ª), ×”×©×ª××© ×‘-tiktoken:

```python
# ×”×•×¡×£ ×œ-embedding_service.py

try:
    import tiktoken
    _TOKENIZER = tiktoken.encoding_for_model("text-embedding-3-small")
except Exception:
    _TOKENIZER = None


def _truncate_by_tokens(text: str, max_tokens: int = 8000) -> str:
    """
    ×—×™×ª×•×š ×˜×§×¡×˜ ×œ×¤×™ ××¡×¤×¨ tokens (×œ× ×ª×•×•×™×).
    ××“×•×™×§ ×™×•×ª×¨ ×•××•× ×¢ ×—×™×ª×•×š ××’×¨×¡×™×‘×™ ××“×™.
    
    ×”×¢×¨×”: text-embedding-3-small ××§×‘×œ ×¢×“ 8191 tokens.
    """
    if _TOKENIZER is None:
        # Fallback ×œ×—×™×ª×•×š ×œ×¤×™ ×ª×•×•×™× (×¤×—×•×ª ××“×•×™×§)
        return _truncate_text(text, max_tokens * 4)  # ~4 chars/token average
    
    try:
        tokens = _TOKENIZER.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        # ×—×™×ª×•×š ×•× ×™×¡×™×•×Ÿ ×œ×¡×™×™× ×‘××™×œ×” ×©×œ××”
        truncated_tokens = tokens[:max_tokens]
        truncated_text = _TOKENIZER.decode(truncated_tokens)
        
        # × ×¡×” ×œ×—×ª×•×š ×‘×¡×•×£ ××©×¤×˜
        for sep in ["\n\n", "\n", ". "]:
            last_sep = truncated_text.rfind(sep)
            if last_sep > len(truncated_text) * 0.7:
                return truncated_text[:last_sep + len(sep)].strip()
        
        return truncated_text.strip()
        
    except Exception:
        return _truncate_text(text, max_tokens * 4)


# ×©×™××•×© ×‘-_prepare_text_for_embedding:
def _prepare_text_for_embedding(...) -> str:
    # ... ×‘× ×™×™×ª parts ...
    
    full_text = "\n".join(parts)
    
    # ×—×™×ª×•×š ×œ×¤×™ tokens (×œ× ×ª×•×•×™×!)
    max_tokens = int(getattr(config, "EMBEDDING_MAX_TOKENS", 7500) or 7500)
    return _truncate_by_tokens(full_text, max_tokens)
```

**×œ××” ×–×” ×—×©×•×‘?**
- 2000 ×ª×•×•×™× ×‘×¢×‘×¨×™×ª â‰ˆ 4000+ tokens (×›×œ ××•×ª ×¢×‘×¨×™×ª = ~2 tokens)
- 2000 ×ª×•×•×™× ×‘×× ×’×œ×™×ª â‰ˆ 500 tokens
- ×—×™×ª×•×š ×œ×¤×™ ×ª×•×•×™× ×¢×œ×•×œ ×œ×—×ª×•×š ×™×•×ª×¨ ××“×™ ××ª×•×›×Ÿ ×‘×× ×’×œ×™×ª

### 9.3 Message Queue ×œ××™× ×“×•×§×¡ (Production)

×œ×¡×‘×™×‘×•×ª Production ×¢××•×¡×•×ª, ×”×—×œ×£ ××ª ×”-Thread ×‘-Task Queue:

```python
# ××•×¤×¦×™×” 1: Redis Queue (×¤×©×•×˜)
# requirements: rq, redis

from rq import Queue
from redis import Redis

redis_conn = Redis()
embedding_queue = Queue('embeddings', connection=redis_conn)

def _schedule_embedding_update(self, snippet: CodeSnippet) -> None:
    """×ª×–××•×Ÿ ×¢×“×›×•×Ÿ embedding ×“×¨×š Redis Queue."""
    from tasks.embedding_tasks import update_embedding_task
    
    embedding_queue.enqueue(
        update_embedding_task,
        snippet.user_id,
        snippet.file_name,
        job_timeout='5m',
        retry=3,
    )


# tasks/embedding_tasks.py
def update_embedding_task(user_id: int, file_name: str) -> bool:
    """Task ×œ×¢×“×›×•×Ÿ embedding ×©×œ ×§×•×‘×¥."""
    from database import db
    from services.embedding_service import generate_embedding_sync
    
    file_data = db.get_latest_version(user_id, file_name)
    if not file_data:
        return False
    
    # ... ×œ×•×’×™×§×ª ×™×¦×™×¨×ª embedding ×•×©××™×¨×” ...
    return True
```

**××ª×™ ×œ×”×©×ª××© ×‘-Queue?**
- ×™×•×ª×¨ ×-100 ×©××™×¨×•×ª ×‘×“×§×”
- Gunicorn ×¢× workers ×¨×‘×™×
- ×¦×•×¨×š ×‘-retries ×××™× ×™×
- × ×™×˜×•×¨ ×•×“×©×‘×•×¨×“ ×©×œ ××©×™××•×ª

### 9.4 Batch Embedding API

×œ××™× ×“×•×§×¡ ××¡×™×‘×™, ×”×©×ª××© ×‘-Batch API ×©×œ OpenAI (×—×•×¡×š ×¢×“ 50% ×‘×¢×œ×•×™×•×ª):

```python
async def generate_embeddings_batch(
    texts: List[str],
    *,
    timeout: float = 30.0,
) -> List[List[float]]:
    """
    ×™×¦×™×¨×ª embeddings ×œ-batch ×©×œ ×˜×§×¡×˜×™×.
    ×™×¢×™×œ ×™×•×ª×¨ ××§×¨×™××•×ª ×‘×•×“×“×•×ª (×¢×“ 2048 ×˜×§×¡×˜×™× ×‘×‘×§×©×” ××—×ª).
    """
    api_key = _get_api_key()
    model = _get_model()
    dimensions = _get_dimensions()
    
    # OpenAI ×××¤×©×¨ ×¢×“ 2048 inputs ×‘×‘×§×©×” ××—×ª
    MAX_BATCH = 2048
    
    all_embeddings: List[List[float]] = []
    
    for i in range(0, len(texts), MAX_BATCH):
        batch = texts[i:i + MAX_BATCH]
        
        payload = {
            "model": model,
            "input": [t.strip() for t in batch],
            "dimensions": dimensions,
        }
        
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                _OPENAI_API_URL,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json=payload,
            )
            response.raise_for_status()
            data = response.json()
        
        # ×©××™×¨×” ×¢×œ ×¡×“×¨ (OpenAI ××—×–×™×¨ index)
        batch_embeddings = [None] * len(batch)
        for item in data["data"]:
            batch_embeddings[item["index"]] = item["embedding"]
        
        all_embeddings.extend(batch_embeddings)
    
    return all_embeddings
```

---

## ğŸ§ª ×‘×“×™×§×•×ª

### 9.1 ×™×¦×™×¨×ª ×˜×¡×˜×™× ×œ×©×™×¨×•×ª Embeddings

×¦×•×¨ ×§×•×‘×¥ `tests/test_embedding_service.py`:

```python
"""
×‘×“×™×§×•×ª ×œ×©×™×¨×•×ª Embeddings.
"""

import pytest
from unittest.mock import AsyncMock, patch, MagicMock
import asyncio


@pytest.fixture
def mock_openai_response():
    """Mock response from OpenAI API."""
    return {
        "data": [
            {
                "embedding": [0.1] * 1536,
                "index": 0,
            }
        ],
        "model": "text-embedding-3-small",
        "usage": {"prompt_tokens": 10, "total_tokens": 10},
    }


class TestEmbeddingService:
    """×‘×“×™×§×•×ª ×œ×©×™×¨×•×ª Embeddings."""
    
    @pytest.mark.asyncio
    async def test_generate_embedding_success(self, mock_openai_response):
        """×‘×“×™×§×ª ×™×¦×™×¨×ª embedding ×‘×”×¦×œ×—×”."""
        with patch("services.embedding_service._get_api_key", return_value="test-key"):
            with patch("httpx.AsyncClient") as mock_client:
                mock_response = MagicMock()
                mock_response.json.return_value = mock_openai_response
                mock_response.raise_for_status = MagicMock()
                
                mock_client.return_value.__aenter__.return_value.post = AsyncMock(
                    return_value=mock_response
                )
                
                from services.embedding_service import generate_embedding
                
                result = await generate_embedding("test text")
                
                assert isinstance(result, list)
                assert len(result) == 1536
                assert all(isinstance(x, float) for x in result)
    
    @pytest.mark.asyncio
    async def test_generate_embedding_empty_text(self):
        """×‘×“×™×§×ª ×©×’×™××” ×¢×œ ×˜×§×¡×˜ ×¨×™×§."""
        from services.embedding_service import generate_embedding, EmbeddingError
        
        with pytest.raises(EmbeddingError, match="empty_text"):
            await generate_embedding("")
    
    @pytest.mark.asyncio
    async def test_generate_embedding_no_api_key(self):
        """×‘×“×™×§×ª ×©×’×™××” ×›×©××™×Ÿ API key."""
        with patch("services.embedding_service._get_api_key", return_value=""):
            from services.embedding_service import generate_embedding, EmbeddingError
            
            with pytest.raises(EmbeddingError, match="api_key_missing"):
                await generate_embedding("test")
    
    def test_prepare_text_truncation(self):
        """×‘×“×™×§×ª ×§×™×¦×•×¨ ×˜×§×¡×˜ ××¨×•×š."""
        from services.embedding_service import _truncate_text
        
        long_text = "x" * 5000
        result = _truncate_text(long_text, 2000)
        
        assert len(result) <= 2000
    
    def test_prepare_text_for_embedding(self):
        """×‘×“×™×§×ª ×”×›× ×ª ×˜×§×¡×˜ ×¢× ××˜×-×“××˜×”."""
        from services.embedding_service import _prepare_text_for_embedding
        
        result = _prepare_text_for_embedding(
            code="print('hello')",
            file_name="test.py",
            description="A test file",
            tags=["python", "test"],
            programming_language="python",
        )
        
        assert "File: test.py" in result
        assert "Language: python" in result
        assert "Description: A test file" in result
        assert "print('hello')" in result
```

### 9.2 ×‘×“×™×§×•×ª ××™× ×˜×’×¨×¦×™×”

×¦×•×¨ ×§×•×‘×¥ `tests/test_semantic_search_integration.py`:

```python
"""
×‘×“×™×§×•×ª ××™× ×˜×’×¨×¦×™×” ×œ×—×™×¤×•×© ×¡×× ×˜×™.
"""

import pytest
from unittest.mock import patch, MagicMock


class TestSemanticSearchIntegration:
    """×‘×“×™×§×•×ª ××™× ×˜×’×¨×¦×™×” ×œ×—×™×¤×•×© ×¡×× ×˜×™."""
    
    @pytest.fixture
    def mock_embedding(self):
        """Mock embedding vector."""
        return [0.1] * 1536
    
    def test_search_api_semantic_type(self, client, mock_embedding):
        """×‘×“×™×§×ª API ×¢× ×¡×•×’ ×—×™×¤×•×© ×¡×× ×˜×™."""
        with patch("search_engine.search_engine.search") as mock_search:
            mock_search.return_value = []
            
            response = client.get("/api/search?q=test&type=semantic")
            
            assert response.status_code in (200, 401)  # ×ª×œ×•×™ ×‘××™××•×ª
    
    def test_semantic_search_fallback(self, mock_embedding):
        """×‘×“×™×§×ª fallback ×›×©×¡×× ×˜×™ ××•×©×‘×ª."""
        with patch("config.config.SEMANTIC_SEARCH_ENABLED", False):
            from search_engine import search_engine, SearchType
            
            # ×¦×¨×™×š ×œ×¢×©×•×ª fallback ×œ×—×™×¤×•×© ××—×¨
            # (×”×‘×“×™×§×” ×”××œ××” ×ª×œ×•×™×” ×‘××™××•×©)
    
    def test_embedding_stored_on_save(self, db_fixture, mock_embedding):
        """×‘×“×™×§×” ×©embedding × ×©××¨ ×‘×¢×ª ×©××™×¨×ª ×§×•×‘×¥."""
        with patch("services.embedding_service.generate_embedding_sync", return_value=mock_embedding):
            with patch("config.config.SEMANTIC_SEARCH_INDEX_ON_SAVE", True):
                # ×©××™×¨×ª ×§×•×‘×¥
                # ×‘×“×™×§×” ×©×”-embedding × ×©××¨
                pass
```

---

## ğŸ“š × ×¡×¤×—×™×

### × ×¡×¤×— ×': ×¢×œ×•×™×•×ª ××©×•×¢×¨×•×ª

| ×¤×¢×•×œ×” | ×¢×œ×•×ª ××©×•×¢×¨×ª (OpenAI) |
|-------|---------------------|
| ××™× ×“×•×§×¡ 1,000 ×§×‘×¦×™× | ~$0.02 |
| ××™× ×“×•×§×¡ 10,000 ×§×‘×¦×™× | ~$0.20 |
| ×—×™×¤×•×© ×™×—×™×“ | ~$0.00001 |
| 1,000 ×—×™×¤×•×©×™× | ~$0.01 |

**×”×¢×¨×”:** ×”×¢×œ×•×™×•×ª ××‘×•×¡×¡×•×ª ×¢×œ `text-embedding-3-small` ×‘××—×™×¨×™ ×“×¦××‘×¨ 2025.

### × ×¡×¤×— ×‘': ××•×“×œ×™× ×—×œ×•×¤×™×™×

| ××•×“×œ | ××™××“×™× | ×™×ª×¨×•× ×•×ª | ×—×¡×¨×•× ×•×ª |
|------|--------|---------|---------|
| `text-embedding-3-small` | 1536 | ×–×•×œ, ××”×™×¨ | ×“×™×•×§ ×‘×™× ×•× ×™ |
| `text-embedding-3-large` | 3072 | ×“×™×•×§ ×’×‘×•×” | ×™×§×¨ ×™×•×ª×¨ |
| `text-embedding-ada-002` | 1536 | ×™×¦×™×‘ | ×“×•×¨ ×§×•×“× |

### × ×¡×¤×— ×’': Troubleshooting

| ×‘×¢×™×” | ×¤×ª×¨×•×Ÿ |
|------|-------|
| "vectorSearch index not found" | ×¦×•×¨ ××ª ×”××™× ×“×§×¡ ×“×¨×š Atlas UI |
| ×—×™×¤×•×© ×¡×× ×˜×™ ×œ× ××—×–×™×¨ ×ª×•×¦××•×ª | ×‘×“×•×§ ×©-embeddings ×§×™×™××™× ×‘-DB |
| ×©×’×™××ª timeout | ×”×’×“×œ ××ª `EMBEDDING_TIMEOUT` |
| "api_key_missing" | ×”×’×“×¨ `OPENAI_API_KEY` ×‘-ENV |
| ×§×•×‘×¥ ×©× ××—×§ ×¢×“×™×™×Ÿ ××•×¤×™×¢ ×‘×—×™×¤×•×© | ×•×“× ×©×”×¤×™×œ×˜×¨ `is_active: {$ne: False}` ×§×™×™× |
| ×¢×“×›×•×Ÿ ×ª×™××•×¨/×ª×’×™×•×ª ×œ× ××©× ×” ×ª×•×¦××•×ª | ×•×“× ×©×”×ª×™×§×•×Ÿ ×‘×¡×¢×™×£ 3.3 ××™×•×©× (×‘×“×™×§×ª ×›×œ ×”×©×“×•×ª) |

### × ×¡×¤×— ×“': ××—×™×§×ª ×§×‘×¦×™× ×•-Embedding

×›××©×¨ ×§×•×‘×¥ × ××—×§ (soft delete), ×”-Embedding ×©×œ×• × ×©××¨ ×‘-DB ××š ×œ× ××•×—×–×¨ ×‘×—×™×¤×•×© ×‘×–×›×•×ª ×”×¤×™×œ×˜×¨:

```python
# ×‘×©××™×œ×ª×ª vectorSearch:
"filter": {
    "user_id": user_id,
    "is_active": {"$ne": False},  # â¬…ï¸ ××¡× ×Ÿ ×§×‘×¦×™× ××—×•×§×™×
}
```

**××™××•×ª:** ×”×§×•×“ ×”×§×™×™× ×‘-`repository.py` ×›×‘×¨ ××©×ª××© ×‘-`is_active: False` ×œ××—×™×§×” ×¨×›×”, ×•×”×¤×™×œ×˜×¨ ×‘×—×™×¤×•×© ×”×¡×× ×˜×™ ××›×¡×” ××ª ×–×”.

**××•×¤×¦×™×•× ×œ×™ - × ×™×§×•×™ Embeddings ×™×©× ×™×:**
```python
# scripts/cleanup_deleted_embeddings.py
def cleanup_old_embeddings():
    """××—×™×§×ª embeddings ×©×œ ×§×‘×¦×™× ×©× ××—×§×• ×œ×¤× ×™ ×™×•×ª×¨ ×-30 ×™×•×."""
    from datetime import datetime, timedelta, timezone
    
    cutoff = datetime.now(timezone.utc) - timedelta(days=30)
    
    result = db.manager.collection.update_many(
        {
            "is_active": False,
            "deleted_at": {"$lt": cutoff},
            "embedding": {"$exists": True},
        },
        {
            "$unset": {"embedding": "", "embedding_model": "", "embedding_updated_at": ""}
        }
    )
    
    print(f"Cleaned {result.modified_count} old embeddings")
```

### × ×¡×¤×— ×“': ×§×‘×¦×™× ×©× ×•×¦×¨×•/×¢×•×“×›× ×•

**×§×‘×¦×™× ×—×“×©×™×:**
- `services/embedding_service.py`
- `webapp/search_api.py`
- `scripts/index_embeddings.py`
- `scripts/create_vector_index.py`
- `tests/test_embedding_service.py`
- `tests/test_semantic_search_integration.py`

**×§×‘×¦×™× ××¢×•×“×›× ×™×:**
- `config.py` - ×”×•×¡×¤×ª ×”×’×“×¨×•×ª
- `database/models.py` - ×”×•×¡×¤×ª ×©×“×•×ª embedding
- `database/repository.py` - ×¢×“×›×•×Ÿ projection ×•×”×•×¡×¤×ª semantic_search
- `search_engine.py` - ×”×•×¡×¤×ª _semantic_search
- `webapp/app.py` - ×¨×™×©×•× search_bp
- `requirements/base.txt` - ×”×•×¡×¤×ª openai

---

## âœ… Checklist ×œ××™××•×©

### ×©×œ×‘ ×‘×¡×™×¡×™ (MVP)
- [ ] ×”×’×“×¨×ª `OPENAI_API_KEY` ×‘-ENV
- [ ] ×”×•×¡×¤×ª ×”×’×“×¨×•×ª ×œ-`config.py`
- [ ] ×™×¦×™×¨×ª `services/embedding_service.py`
- [ ] ×¢×“×›×•×Ÿ `database/models.py` ×¢× ×©×“×•×ª embedding
- [ ] ×¢×“×›×•×Ÿ `database/repository.py` **(×›×•×œ×œ ×‘×“×™×§×ª ×©×™× ×•×™ ×‘×›×œ ×”×©×“×•×ª!)**
- [ ] ×™×¦×™×¨×ª Vector Index ×‘-MongoDB Atlas
- [ ] ×”×•×¡×¤×ª `_semantic_search` ×œ-`search_engine.py`
- [ ] ×™×¦×™×¨×ª `webapp/search_api.py`
- [ ] ×¢×“×›×•×Ÿ Frontend ×¢× toggle
- [ ] ×”×¨×¦×ª `scripts/index_embeddings.py` ×œ××™× ×“×•×§×¡ ×§×‘×¦×™× ×§×™×™××™×
- [ ] ×›×ª×™×‘×ª ×˜×¡×˜×™×
- [ ] ×‘×“×™×§×ª E2E

### ×©×™×¤×•×¨×™× ××•××œ×¦×™× (××—×¨×™ MVP)
- [ ] ğŸ”€ Hybrid Search - ×©×™×œ×•×‘ ×ª×•×¦××•×ª ×˜×§×¡×˜ + ×¡×× ×˜×™
- [ ] ğŸ“Š TikToken - ×—×™×ª×•×š ×œ×¤×™ tokens ×‘××§×•× ×ª×•×•×™×
- [ ] ğŸ“¦ Batch Embedding - ××™× ×“×•×§×¡ ×™×¢×™×œ ×™×•×ª×¨
- [ ] ğŸ”„ Message Queue - ×œ×¡×‘×™×‘×•×ª Production ×¢××•×¡×•×ª
- [ ] ğŸ§¹ Cleanup Job - × ×™×§×•×™ embeddings ×©×œ ×§×‘×¦×™× ××—×•×§×™×

---

**× ×›×ª×‘ ×¢"×™:** CodeBot Assistant  
**×ª××¨×™×š ×¢×“×›×•×Ÿ ××—×¨×•×Ÿ:** ×“×¦××‘×¨ 2025
