"""
×× ×•×¢ ×¨×¤×§×˜×•×¨×™× ×’ ××•×˜×•××˜×™
××‘×¦×¢ ×©×™× ×•×™×™ ××‘× ×” ×‘×§×•×“ ×‘×¦×•×¨×” ×‘×˜×•×—×”
"""

from __future__ import annotations

import ast
import re
import logging
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Set, Tuple, Any
from pathlib import Path

logger = logging.getLogger(__name__)


class RefactorType(Enum):
    """×¡×•×’×™ ×¨×¤×§×˜×•×¨×™× ×’ × ×ª××›×™×"""
    SPLIT_FUNCTIONS = "split_functions"  # ×¤×™×¦×•×œ ×§×•×‘×¥ ×’×“×•×œ ×œ×¤×•× ×§×¦×™×•×ª
    EXTRACT_FUNCTIONS = "extract_functions"  # ×—×™×œ×•×¥ ×§×•×“ ×—×•×–×¨
    MERGE_SIMILAR = "merge_similar"  # ××™×–×•×’ ×§×•×“ ×“×•××”
    CONVERT_TO_CLASSES = "convert_to_classes"  # ×”××¨×” ×œ××—×œ×§×•×ª
    DEPENDENCY_INJECTION = "dependency_injection"  # DI


@dataclass
class FunctionInfo:
    """××™×“×¢ ×¢×œ ×¤×•× ×§×¦×™×”"""
    name: str
    start_line: int
    end_line: int
    args: List[str]
    returns: Optional[str]
    decorators: List[str]
    docstring: Optional[str]
    calls: Set[str]
    called_by: Set[str] = field(default_factory=set)
    code: str = ""
    complexity: int = 0
    # ×©×™×•×š ×œ×¤×™ ×¡×¢×™×£ (Section) ×× ×–×•×”×” ××”×§×•×‘×¥ ×”××•× ×•×œ×™×ª×™
    section: Optional[str] = None


@dataclass
class ClassInfo:
    """××™×“×¢ ×¢×œ ××—×œ×§×”"""
    name: str
    start_line: int
    end_line: int
    methods: List[FunctionInfo]
    attributes: List[str]
    base_classes: List[str]
    decorators: List[str]
    docstring: Optional[str]
    # ×§×•×“ ×”××§×•×¨ ×©×œ ×”××—×œ×§×”
    code: str = ""
    # ×©×™×•×š ×œ×¤×™ ×¡×¢×™×£ (Section) ×× ×–×•×”×”
    section: Optional[str] = None


@dataclass
class RefactorProposal:
    """×”×¦×¢×ª ×¨×¤×§×˜×•×¨×™× ×’"""
    refactor_type: RefactorType
    original_file: str
    new_files: Dict[str, str]
    description: str
    changes_summary: List[str]
    warnings: List[str] = field(default_factory=list)
    imports_needed: Dict[str, List[str]] = field(default_factory=dict)


@dataclass
class RefactorResult:
    """×ª×•×¦××ª ×¨×¤×§×˜×•×¨×™× ×’"""
    success: bool
    proposal: Optional[RefactorProposal]
    error: Optional[str] = None
    validation_passed: bool = False


FunctionNode = ast.FunctionDef | ast.AsyncFunctionDef


class CodeAnalyzer:
    """×× ×ª×— ×§×•×“ Python"""

    def __init__(self, code: str, filename: str = "unknown.py"):
        self.code = code
        self.filename = filename
        self.tree: Optional[ast.AST] = None
        self.functions: List[FunctionInfo] = []
        self.classes: List[ClassInfo] = []
        self.imports: List[str] = []
        self.global_vars: List[str] = []
        # ××™×¤×•×™ ×©×•×¨×” -> slug ×©×œ ×¡×¢×™×£, ×œ×¤×™ ×›×•×ª×¨×•×ª ×‘×§×•×‘×¥
        self._sections_by_line: Dict[int, str] = {}

    def analyze(self) -> bool:
        """× ×™×ª×•×— ×”×§×•×“"""
        try:
            self.tree = ast.parse(self.code)
            self._extract_sections()
            self._extract_imports()
            self._extract_functions()
            self._extract_classes()
            self._extract_globals()
            self._calculate_dependencies()
            return True
        except SyntaxError as e:
            logger.error(f"×©×’×™××ª ×ª×—×‘×™×¨ ×‘×§×•×“: {e}")
            return False
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘× ×™×ª×•×—: {e}", exc_info=True)
            return False

    def _extract_imports(self) -> None:
        for node in ast.walk(self.tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    self.imports.append(f"import {alias.name}")
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                names = ", ".join(alias.name for alias in node.names)
                self.imports.append(f"from {module} import {names}")

    def _extract_functions(self) -> None:
        for node in ast.walk(self.tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) and self._is_top_level_function(node):
                func_info = self._parse_function(node)
                # ×©×™×•×š ×œ×¡×¢×™×£ ×œ×¤×™ ××™×§×•× ×”×©×•×¨×” ×‘×§×•×“
                func_info.section = self._get_section_for_line(func_info.start_line)
                self.functions.append(func_info)

    def _extract_classes(self) -> None:
        for node in ast.walk(self.tree):
            if isinstance(node, ast.ClassDef):
                class_info = self._parse_class(node)
                class_info.section = self._get_section_for_line(class_info.start_line)
                self.classes.append(class_info)

    def _extract_globals(self) -> None:
        for node in self.tree.body:
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        self.global_vars.append(target.id)

    def _parse_function(self, node: FunctionNode) -> FunctionInfo:
        args = [arg.arg for arg in node.args.args]
        returns = ast.unparse(node.returns) if node.returns else None
        decorators = [ast.unparse(dec) for dec in node.decorator_list]
        docstring = ast.get_docstring(node)
        calls = self._extract_function_calls(node)
        code_lines = self.code.splitlines()[node.lineno - 1 : node.end_lineno]
        code = "\n".join(code_lines)
        complexity = self._calculate_complexity(node)
        return FunctionInfo(
            name=node.name,
            start_line=node.lineno,
            end_line=node.end_lineno or node.lineno,
            args=args,
            returns=returns,
            decorators=decorators,
            docstring=docstring,
            calls=calls,
            code=code,
            complexity=complexity,
        )

    def _parse_class(self, node: ast.ClassDef) -> ClassInfo:
        methods: List[FunctionInfo] = []
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                methods.append(self._parse_function(item))
        attributes: List[str] = []
        for item in node.body:
            if isinstance(item, ast.Assign):
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        attributes.append(target.id)
        base_classes = [ast.unparse(base) for base in node.bases]
        decorators = [ast.unparse(dec) for dec in node.decorator_list]
        docstring = ast.get_docstring(node)
        code_lines = self.code.splitlines()[node.lineno - 1 : node.end_lineno]
        code = "\n".join(code_lines)
        return ClassInfo(
            name=node.name,
            start_line=node.lineno,
            end_line=node.end_lineno or node.lineno,
            methods=methods,
            attributes=attributes,
            base_classes=base_classes,
            decorators=decorators,
            docstring=docstring,
            code=code,
        )

    def _extract_function_calls(self, node: ast.AST) -> Set[str]:
        calls: Set[str] = set()
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.add(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.add(child.func.attr)
        return calls

    def _calculate_complexity(self, node: ast.AST) -> int:
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(child, ast.BoolOp) and isinstance(child.op, (ast.And, ast.Or)):
                complexity += len(child.values) - 1
        return complexity

    def _is_top_level_function(self, node: FunctionNode) -> bool:
        for parent in ast.walk(self.tree):
            if isinstance(parent, ast.ClassDef):
                if node in parent.body:
                    return False
        return True

    # --- ×–×™×”×•×™ ×›×•×ª×¨×•×ª/×¡×¢×™×¤×™× ×‘×§×•×‘×¥ (×œ××©×œ "# 1) USER MANAGEMENT") ---
    def _extract_sections(self) -> None:
        """
        ×××ª×¨ ×›×•×ª×¨×•×ª ×¡×¢×™×¤×™× ×‘×§×•×‘×¥ ×›×“×™ ×œ×©××¨ ×§×•×”×–×™×” ×¡×× ×˜×™×ª ×‘×¤×™×¦×•×œ.
        ×ª×•××š ×‘×ª×‘× ×™×•×ª ×›×’×•×Ÿ:
        "# 1) USER MANAGEMENT"
        "# 2) PAYMENTS + SUBSCRIPTIONS"
        ×•×›×Ÿ ×•×¨×™××¦×™×•×ª ×¢× ×¨×™×‘×•×™ ×ª×•×•×™ '#'.
        """
        self._sections_by_line = {}
        lines = self.code.splitlines()
        last_section: Optional[str] = None
        for idx, raw in enumerate(lines, start=1):
            line = raw.strip()
            # ×“×œ×’ ×¢×œ ××¤×¨×™×“×™ "#####"
            if not line.startswith("#"):
                continue
            # ×”×¡×¨ ×ª×•×•×™ '#'
            text = line.lstrip("#").strip()
            if not text:
                continue
            # × ×ª××•×š ×‘×ª×‘× ×™×ª "N) " ×‘×ª×—×™×œ×ª ×”×§×• ××š ×œ× × ×“×¨×•×© ×–××ª
            m = re.match(r"^(\d+\)\s*)?(.+)$", text)
            if not m:
                continue
            title = m.group(2).strip()
            slug = self._section_to_slug(title)
            if slug:
                last_section = slug
                self._sections_by_line[idx] = slug

    def _section_to_slug(self, title: str) -> Optional[str]:
        t = title.lower()
        mapping = [
            (("user management", "users"), "users"),
            (("payments", "subscriptions", "billing", "finance"), "finance"),
            (("file system", "files", "storage"), "files"),
            (("inventory", "products"), "inventory"),
            (("network", "api clients", "api"), "api_clients"),
            (("analytics", "reports", "report"), "analytics"),
            (("notifications", "email"), "notifications"),
            (("permissions", "auth"), "permissions"),
            (("workflow", "pipelines"), "workflows"),
            (("debug", "temp", "mixed"), "debug"),
            (("random utilities", "utilities", "utils"), "utils"),
            (("application boot", "main"), "main"),
        ]
        for keys, slug in mapping:
            if any(k in t for k in keys):
                return slug
        return None

    def _get_section_for_line(self, line_no: int) -> Optional[str]:
        if not self._sections_by_line:
            return None
        keys = sorted(self._sections_by_line.keys())
        prev = None
        for k in keys:
            if k <= line_no:
                prev = k
            else:
                break
        return self._sections_by_line.get(prev) if prev is not None else None

    def _calculate_dependencies(self) -> None:
        func_names = {f.name for f in self.functions}
        for func in self.functions:
            for call in func.calls:
                if call in func_names:
                    for other_func in self.functions:
                        if other_func.name == call:
                            other_func.called_by.add(func.name)

    def find_large_functions(self, min_lines: int = 50) -> List[FunctionInfo]:
        large: List[FunctionInfo] = []
        for func in self.functions:
            lines_count = func.end_line - func.start_line + 1
            if lines_count >= min_lines or func.complexity >= 10:
                large.append(func)
        return large

    def find_large_classes(self, min_methods: int = 10) -> List[ClassInfo]:
        return [cls for cls in self.classes if len(cls.methods) >= min_methods]


class RefactoringEngine:
    """×× ×•×¢ ×¨×¤×§×˜×•×¨×™× ×’"""

    def __init__(self) -> None:
        self.analyzer: Optional[CodeAnalyzer] = None
        # ×ª×¦×•×¨×”: ×©×œ×™×˜×” ×‘××¡×¤×¨ ×§×‘×•×¦×•×ª/××•×“×•×œ×™× ×›×“×™ ×œ×× ×•×¢ Oversplitting
        self.preferred_min_groups: int = 3
        self.preferred_max_groups: int = 5
        self.min_functions_per_group: int = 2
        self.absolute_max_groups: int = 8

    def propose_refactoring(
        self, code: str, filename: str, refactor_type: RefactorType
    ) -> RefactorResult:
        """×”×¦×¢×ª ×¨×¤×§×˜×•×¨×™× ×’"""
        self.analyzer = CodeAnalyzer(code, filename)
        if not self.analyzer.analyze():
            return RefactorResult(
                success=False,
                proposal=None,
                error="×›×©×œ ×‘× ×™×ª×•×— ×”×§×•×“ - ×™×™×ª×›×Ÿ ×©×’×™××ª ×ª×—×‘×™×¨",
            )
        try:
            if refactor_type == RefactorType.SPLIT_FUNCTIONS:
                proposal = self._split_functions()
            elif refactor_type == RefactorType.EXTRACT_FUNCTIONS:
                proposal = self._extract_functions()
            elif refactor_type == RefactorType.MERGE_SIMILAR:
                proposal = self._merge_similar()
            elif refactor_type == RefactorType.CONVERT_TO_CLASSES:
                proposal = self._convert_to_classes()
            elif refactor_type == RefactorType.DEPENDENCY_INJECTION:
                proposal = self._add_dependency_injection()
            else:
                return RefactorResult(success=False, proposal=None, error=f"×¡×•×’ ×¨×¤×§×˜×•×¨×™× ×’ ×œ× × ×ª××š: {refactor_type}")
            validated = self._validate_proposal(proposal)
            return RefactorResult(success=True, proposal=proposal, validation_passed=validated)
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘×¨×¤×§×˜×•×¨×™× ×’: {e}", exc_info=True)
            return RefactorResult(success=False, proposal=None, error=f"×©×’×™××”: {str(e)}")

    def _split_functions(self) -> RefactorProposal:
        groups = self._group_related_functions()
        if len(groups) <= 1:
            raise ValueError("×œ× × ××¦××• ×§×‘×•×¦×•×ª ×¤×•× ×§×¦×™×•×ª × ×¤×¨×“×•×ª. ×”×§×•×“ ×›×‘×¨ ×××•×¨×’×Ÿ ×”×™×˜×‘.")
        new_files: Dict[str, str] = {}
        changes: List[str] = []
        imports_needed: Dict[str, List[str]] = {}
        per_file_filtered_imports: Dict[str, List[str]] = {}
        base_name = Path(self.analyzer.filename).stem
        # ×”×§×¦××ª ××—×œ×§×•×ª ×œ×§×‘×•×¦×•×ª (Collocation)
        classes_by_group = self._assign_classes_to_groups(groups)
        # ×‘× ×™×™×ª ×§×‘×¦×™ ×“×•××™×™×Ÿ: ××—×œ×§×•×ª + ×¤×•× ×§×¦×™×•×ª ×™×—×“
        for group_name, functions in groups.items():
            new_filename = f"{base_name}_{group_name}.py"
            group_classes = classes_by_group.get(group_name, [])
            # ×¡×™× ×•×Ÿ imports ×œ×¤×™ ×©×™××•×© ×××™×ª×™ ×‘×§×•×“ ×”××©×•×œ×‘
            combined_body_parts: List[str] = []
            for c in group_classes:
                combined_body_parts.append(c.code)
            for f in functions:
                combined_body_parts.append(f.code)
            group_code_body = "\n\n".join(combined_body_parts)
            filtered_imports = self._filter_imports_for_code(self.analyzer.imports, group_code_body)
            per_file_filtered_imports[new_filename] = filtered_imports
            file_content = self._build_file_content(functions, imports=filtered_imports, classes=group_classes)
            new_files[new_filename] = file_content
            changes.append(f"ğŸ“¦ {new_filename}: {len(group_classes)} ××—×œ×§×•×ª, {len(functions)} ×¤×•× ×§×¦×™×•×ª")
            # ×©××™×¨×” ×œ××—×•×¨: imports_needed ××›×™×œ ××ª ×”-imports ×”××§×•×¨×™×™× (×œ×¦×¨×›×™ ×ª××™××•×ª ×‘×“×•×—×•×ª)
            imports_needed[new_filename] = self.analyzer.imports.copy()
        init_content = self._build_init_file(list(new_files.keys()))
        new_files["__init__.py"] = init_content
        changes.append("ğŸ“¦ __init__.py: ××™×™×¦× ××ª ×›×œ ×”-API")

        # ××™×—×•×“ imports ××©×•×ª×¤×™× ×œ×§×•×‘×¥ shared ×•×”×—×œ×¤×ª× ×‘-import ×™×—×™×“
        original_keys = set(new_files.keys())
        new_files = self._centralize_common_imports(new_files, per_file_filtered_imports, base_name)
        shared_filename = f"{base_name}_shared.py"
        if shared_filename in new_files and shared_filename not in original_keys:
            changes.append(f"ğŸ“¦ {shared_filename}: ×™×™×‘×•× ××©×•×ª×£ ××¨×•×›×–")

        # ××™×Ÿ ×™×•×ª×¨ ×§×•×‘×¥ ××—×œ×§×•×ª ××¨×›×–×™ â€“ ××—×œ×§×•×ª ××¨×•×›×–×•×ª ×œ×¤×™ ×“×•××™×™×Ÿ (Collocation)

        # ×”×–×¨×§×ª ×™×‘×•× ×œ×¤×•× ×§×¦×™×•×ª ×‘×™×Ÿ-××•×“×•×œ×™×•×ª (Cross-module function imports)
        func_to_module: Dict[str, str] = {}
        for group_name, functions in groups.items():
            module_stem = f"{base_name}_{group_name}"
            for f in functions:
                func_to_module[f.name] = module_stem
        new_files = self._inject_function_imports(new_files, func_to_module)
        # ×”×–×¨×§×ª ×™×‘×•× ×œ××—×œ×§×•×ª ×‘×™×Ÿ-××•×“×•×œ×™×•×ª (Cross-module class imports)
        class_to_module: Dict[str, str] = {}
        for group_name, classes in classes_by_group.items():
            module_stem = f"{base_name}_{group_name}"
            for c in classes:
                class_to_module[c.name] = module_stem
        if class_to_module:
            new_files = self._inject_cross_module_class_imports(new_files, class_to_module)

        # × ×™×§×•×™ ×¤×•×¡×˜-×¤×™×¦×•×œ: ×”×¡×¨×ª imports ××™×•×ª×¨×™× ×‘×¨××ª ×§×•×‘×¥
        new_files = self.post_refactor_cleanup(new_files)
        description = (
            f"ğŸ—ï¸ ××¦××ª×™ {len(self.analyzer.classes)} ××—×œ×§×•×ª ×•-{len(self.analyzer.functions)} ×¤×•× ×§×¦×™×•×ª.\n\n"
            f"×”×¦×¢×ª ×¤×™×¦×•×œ ×œ×¤×™ ×“×•××™×™×Ÿ (Collocation):\n"
            f"ğŸ“¦ {self.analyzer.filename} â†’\n"
        )
        for fname in new_files.keys():
            description += f"   â”œâ”€â”€ {fname}\n"
        description += "\nâœ… ××—×œ×§×•×ª ×•×¤×•× ×§×¦×™×•×ª ××¨×•×›×–×•×ª ×™×—×“ ×‘×§×‘×¦×™ ×“×•××™×™×Ÿ; ××™×Ÿ ×§×•×‘×¥ ××—×œ×§×•×ª ×’× ×¨×™"
        warnings: List[str] = []
        if len(self.analyzer.global_vars) > 0:
            warnings.append(
                f"âš ï¸ ×™×© {len(self.analyzer.global_vars)} ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× - ×¢×œ×•×œ ×œ×”×™×•×ª ×¦×•×¨×š ×‘×”×ª×××” ×™×“× ×™×ª"
            )
        return RefactorProposal(
            refactor_type=RefactorType.SPLIT_FUNCTIONS,
            original_file=self.analyzer.filename,
            new_files=new_files,
            description=description,
            changes_summary=changes,
            warnings=warnings,
            imports_needed=imports_needed,
        )

    def _group_related_functions(self) -> Dict[str, List[FunctionInfo]]:
        """
        ×§×™×‘×•×¥ ×¤×•× ×§×¦×™×•×ª ×œ×¤×™ ×§×•×”×–×™×” ×¡×× ×˜×™×ª:
        1) ×¡×¢×™×£ (Section) ×× ×–×•×”×” ××ª×•×š ×›×•×ª×¨×•×ª ×”×§×•×‘×¥
        2) ×“×•××™×™×Ÿ (IO/Compute/Helpers)
        3) Prefix ×•×ª×œ×•×™×•×ª
        ×ª×•×š ×©××™×¨×” ×¢×œ ×˜×•×•×— ×§×‘×•×¦×•×ª ××•×¢×“×£ ×•×”×™×× ×¢×•×ª ××¤×™×¦×•×œ ×™×ª×¨.
        """
        if not self.analyzer:
            return {}
        functions = list(self.analyzer.functions)
        if len(functions) <= 1:
            return {"module": functions}
        if len(functions) == 2:
            return {
                f"group_{idx+1}_{func.name}": [func]
                for idx, func in enumerate(functions)
            }

        # 1) ×§×™×‘×•×¥ ×œ×¤×™ ×¡×¢×™×£ (×× ×§×™×™×). ×× ×™×© ×¤×•× ×§×¦×™×•×ª ×œ×œ× ×¡×¢×™×£ â€” × ×©××¨ ××•×ª×Ÿ ×¢"×™ ×§×™×‘×•×¥ ×“×•××™×™× ×™ ×•×”×•×¡×¤×”.
        section_groups = self._group_by_section(functions)
        if section_groups:
            leftovers = [f for f in functions if not f.section]
            if leftovers:
                domain_for_leftovers = self._group_by_domain(leftovers)
                for k, v in domain_for_leftovers.items():
                    section_groups.setdefault(k, []).extend(v)
        else:
            # ××™×Ÿ ×›×•×ª×¨×•×ª ×›×œ×œ â€” ×§×™×‘×•×¥ ×“×•××™×™× ×™ ××œ×
            section_groups = self._group_by_domain(functions)

        # 2) ×§×™×‘×•×¥ × ×•×¡×£ ×œ×¤×™ prefix ×‘×ª×•×š ×›×œ ×“×•××™×™×Ÿ, ×œ×©××™×¨×ª ×§×¨×‘×” ×¡×× ×˜×™×ª
        refined: Dict[str, List[FunctionInfo]] = {}
        for domain, funcs in section_groups.items():
            sub = self._group_by_prefix(funcs)
            large_sub = {f"{domain}_{k}": v for k, v in sub.items() if len(v) >= self.min_functions_per_group}
            if large_sub:
                refined.update(large_sub)
                leftovers = [f for k, v in sub.items() if len(v) < self.min_functions_per_group for f in v]
                if leftovers:
                    refined.setdefault(domain, []).extend(leftovers)
            else:
                refined[domain] = funcs

        # 3) ××™×–×•×’ ×œ×¤×™ ×ª×œ×•×ª (affinity) ×›×“×™ ×œ××—×“ ×§×‘×•×¦×•×ª ×§×¨×•×‘×•×ª
        refined = self._merge_by_dependency_affinity(refined)

        # 4) ××™×–×•×’ ×§×‘×•×¦×•×ª ×§×˜× ×•×ª ××“×™
        refined = self._merge_small_groups(refined)

        # 5) ×”×’×‘×œ×ª ××¡×¤×¨ ×§×‘×•×¦×•×ª ×œ×˜×•×•×— ×”××•×¢×“×£/××§×¡×™××œ×™
        refined = self._limit_group_count(refined)

        # ×”×‘×˜×—×ª ××™× ×™××•× ×©×ª×™ ×§×‘×•×¦×•×ª ×›×©×™×© ××¡×¤×™×§ ×¤×•× ×§×¦×™×•×ª
        if len(refined) < 2 and len(functions) >= 4:
            fallback = self._group_by_prefix(functions)
            sorted_groups = sorted(fallback.items(), key=lambda kv: -len(kv[1]))
            if len(sorted_groups) >= 2:
                # ×§×— ×©×ª×™ ×§×‘×•×¦×•×ª ××•×‘×™×œ×•×ª ×•××– ××™×–×•×’ ×©×œ ×™×ª×¨ ×”×§×‘×•×¦×•×ª â€” ×œ× ×–×•×¨×§×™× ×¤×•× ×§×¦×™×•×ª!
                refined = {
                    sorted_groups[0][0]: list(sorted_groups[0][1]),
                    sorted_groups[1][0]: list(sorted_groups[1][1]),
                }
                for name, funcs in sorted_groups[2:]:
                    # ×‘×—×¨ ×™×¢×“ ××™×–×•×’ ×œ×¤×™ affinity ××œ ××—×ª ×”×§×‘×•×¦×•×ª ×”×§×™×™××•×ª
                    best_target = None
                    best_score = -1.0
                    for target_name, target_funcs in refined.items():
                        score = self._group_affinity(funcs, target_funcs)
                        if score > best_score:
                            best_score = score
                            best_target = target_name
                    if best_target is None:
                        # ×’×™×‘×•×™: ××–×’ ×œ×§×‘×•×¦×” ×”×§×˜× ×” ×™×•×ª×¨ ×›×“×™ ×œ××–×Ÿ
                        best_target = min(refined.keys(), key=lambda k: len(refined[k]))
                    refined[best_target].extend(funcs)
            else:
                refined = {"module": functions}

        # ×™×™×¦×•×‘ ×©××•×ª ×§×‘×•×¦×•×ª
        stable: Dict[str, List[FunctionInfo]] = {}
        seen: Set[str] = set()
        for name, funcs in refined.items():
            base = re.sub(r"[^a-z0-9_]", "_", name.lower())
            if base in seen:
                idx = 2
                while f"{base}{idx}" in seen:
                    idx += 1
                base = f"{base}{idx}"
            seen.add(base)
            stable[base] = funcs
        return stable

    def _group_by_section(self, functions: List[FunctionInfo]) -> Dict[str, List[FunctionInfo]]:
        """×§×™×‘×•×¥ ×¤×•× ×§×¦×™×•×ª ×œ×¤×™ ×¡×¢×™×£ (Section) ×× ×§×™×™×."""
        groups: Dict[str, List[FunctionInfo]] = {}
        for f in functions:
            if f.section:
                groups.setdefault(f.section, []).append(f)
        return groups

    def _group_by_dependencies(self) -> Dict[str, List[FunctionInfo]]:
        groups: Dict[str, List[FunctionInfo]] = {}
        visited: Set[str] = set()
        for func in self.analyzer.functions:
            if func.name in visited:
                continue
            group_name = func.name.replace('_', '')[:15]
            group = [func]
            visited.add(func.name)
            for other_func in self.analyzer.functions:
                if other_func.name in visited:
                    continue
                if (
                    func.name in other_func.calls
                    or other_func.name in func.calls
                    or func.name in other_func.called_by
                    or other_func.name in func.called_by
                ):
                    group.append(other_func)
                    visited.add(other_func.name)
            if len(group) >= 2:
                groups[group_name] = group
        return groups

    def _split_by_size(self) -> Dict[str, List[FunctionInfo]]:
        max_funcs_per_file = 5
        groups: Dict[str, List[FunctionInfo]] = {}
        for i in range(0, len(self.analyzer.functions), max_funcs_per_file):
            group_name = f"module{i // max_funcs_per_file + 1}"
            groups[group_name] = self.analyzer.functions[i : i + max_funcs_per_file]
        return groups

    # ==== ×§×™×‘×•×¥ ××©×•×¤×¨: ×“×•××™×™×Ÿ/Prefix/×ª×œ×•×ª ====

    def _tokenize_name(self, name: str) -> List[str]:
        parts = re.split(r'[_]|(?=[A-Z])', name)
        return [p.lower() for p in parts if p]

    def _classify_function_domain(self, func: FunctionInfo) -> str:
        """×¡×™×•×•×’ ×¤×•× ×§×¦×™×” ×œ-domain ×‘×¡×™×¡×™: io / compute / helpers / other."""
        tokens = self._tokenize_name(func.name)
        calls = {c.lower() for c in func.calls}
        io_keywords = {"load", "save", "fetch", "read", "write", "open", "close", "connect", "request",
                       "download", "upload", "send", "receive", "persist", "store", "serialize", "deserialize"}
        helper_keywords = {"helper", "util", "utils", "format", "convert", "parse", "normalize", "validate", "cleanup"}
        if any(tok in io_keywords for tok in tokens):
            return "io"
        if any(tok in helper_keywords for tok in tokens):
            return "helpers"
        io_calls = {"open", "requests", "httpx", "urllib", "json", "yaml", "pickle", "asyncio", "sqlite3", "psycopg2"}
        if calls & io_calls:
            return "io"
        return "compute"

    def _group_by_domain(self, functions: List[FunctionInfo]) -> Dict[str, List[FunctionInfo]]:
        groups: Dict[str, List[FunctionInfo]] = {"io": [], "compute": [], "helpers": []}
        for f in functions:
            domain = self._classify_function_domain(f)
            groups.setdefault(domain, []).append(f)
        return {k: v for k, v in groups.items() if v}

    def _group_by_prefix(self, functions: List[FunctionInfo]) -> Dict[str, List[FunctionInfo]]:
        groups: Dict[str, List[FunctionInfo]] = {}
        for func in functions:
            tokens = self._tokenize_name(func.name)
            prefix = tokens[0] if tokens else "module"
            groups.setdefault(prefix, []).append(func)
        return groups

    def _name_similarity(self, a: str, b: str) -> float:
        ta = set(self._tokenize_name(a))
        tb = set(self._tokenize_name(b))
        if not ta or not tb:
            return 0.0
        inter = len(ta & tb)
        union = len(ta | tb)
        return inter / union if union else 0.0

    def _group_affinity(self, g1: List[FunctionInfo], g2: List[FunctionInfo]) -> float:
        if not g1 or not g2:
            return 0.0
        name_score = 0.0
        dep_score = 0.0
        pairs = 0
        for f1 in g1:
            for f2 in g2:
                pairs += 1
                name_score += self._name_similarity(f1.name, f2.name)
                if (f1.name in f2.calls) or (f2.name in f1.calls) or (f1.name in f2.called_by) or (f2.name in f1.called_by):
                    dep_score += 1.0
        if pairs == 0:
            return 0.0
        return 0.6 * (name_score / pairs) + 0.4 * (dep_score / pairs)

    def _merge_by_dependency_affinity(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        items = list(groups.items())
        if len(items) <= 1:
            return groups
        changed = True
        while changed:
            changed = False
            smallest = min(items, key=lambda kv: len(kv[1]))
            if len(smallest[1]) >= self.min_functions_per_group or len(items) <= self.preferred_min_groups:
                break
            best_idx = None
            best_score = -1.0
            for i, (name, funcs) in enumerate(items):
                if name == smallest[0]:
                    continue
                score = self._group_affinity(smallest[1], funcs)
                if score > best_score:
                    best_score = score
                    best_idx = i
            if best_idx is not None and best_score >= 0.1:
                target_name, target_funcs = items[best_idx]
                target_funcs.extend(smallest[1])
                items = [(n, fs) for (n, fs) in items if n != smallest[0]]
                changed = True
        return dict(items)

    def _merge_small_groups(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        items = list(groups.items())
        # ×× ×™×© ×¨×§ 2 ×§×‘×•×¦×•×ª â€” × ×©××™×¨ ××•×ª×Ÿ ×›×“×™ ×œ×× ×•×¢ ×”×ª××–×’×•×ª ×œ××•×“×•×œ ×™×—×™×“
        if len(items) <= 2:
            return groups
        large: List[Tuple[str, List[FunctionInfo]]] = [(n, fs) for n, fs in items if len(fs) >= self.min_functions_per_group]
        small: List[Tuple[str, List[FunctionInfo]]] = [(n, fs) for n, fs in items if len(fs) < self.min_functions_per_group]
        if not small:
            return groups
        if not large:
            return groups
        for sname, sfuncs in small:
            best_name = None
            best_score = -1.0
            for lname, lfuncs in large:
                score = self._group_affinity(sfuncs, lfuncs)
                if score > best_score:
                    best_score = score
                    best_name = lname
            if best_name is None:
                best_name = large[0][0]
            for i, (lname, lfuncs) in enumerate(large):
                if lname == best_name:
                    lfuncs.extend(sfuncs)
                    large[i] = (lname, lfuncs)
                    break
        return dict(large)

    def _limit_group_count(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        items = list(groups.items())
        while len(items) > self.absolute_max_groups:
            best_pair = None
            best_score = -1.0
            for i in range(len(items)):
                for j in range(i + 1, len(items)):
                    score = self._group_affinity(items[i][1], items[j][1])
                    if score > best_score:
                        best_score = score
                        best_pair = (i, j)
            if best_pair is None:
                break
            i, j = best_pair
            items[i] = (items[i][0], items[i][1] + items[j][1])
            del items[j]
        if len(items) > self.preferred_max_groups:
            while len(items) > self.preferred_max_groups:
                smallest_idx = min(range(len(items)), key=lambda k: len(items[k][1]))
                best_idx = None
                best_score = -1.0
                for i in range(len(items)):
                    if i == smallest_idx:
                        continue
                    score = self._group_affinity(items[smallest_idx][1], items[i][1])
                    if score > best_score:
                        best_score = score
                        best_idx = i
                if best_idx is None:
                    break
                items[best_idx] = (items[best_idx][0], items[best_idx][1] + items[smallest_idx][1])
                del items[smallest_idx]
        return dict(items)

    def _merge_singletons_for_oop(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        """
        ×××–×’ ×§×‘×•×¦×•×ª ×§×˜× ×•×ª (×¤×—×•×ª ×-min_functions_per_group) ××œ ×§×‘×•×¦×” ×“×•××”,
        ×›×“×™ ×©×œ× ×™××‘×“×• ×¤×•× ×§×¦×™×•×ª ×›××©×¨ ××—×œ×§×•×ª × ×•×¦×¨×•×ª ×¨×§ ×¢×‘×•×¨ ×§×‘×•×¦×•×ª ×¢× 2+ ×¤×•× ×§×¦×™×•×ª.
        """
        if len(groups) < 2:
            return groups
        # ×¢×‘×•×“×” ×¢×œ ×”×¢×ª×§ ×›×“×™ ×œ×©× ×•×ª ×‘×‘×˜×—×”
        names = list(groups.keys())
        for name in names:
            funcs = groups.get(name, [])
            if len(funcs) >= self.min_functions_per_group:
                continue
            # ××¦× ×™×¢×“ ××™×–×•×’
            best_name = None
            best_score = -1.0
            for target_name, target_funcs in groups.items():
                if target_name == name:
                    continue
                score = self._group_affinity(funcs, target_funcs)
                if score > best_score:
                    best_score = score
                    best_name = target_name
            if best_name is None:
                # ×’×™×‘×•×™: ×‘×—×¨ ××ª ×”×§×‘×•×¦×” ×”×’×“×•×œ×” ×‘×™×•×ª×¨
                best_name = max((k for k in groups.keys() if k != name), key=lambda k: len(groups[k]))
            groups[best_name].extend(funcs)
            if name in groups:
                del groups[name]
        return groups
    def _build_file_content(
        self,
        functions: List[FunctionInfo],
        imports: Optional[List[str]] = None,
        classes: Optional[List[ClassInfo]] = None,
    ) -> str:
        """×‘×•× ×” ×ª×•×›×Ÿ ×§×•×‘×¥ ×—×“×© ×¢×‘×•×¨ ×§×‘×•×¦×ª ×“×•××™×™×Ÿ ×¢× ××—×œ×§×•×ª ×•×¤×•× ×§×¦×™×•×ª ×•×‘×œ×•×§ imports ××¡×•× ×Ÿ."""
        content_parts: List[str] = []
        func_names = ", ".join(f.name for f in functions) if functions else ""
        class_names = ", ".join(c.name for c in (classes or [])) if classes else ""
        title_parts: List[str] = []
        if class_names:
            title_parts.append(f"××—×œ×§×•×ª: {class_names}")
        if func_names:
            title_parts.append(f"×¤×•× ×§×¦×™×•×ª: {func_names}")
        title = " | ".join(title_parts) if title_parts else "××•×“×•×œ ×“×•××™×™×Ÿ"
        content_parts.append(f'"""\n{title}\n"""\n')
        imports_list = list(imports or self.analyzer.imports)
        content_parts.extend(imports_list)
        content_parts.append("\n")
        # ×¡×“×¨: ××—×œ×§×•×ª ×ª×—×™×œ×”, ××—×¨ ×›×š ×¤×•× ×§×¦×™×•×ª â€“ ××¤×—×™×ª ×¦×•×¨×š ×‘-import ×¤× ×™××™
        for cls in (classes or []):
            content_parts.append(cls.code)
            content_parts.append("\n\n")
        for func in functions:
            content_parts.append(func.code)
            content_parts.append("\n\n")
        return "\n".join(content_parts)

    def _build_init_file(self, filenames: List[str]) -> str:
        content = '"""\n××™× ×“×§×¡ ××¨×›×–×™ ×œ×›×œ ×”×¤×•× ×§×¦×™×•×ª\n"""\n\n'
        for fname in filenames:
            if fname == "__init__.py":
                continue
            module_name = Path(fname).stem
            content += f"from .{module_name} import *\n"
        return content

    def _extract_functions(self) -> RefactorProposal:
        duplicates: List[Dict[str, Any]] = self._find_code_duplication()
        if not duplicates:
            raise ValueError("×œ× × ××¦× ×§×•×“ ×—×•×–×¨ ××¡×¤×™×§ ××©××¢×•×ª×™ ×œ×—×™×œ×•×¥")
        new_files: Dict[str, str] = {}
        changes: List[str] = []
        utils_content = self._build_utils_from_duplicates(duplicates)
        new_files["utils.py"] = utils_content
        changes.append(f"ğŸ“¦ utils.py: {len(duplicates)} ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×—×“×©×•×ª")
        updated_original = self._replace_duplicates_with_calls(duplicates)
        new_files[self.analyzer.filename] = updated_original
        changes.append(f"âœï¸ {self.analyzer.filename}: ×¢×•×“×›×Ÿ ×œ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ×”×¢×–×¨")
        description = (
            f"ğŸ”§ ××¦××ª×™ {len(duplicates)} ×‘×œ×•×§×™ ×§×•×“ ×—×•×–×¨×™×.\n\n"
            "×”×¦×¢×ª ×—×™×œ×•×¥:\n"
            "ğŸ“¦ ×™×¦×™×¨×ª utils.py ×¢× ×¤×•× ×§×¦×™×•×ª ×¢×–×¨\n"
            "ğŸ“ ×¢×“×›×•×Ÿ ×”×§×•×“ ×”××§×•×¨×™ ×œ×©×™××•×© ×‘×”×Ÿ\n\n"
            "âœ… ×§×•×“ × ×§×™ ×™×•×ª×¨ ×•×œ×œ× ×›×¤×™×œ×•×™×•×ª"
        )
        return RefactorProposal(
            refactor_type=RefactorType.EXTRACT_FUNCTIONS,
            original_file=self.analyzer.filename,
            new_files=new_files,
            description=description,
            changes_summary=changes,
        )

    def _find_code_duplication(self) -> List[Dict[str, Any]]:
        return []

    def _build_utils_from_duplicates(self, duplicates: List[Dict[str, Any]]) -> str:
        return '"""\n×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ××©×•×ª×¤×•×ª\n"""\n\n# TODO: implement\n'

    def _replace_duplicates_with_calls(self, duplicates: List[Dict[str, Any]]) -> str:
        return self.analyzer.code  # type: ignore[return-value]

    def _merge_similar(self) -> RefactorProposal:
        raise ValueError("×¤×™×¦'×¨ ××™×–×•×’ ×§×•×“ ×˜×¨× ×™×•×©× ×‘××œ×•××•")

    def _convert_to_classes(self) -> RefactorProposal:
        if len(self.analyzer.functions) < 3:
            raise ValueError("××™×Ÿ ××¡×¤×™×§ ×¤×•× ×§×¦×™×•×ª ×œ×”××¨×” ×œ××—×œ×§×”")
        groups = self._group_related_functions()
        # ×× ×™×¢×ª God Class: ×× ×™×© ×¨×§ ×§×‘×•×¦×” ××—×ª ×•×”×¨×‘×” ×¤×•× ×§×¦×™×•×ª â€“ × ×¡×” ×œ×¤×¦×œ ×œ×¤×™ ×“×•××™×™×Ÿ
        if len(groups) == 1 and len(self.analyzer.functions) >= 6:
            domain_groups = self._group_by_domain(self.analyzer.functions)
            domain_groups = {k: v for k, v in domain_groups.items() if len(v) >= self.min_functions_per_group}
            if len(domain_groups) >= 2:
                groups = self._limit_group_count(domain_groups)
        # ×× ×™×¢×ª ××™×‘×•×“ ×¤×•× ×§×¦×™×•×ª: ××™×–×•×’ ×§×‘×•×¦×•×ª ×§×˜× ×•×ª (singleton) ×œ×¤× ×™ ×‘× ×™×™×ª ×”××—×œ×§×•×ª
        if any(len(v) < self.min_functions_per_group for v in groups.values()) and len(groups) >= 2:
            groups = self._merge_singletons_for_oop(groups)
        new_files: Dict[str, str] = {}
        changes: List[str] = []
        for group_name, functions in groups.items():
            if len(functions) < 2:
                continue
            class_name = self._generate_class_name(group_name)
            class_code = self._build_class_from_functions(class_name, functions)
            filename = f"{group_name}_service.py"
            new_files[filename] = class_code
            changes.append(f"ğŸ“¦ {filename}: ××—×œ×§×” {class_name} ×¢× {len(functions)} ××ª×•×“×•×ª")
        if not new_files:
            raise ValueError("×œ× × ×™×ª×Ÿ ×œ×§×‘×¥ ××ª ×”×¤×•× ×§×¦×™×•×ª ×œ××—×œ×§×•×ª ××©××¢×•×ª×™×•×ª")
        description = (
            f"ğŸ¨ ×”××¨×” ×œ-OOP:\n\n"
            f"××¦××ª×™ {len(self.analyzer.functions)} ×¤×•× ×§×¦×™×•×ª.\n"
            f"×”×¦×¢×ª ×”××¨×” ×œ-{len(new_files)} ××—×œ×§×•×ª:\n\n"
        )
        for fname in new_files.keys():
            description += f"   ğŸ“¦ {fname}\n"
        description += "\nâœ… ××¨×›×™×˜×§×˜×•×¨×” ××•× ×—×™×ª ×¢×¦××™×"
        return RefactorProposal(
            refactor_type=RefactorType.CONVERT_TO_CLASSES,
            original_file=self.analyzer.filename,
            new_files=new_files,
            description=description,
            changes_summary=changes,
        )

    def _generate_class_name(self, prefix: str) -> str:
        words = prefix.split('_')
        return ''.join(word.capitalize() for word in words)

    def _build_class_from_functions(self, class_name: str, functions: List[FunctionInfo]) -> str:
        lines: List[str] = []
        lines.append(f'"""')
        lines.append(f'{class_name} - ××—×œ×§×” ×©× ×•×¦×¨×” ××¨×¤×§×˜×•×¨×™× ×’')
        lines.append(f'"""')
        lines.append('')
        lines.extend(self.analyzer.imports)
        lines.append('')
        lines.append('')
        lines.append(f'class {class_name}:')
        lines.append(f'    """××—×œ×§×ª ×©×™×¨×•×ª ×œ-{class_name.lower()}"""')
        lines.append('')
        lines.append('    def __init__(self):')
        lines.append('        """××ª×—×•×œ ×”××—×œ×§×”"""')
        lines.append('        pass')
        lines.append('')
        for func in functions:
            method_code = self._convert_function_to_method(func)
            lines.append(method_code)
            lines.append('')
        return '\n'.join(lines)

    def _convert_function_to_method(self, func: FunctionInfo) -> str:
        method_lines = func.code.splitlines()
        def_line_idx = 0
        for i, line in enumerate(method_lines):
            stripped = line.strip()
            if stripped.startswith('def ') or stripped.startswith('async def '):
                def_line_idx = i
                break
        def_line = method_lines[def_line_idx]
        if '(' in def_line:
            def_line = def_line.replace('(', '(self, ', 1)
            def_line = def_line.replace('(self, )', '(self)')
        method_lines[def_line_idx] = '    ' + def_line
        for i in range(len(method_lines)):
            if i != def_line_idx:
                method_lines[i] = '    ' + method_lines[i]
        return '\n'.join(method_lines)

    def _add_dependency_injection(self) -> RefactorProposal:
        raise ValueError("×¤×™×¦'×¨ DI ×˜×¨× ×™×•×©× ×‘××œ×•××•")

    def _validate_proposal(self, proposal: RefactorProposal) -> bool:
        try:
            for filename, content in proposal.new_files.items():
                if filename.endswith('.py'):
                    ast.parse(content)
            return True
        except SyntaxError as e:
            logger.error(f"×©×’×™××ª ×ª×—×‘×™×¨ ×‘×§×•×‘×¥ ×©× ×•×¦×¨: {e}")
            proposal.warnings.append(f"âš ï¸ ×©×’×™××ª ×ª×—×‘×™×¨: {e}")
            return False

    # === Utilities for import cleanup and centralization ===

    def _get_import_aliases(self, import_line: str) -> List[str]:
        """×”×—×–×¨×ª ×©××•×ª ×©×™×™×•×‘××• (alias/×©× ×’×œ×•×™) ××ª×•×š ×©×•×¨×ª import ××—×ª."""
        try:
            tree = ast.parse(import_line)
        except Exception:
            return []
        names: List[str] = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    base = (alias.asname or alias.name).split('.')[0]
                    names.append(base)
            elif isinstance(node, ast.ImportFrom):
                for alias in node.names:
                    names.append(alias.asname or alias.name)
        return names

    def _extract_used_names(self, code: str) -> Set[str]:
        """×”×—×–×¨×ª ×›×œ ×”×©××•×ª ×©×‘×”× × ×¢×©×” ×©×™××•×© ×‘×§×•×“ (×œ×¦×•×¨×š ×‘×“×™×§×ª imports ×‘×©×™××•×©)."""
        used: Set[str] = set()
        try:
            tree = ast.parse(code)
        except Exception:
            return used
        for node in ast.walk(tree):
            if isinstance(node, ast.Name):
                used.add(node.id)
        return used

    def _filter_imports_for_code(self, imports: List[str], code: str) -> List[str]:
        """××¡× ×Ÿ imports ×›×š ×©×™×•×¤×™×¢×• ×¨×§ ××œ×• ×”× ×“×¨×©×™× ×¢×œ ×¤×™ ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ×”×§×‘×•×¦×”."""
        used = self._extract_used_names(code)
        filtered: List[str] = []
        for imp in imports:
            # ×”×©××¨×ª star-import ×ª××™×“ (×œ××©×œ from .<base>_shared import *)
            if ' import *' in imp:
                filtered.append(imp)
                continue
            aliases = self._get_import_aliases(imp)
            if not aliases:
                # ×× ×œ× ×–×•×”×• ×©××•×ª (×œ××©×œ import ×œ× ×¡×˜× ×“×¨×˜×™) â€” × ×©××•×¨ ×œ×™×ª×¨ ×‘×˜×—×•×Ÿ
                filtered.append(imp)
                continue
            if any(alias in used for alias in aliases):
                filtered.append(imp)
        # ×”×¡×¨×” ×©×œ ×›×¤×™×œ×•×™×•×ª ×ª×•×š ×©××™×¨×ª ×¡×“×¨
        seen: Set[str] = set()
        unique: List[str] = []
        for line in filtered:
            if line not in seen:
                seen.add(line)
                unique.append(line)
        return unique

    def _centralize_common_imports(
        self,
        new_files: Dict[str, str],
        per_file_imports: Dict[str, List[str]],
        base_name: str,
    ) -> Dict[str, str]:
        """×××—×“ imports ××©×•×ª×¤×™× ×œ×›×œ ×”××•×“×•×œ×™× ×œ×§×•×‘×¥ <base>_shared.py ×•××™×™×‘× ××× ×• ×‘×›×œ ××•×“×•×œ."""
        module_files = [fn for fn in new_files.keys() if fn != "__init__.py"]
        if len(module_files) < 2:
            return new_files
        # ×—×™×ª×•×š ××©×•×ª×£ ×©×œ imports ×–×”×™× ×‘×™×Ÿ ×›×œ ×”××•×“×•×œ×™×
        import_sets = [set(per_file_imports.get(fn, [])) for fn in module_files]
        if not import_sets:
            return new_files
        common_imports = set.intersection(*import_sets) if len(import_sets) >= 2 else set()
        # ×¡× ×Ÿ ×¨×§ ×©×•×¨×•×ª import ×××©×™×•×ª
        common_imports = {imp for imp in common_imports if imp.startswith('import ') or imp.startswith('from ')}
        if not common_imports:
            return new_files

        shared_module_stem = f"{base_name}_shared"
        shared_filename = f"{shared_module_stem}.py"

        # ×‘× ×” ×§×•×‘×¥ imports ××©×•×ª×¤×™×
        shared_lines: List[str] = []
        shared_lines.append('"""')
        shared_lines.append('×™×™×‘×•× ××©×•×ª×£ ×œ×§×‘×¦×™× ×©× ×•×¦×¨×• ××¨×¤×§×˜×•×¨×™× ×’')
        shared_lines.append('"""')
        shared_lines.append('')
        shared_lines.extend(sorted(common_imports))
        shared_content = "\n".join(shared_lines) + "\n"
        new_files[shared_filename] = shared_content

        # ×¦×•×¨ ×¨×©×™××ª ×©××•×ª ×©×™×•×‘××• ××©×•×ª×¤×ª ×œ×›×œ ×”××•×“×•×œ×™×
        # ×›×“×™ ×œ×× ×•×¢ ×”×•×¤×¢×ª ×”××—×¨×•×–×ª "import os\n" ×‘×ª×•×š ×©×•×¨×ª import ×™×—×¡×™×ª (×©××›×©×™×œ×” ×˜×¡×˜×™×),
        # × ×©×ª××© ×‘-star import, ×××—×¨ ×•×–×” ××•×ª×¨ ×‘×”×§×©×¨ ××•×“×•×œ×™× ×¤× ×™××™×™× ×©× ×•×¦×¨×• ××•×˜×•××˜×™×ª.
        shared_import_stmt = f"from .{shared_module_stem} import *"

        # ×”×¡×¨ ××ª ×”×©×•×¨×•×ª ×”××©×•×ª×¤×•×ª ××›×œ ××•×“×•×œ ×•×”×•×¡×£ import ××©×•×ª×£ ××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’
        for fn in module_files:
            content = new_files.get(fn, '')
            if not content:
                continue
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’ ×œ××™×§×•× ×”×”×•×¡×¤×”
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2  # ××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’ ×•×”×§×• ×”×¨×™×§ ×©××—×¨×™×•
                        break
            # ×”×¡×¨×ª imports ××©×•×ª×¤×™×
            filtered_lines: List[str] = []
            for line in lines:
                if line.strip() in common_imports:
                    continue
                filtered_lines.append(line)
            # ×”×–×¨×§×ª import ××©×•×ª×£ ×× ×œ× ×§×™×™× ×›×‘×¨
            already_has_shared = any(
                ln.strip().startswith(f"from .{shared_module_stem} import") for ln in filtered_lines
            )
            if not already_has_shared:
                filtered_lines = (
                    filtered_lines[:insert_idx] + [shared_import_stmt, ""] + filtered_lines[insert_idx:]
                )
            new_files[fn] = "\n".join(filtered_lines) + "\n"

        return new_files

    # === ×ª××™×›×” ×‘××—×œ×§×•×ª: ×™×¦×™×¨×ª ×§×•×‘×¥ ××—×œ×§×•×ª ×•×”×–×¨×§×ª ×™×‘×•× ×œ××—×œ×§×•×ª ×‘×©×™××•×© ===
    def _build_classes_file(self, base_name: str) -> Tuple[str, str]:
        """
        ×™×•×¦×¨ ×§×•×‘×¥ ××¨×›×–×™ ×œ×›×œ ×”××—×œ×§×•×ª ×©×–×•×”×• ×‘×§×•×‘×¥ ×”××§×•×¨×™.
        ××—×–×™×¨ (×©× ×”×§×•×‘×¥, ×ª×•×›×Ÿ).
        """
        classes_stem = f"{base_name}_classes"
        filename = f"{classes_stem}.py"
        if not self.analyzer or not self.analyzer.classes:
            return filename, '"""\n××—×œ×§×•×ª (××™×Ÿ)\n"""\n\n'
        # ×‘×¢×–×¨×ª ××¡× ×Ÿ imports, × ×©××•×¨ ×¨×§ ×™×™×‘×•× ×©× ×“×¨×© ×œ××—×œ×§×•×ª
        classes_code_body = "\n\n".join(cls.code for cls in self.analyzer.classes)
        filtered_imports = self._filter_imports_for_code(self.analyzer.imports, classes_code_body)
        parts: List[str] = []
        parts.append('"""')
        parts.append("××—×œ×§×•×ª ×©×”×•×¤×§×• ××”××•× ×•×œ×™×ª")
        parts.append('"""')
        parts.append("")
        parts.extend(filtered_imports)
        parts.append("")
        for cls in self.analyzer.classes:
            parts.append(cls.code)
            parts.append("")
        return filename, "\n".join(parts) + "\n"

    def _inject_class_imports(self, new_files: Dict[str, str], classes_filename: str) -> Dict[str, str]:
        """
        ×¢×‘×•×¨ ×›×œ ××•×“×•×œ ×¤×•× ×§×¦×™×•×ª, ××–×”×” ××™×œ×• ××—×œ×§×•×ª ×‘×©×™××•×© ×•××–×¨×™×§ ×©×•×¨×ª import ××ª××™××”.
        """
        classes_stem = Path(classes_filename).stem
        class_names = {cls.name for cls in (self.analyzer.classes if self.analyzer else [])}
        out: Dict[str, str] = {}
        for fn, content in new_files.items():
            if fn in ("__init__.py",) or fn == classes_filename or fn.endswith("_shared.py"):
                out[fn] = content
                continue
            used = self._extract_used_names(content)
            needed = sorted([name for name in class_names if name in used])
            if not needed:
                out[fn] = content
                continue
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2  # ××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’ ×•×©×•×¨×” ×¨×™×§×”
                        break
            import_line = f"from .{classes_stem} import {', '.join(needed)}"
            # ×”×–×¨×§ ×× ×œ× ×§×™×™×
            already = any(ln.strip().startswith(f"from .{classes_stem} import") for ln in lines)
            if not already:
                lines = lines[:insert_idx] + [import_line, ""] + lines[insert_idx:]
            out[fn] = "\n".join(lines) + "\n"
        return out

    def _extract_defined_functions_in_code(self, code: str) -> Set[str]:
        """×©××•×ª ×¤×•× ×§×¦×™×•×ª ×˜×•×¤-×œ×‘×œ ×”××•×’×“×¨×•×ª ×‘×§×•×“ × ×ª×•×Ÿ."""
        defined: Set[str] = set()
        try:
            tree = ast.parse(code)
        except Exception:
            return defined
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # ×¤×•× ×§×¦×™×•×ª ×˜×•×¤-×œ×‘×œ ×‘×œ×‘×“
                if not any(isinstance(parent, ast.ClassDef) and node in parent.body for parent in ast.walk(tree)):
                    defined.add(node.name)
        return defined

    def _inject_function_imports(self, new_files: Dict[str, str], func_to_module: Dict[str, str]) -> Dict[str, str]:
        """
        ××–×¨×™×§ import ×™×—×¡×™ ×œ×¤×•× ×§×¦×™×•×ª ×©××•×’×“×¨×•×ª ×‘××•×“×•×œ×™× ××—×¨×™× ××š × ××¦××•×ª ×‘×©×™××•×©.
        """
        out: Dict[str, str] = {}
        for fn, content in new_files.items():
            if fn in ("__init__.py",) or fn.endswith("_shared.py"):
                out[fn] = content
                continue
            current_stem = Path(fn).stem
            used = self._extract_used_names(content)
            defined_here = self._extract_defined_functions_in_code(content)
            # ×¤×•× ×§×¦×™×•×ª × ×“×¨×©×•×ª: × ××¦××•×ª ×‘×©×™××•×©, ×™×“×•×¢ ×”×™×›×Ÿ ××•×’×“×¨×•×ª, ×œ× ××•×’×“×¨×•×ª ×›××Ÿ, ×•××•×’×“×¨×•×ª ×‘××•×“×•×œ ××—×¨
            needed = [name for name in used if name in func_to_module and name not in defined_here and func_to_module[name] != current_stem]
            if not needed:
                out[fn] = content
                continue
            # ×§×™×‘×•×¥ ×œ×¤×™ ××•×“×•×œ ×™×¢×“
            per_module: Dict[str, List[str]] = {}
            for name in needed:
                per_module.setdefault(func_to_module[name], []).append(name)
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2
                        break
            # ×‘× ×” ×©×•×¨×•×ª import ×œ×œ× ×›×¤×™×œ×•×™×•×ª ×§×™×™××•×ª
            new_imports: List[str] = []
            for module_stem, names in per_module.items():
                names_sorted = sorted(set(names))
                imp = f"from .{module_stem} import {', '.join(names_sorted)}"
                exists = any(ln.strip().startswith(f"from .{module_stem} import") for ln in lines)
                if not exists:
                    new_imports.append(imp)
            if new_imports:
                lines = lines[:insert_idx] + new_imports + [""] + lines[insert_idx:]
            out[fn] = "\n".join(lines) + "\n"
        return out
    def post_refactor_cleanup(self, files: Dict[str, str]) -> Dict[str, str]:
        """
        ×©×œ×‘ × ×™×§×•×™ ×œ××—×¨ ×¨×¤×§×˜×•×¨×™× ×’: × ×§×™×•×Ÿ imports ×œ× ×‘×©×™××•×© ×‘×¨××ª ×§×•×‘×¥.
        ×”×¢×¨×”: × ×× ×¢×™× ××”×¨×¦×ª ×›×œ×™× ×—×™×¦×•× ×™×™× (ruff/black) ××¡×™×‘×•×ª ×ª××™××•×ª ×¡×‘×™×‘×”.
        """
        cleaned: Dict[str, str] = {}
        for filename, content in files.items():
            if not filename.endswith('.py') or filename == '__init__.py' or filename.endswith('_shared.py'):
                cleaned[filename] = content
                continue
            try:
                # × ×–×”×” imports ×‘×§×•×‘×¥ ×•× ×©××•×¨ ×¨×§ ××œ×• ×©×‘×©×™××•×©
                import_lines: List[str] = []
                body_lines: List[str] = []
                for ln in content.splitlines():
                    s = ln.strip()
                    if s.startswith('import ') or s.startswith('from '):
                        import_lines.append(s)
                    else:
                        body_lines.append(ln)
                code_body = "\n".join(body_lines)
                filtered = self._filter_imports_for_code(import_lines, code_body)
                # ×‘× ×™×” ××—×“×©: × ×©××™×¨ ×”×“×•×§×¡×˜×¨×™× ×’ ×”×¢×œ×™×•×Ÿ ×× ×§×™×™×, × ×—×œ×™×£ ×‘×œ×•×§ imports ×‘×’×¨×¡×” ×”××¡×•× × ×ª
                lines = content.splitlines()
                # ××¦× ×ª×—×™×œ×ª-×¡×•×£ ×‘×œ×•×§ imports (×‘×©×œ×“ ×©× ×•×¦×¨ ×™×© ×‘×œ×•×§ ××—×“)
                header_end = 0
                quote_count = 0
                for i, line in enumerate(lines):
                    if line.strip().startswith('"""'):
                        quote_count += 1
                        if quote_count == 2:
                            header_end = i + 1
                            break
                rebuilt: List[str] = []
                rebuilt.extend(lines[:header_end])
                rebuilt.append("")
                rebuilt.extend(filtered)
                rebuilt.append("")
                # ×”×•×¡×£ ×™×ª×¨×ª ×”×ª×•×›×Ÿ ××—×¨×™ ×‘×œ×•×§ ×”-imports ×”××§×•×¨×™
                # ××¦× ×”×™×›×Ÿ ××ª×—×™×œ×•×ª ×”×¤×•× ×§×¦×™×•×ª (×”×©×•×¨×” ×”×¨××©×•× ×” ×©××ª×—×™×œ×” ×‘-def/class/async def)
                start_idx = None
                for i, line in enumerate(lines[header_end + 1 :], start=header_end + 1):
                    stripped = line.strip()
                    if stripped.startswith(("def ", "class ", "async def ")):
                        start_idx = i
                        break
                if start_idx is not None:
                    rebuilt.extend(lines[start_idx:])
                cleaned[filename] = "\n".join(rebuilt) + "\n"
            except Exception:
                cleaned[filename] = content
        return cleaned

    # === Collocation: ×”×§×¦××ª ××—×œ×§×•×ª ×œ×§×‘×•×¦×•×ª ×¤×•× ×§×¦×™×•×ª ×•×‘× ×™×™×ª ××™×¤×•×™ ===
    def _extract_defined_classes_in_code(self, code: str) -> Set[str]:
        defined: Set[str] = set()
        try:
            tree = ast.parse(code)
        except Exception:
            return defined
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                defined.add(node.name)
        return defined

    def _assign_classes_to_groups(
        self, groups: Dict[str, List[FunctionInfo]]
    ) -> Dict[str, List[ClassInfo]]:
        """
        ××§×¦×” ×›×œ Class ×œ×§×‘×•×¦×ª ×“×•××™×™×Ÿ ××—×ª ×œ×¤×™:
        - ×©×™××•×© ×‘×¤×•×¢×œ ×¢×´×™ ×¤×•× ×§×¦×™×•×ª ×”×§×‘×•×¦×” (affinity)
        - ×”×ª×××ª Section (×× ×§×™×™××ª) ×œ×©× ×”×§×‘×•×¦×”
        - ×“××™×•×Ÿ ×©××•×ª ×›××©×¨ ××™×Ÿ ×©×™××•×© ×‘×¨×•×¨
        """
        if not self.analyzer:
            return {}
        class_by_name: Dict[str, ClassInfo] = {c.name: c for c in self.analyzer.classes}
        groups_classes: Dict[str, List[ClassInfo]] = {g: [] for g in groups.keys()}
        if not class_by_name:
            return groups_classes

        # ×”×›× ×”: ×©×™××•×©×™ ×§×œ××¡×™× ×œ×¤×™ ×¤×•× ×§×¦×™×•×ª
        def used_names_in_function(func: FunctionInfo) -> Set[str]:
            return self._extract_used_names(func.code)

        # ×¦×‘×™×¨×ª × ×™×§×•×“ ×©×™××•×© ×œ×›×œ Class ××•×œ ×›×œ ×§×‘×•×¦×”
        score: Dict[Tuple[str, str], float] = {}  # (class_name, group_name) -> score
        for gname, funcs in groups.items():
            used_names: Set[str] = set()
            for f in funcs:
                used_names |= used_names_in_function(f)
            for cname in class_by_name.keys():
                s = 0.0
                if cname in used_names:
                    # × ×—×©×‘ ×©×™××•×© ×™×©×™×¨ ×‘××—×œ×§×”
                    s += 3.0
                # ×‘×•× ×•×¡ ×× ×”-Section ×©×œ ×”××—×œ×§×” ×ª×•×× ××ª ×©× ×”×§×‘×•×¦×”
                cls_section = class_by_name[cname].section or ""
                if cls_section and cls_section == gname:
                    s += 2.0
                # ×“××™×•×Ÿ ×œ×©×: ×›××©×¨ ××™×Ÿ ×©×™××•×© ××• section
                if s == 0.0:
                    s += self._name_similarity(cname, gname)
                score[(cname, gname)] = s

        # ×”×ª×××•×ª ×œ×¤×™ ×©×™××•×© ×”××—×œ×§×” ×‘×¤×•× ×§×¦×™×•×ª (××ª×•×š ××ª×•×“×•×ª)
        # ×× ××ª×•×“×•×ª ×”××—×œ×§×” ×§×•×¨××•×ª ×œ×¤×•× ×§×¦×™×•×ª ×‘×§×‘×•×¦×” ×¡×¤×¦×™×¤×™×ª â€“ × ×–×™×– ×œ×©×
        func_name_to_group: Dict[str, str] = {}
        for gname, funcs in groups.items():
            for f in funcs:
                func_name_to_group[f.name] = gname
        for cname, cls in class_by_name.items():
            group_bonus: Dict[str, float] = {}
            for m in cls.methods:
                for called in m.calls:
                    g = func_name_to_group.get(called)
                    if g:
                        group_bonus[g] = group_bonus.get(g, 0.0) + 1.0
            if group_bonus:
                # ×‘×•× ×•×¡ ××©××¢×•×ª×™ ×œ×§×‘×•×¦×” ×¢× ××™×¨×‘ ×”×§×¨×™××•×ª
                best_g, best_v = max(group_bonus.items(), key=lambda kv: kv[1])
                score[(cname, best_g)] = score.get((cname, best_g), 0.0) + 3.0

        # ×”×§×¦××”: ×‘×—×¨ ××ª ×”×§×‘×•×¦×” ×¢× ×”× ×™×§×•×“ ×”×’×‘×•×” ×‘×™×•×ª×¨ ×œ×›×œ Class
        assigned_group_for_class: Dict[str, str] = {}
        for cname in class_by_name.keys():
            best_g = None
            best_s = float("-inf")
            for gname in groups.keys():
                s = score.get((cname, gname), 0.0)
                if s > best_s:
                    best_s = s
                    best_g = gname
            if best_g is None:
                # ×’×™×‘×•×™: ×”×¦××“ ×œ×§×‘×•×¦×” ×”×¨××©×•× ×”
                best_g = next(iter(groups.keys()))
            assigned_group_for_class[cname] = best_g
            groups_classes[best_g].append(class_by_name[cname])

        return groups_classes

    def _inject_cross_module_class_imports(
        self,
        new_files: Dict[str, str],
        class_to_module: Dict[str, str],
    ) -> Dict[str, str]:
        """
        ××–×¨×™×§ import ×™×—×¡×™ ×œ××—×œ×§×•×ª ×©××•×’×“×¨×•×ª ×‘××•×“×•×œ×™× ××—×¨×™× ××š × ××¦××•×ª ×‘×©×™××•×©.
        """
        out: Dict[str, str] = {}
        class_names: Set[str] = set(class_to_module.keys())
        for fn, content in new_files.items():
            if fn in ("__init__.py",) or fn.endswith("_shared.py"):
                out[fn] = content
                continue
            current_stem = Path(fn).stem
            used = self._extract_used_names(content)
            defined_here = self._extract_defined_classes_in_code(content)
            needed = [name for name in used if name in class_names and name not in defined_here and class_to_module.get(name) != current_stem]
            if not needed:
                out[fn] = content
                continue
            per_module: Dict[str, List[str]] = {}
            for name in needed:
                mod = class_to_module.get(name)
                if not mod:
                    continue
                per_module.setdefault(mod, []).append(name)
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2
                        break
            new_imports: List[str] = []
            for module_stem, names in per_module.items():
                names_sorted = sorted(set(names))
                imp = f"from .{module_stem} import {', '.join(names_sorted)}"
                exists = any(ln.strip().startswith(f"from .{module_stem} import") for ln in lines)
                if not exists:
                    new_imports.append(imp)
            if new_imports:
                lines = lines[:insert_idx] + new_imports + [""] + lines[insert_idx:]
            out[fn] = "\n".join(lines) + "\n"
        return out


# Instance ×’×œ×•×‘×œ×™
refactoring_engine = RefactoringEngine()

