"""
×× ×•×¢ ×¨×¤×§×˜×•×¨×™× ×’ ××•×˜×•××˜×™
××‘×¦×¢ ×©×™× ×•×™×™ ××‘× ×” ×‘×§×•×“ ×‘×¦×•×¨×” ×‘×˜×•×—×”
"""

from __future__ import annotations

import ast
import re
import logging
from dataclasses import dataclass, field
import os
from enum import Enum
from typing import Dict, List, Optional, Set, Tuple, Any
from pathlib import Path

logger = logging.getLogger(__name__)


class RefactorType(Enum):
    """×¡×•×’×™ ×¨×¤×§×˜×•×¨×™× ×’ × ×ª××›×™×"""
    SPLIT_FUNCTIONS = "split_functions"  # ×¤×™×¦×•×œ ×§×•×‘×¥ ×’×“×•×œ ×œ×¤×•× ×§×¦×™×•×ª
    EXTRACT_FUNCTIONS = "extract_functions"  # ×—×™×œ×•×¥ ×§×•×“ ×—×•×–×¨
    MERGE_SIMILAR = "merge_similar"  # ××™×–×•×’ ×§×•×“ ×“×•××”
    CONVERT_TO_CLASSES = "convert_to_classes"  # ×”××¨×” ×œ××—×œ×§×•×ª
    DEPENDENCY_INJECTION = "dependency_injection"  # DI


@dataclass
class FunctionInfo:
    """××™×“×¢ ×¢×œ ×¤×•× ×§×¦×™×”"""
    name: str
    start_line: int
    end_line: int
    args: List[str]
    returns: Optional[str]
    decorators: List[str]
    docstring: Optional[str]
    calls: Set[str]
    called_by: Set[str] = field(default_factory=set)
    code: str = ""
    complexity: int = 0
    # ×©×™×•×š ×œ×¤×™ ×¡×¢×™×£ (Section) ×× ×–×•×”×” ××”×§×•×‘×¥ ×”××•× ×•×œ×™×ª×™
    section: Optional[str] = None


@dataclass
class ClassInfo:
    """××™×“×¢ ×¢×œ ××—×œ×§×”"""
    name: str
    start_line: int
    end_line: int
    methods: List[FunctionInfo]
    attributes: List[str]
    base_classes: List[str]
    decorators: List[str]
    docstring: Optional[str]
    # ×§×•×“ ×”××§×•×¨ ×©×œ ×”××—×œ×§×”
    code: str = ""
    # ×©×™×•×š ×œ×¤×™ ×¡×¢×™×£ (Section) ×× ×–×•×”×”
    section: Optional[str] = None


@dataclass
class RefactorProposal:
    """×”×¦×¢×ª ×¨×¤×§×˜×•×¨×™× ×’"""
    refactor_type: RefactorType
    original_file: str
    new_files: Dict[str, str]
    description: str
    changes_summary: List[str]
    warnings: List[str] = field(default_factory=list)
    imports_needed: Dict[str, List[str]] = field(default_factory=dict)


@dataclass
class RefactorResult:
    """×ª×•×¦××ª ×¨×¤×§×˜×•×¨×™× ×’"""
    success: bool
    proposal: Optional[RefactorProposal]
    error: Optional[str] = None
    validation_passed: bool = False


FunctionNode = ast.FunctionDef | ast.AsyncFunctionDef


class CodeAnalyzer:
    """×× ×ª×— ×§×•×“ Python"""

    def __init__(self, code: str, filename: str = "unknown.py"):
        self.code = code
        self.filename = filename
        self.tree: Optional[ast.AST] = None
        self.functions: List[FunctionInfo] = []
        self.classes: List[ClassInfo] = []
        self.imports: List[str] = []
        self.global_vars: List[str] = []
        # ××™×¤×•×™ ×©×•×¨×” -> slug ×©×œ ×¡×¢×™×£, ×œ×¤×™ ×›×•×ª×¨×•×ª ×‘×§×•×‘×¥
        self._sections_by_line: Dict[int, str] = {}

    def analyze(self) -> bool:
        """× ×™×ª×•×— ×”×§×•×“"""
        try:
            self.tree = ast.parse(self.code)
            self._extract_sections()
            self._extract_imports()
            self._extract_functions()
            self._extract_classes()
            self._extract_globals()
            self._calculate_dependencies()
            return True
        except SyntaxError as e:
            logger.error(f"×©×’×™××ª ×ª×—×‘×™×¨ ×‘×§×•×“: {e}")
            return False
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘× ×™×ª×•×—: {e}", exc_info=True)
            return False

    def _extract_imports(self) -> None:
        for node in ast.walk(self.tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    self.imports.append(f"import {alias.name}")
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                names = ", ".join(alias.name for alias in node.names)
                self.imports.append(f"from {module} import {names}")

    def _extract_functions(self) -> None:
        for node in ast.walk(self.tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) and self._is_top_level_function(node):
                func_info = self._parse_function(node)
                # ×©×™×•×š ×œ×¡×¢×™×£ ×œ×¤×™ ××™×§×•× ×”×©×•×¨×” ×‘×§×•×“
                func_info.section = self._get_section_for_line(func_info.start_line)
                self.functions.append(func_info)

    def _extract_classes(self) -> None:
        for node in ast.walk(self.tree):
            if isinstance(node, ast.ClassDef):
                class_info = self._parse_class(node)
                class_info.section = self._get_section_for_line(class_info.start_line)
                self.classes.append(class_info)

    def _extract_globals(self) -> None:
        for node in self.tree.body:
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        self.global_vars.append(target.id)

    def _parse_function(self, node: FunctionNode) -> FunctionInfo:
        args = [arg.arg for arg in node.args.args]
        returns = ast.unparse(node.returns) if node.returns else None
        decorators = [ast.unparse(dec) for dec in node.decorator_list]
        docstring = ast.get_docstring(node)
        calls = self._extract_function_calls(node)
        code_lines = self.code.splitlines()[node.lineno - 1 : node.end_lineno]
        code = "\n".join(code_lines)
        complexity = self._calculate_complexity(node)
        return FunctionInfo(
            name=node.name,
            start_line=node.lineno,
            end_line=node.end_lineno or node.lineno,
            args=args,
            returns=returns,
            decorators=decorators,
            docstring=docstring,
            calls=calls,
            code=code,
            complexity=complexity,
        )

    def _parse_class(self, node: ast.ClassDef) -> ClassInfo:
        methods: List[FunctionInfo] = []
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                methods.append(self._parse_function(item))
        attributes: List[str] = []
        for item in node.body:
            if isinstance(item, ast.Assign):
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        attributes.append(target.id)
        base_classes = [ast.unparse(base) for base in node.bases]
        decorators = [ast.unparse(dec) for dec in node.decorator_list]
        docstring = ast.get_docstring(node)
        code_lines = self.code.splitlines()[node.lineno - 1 : node.end_lineno]
        code = "\n".join(code_lines)
        return ClassInfo(
            name=node.name,
            start_line=node.lineno,
            end_line=node.end_lineno or node.lineno,
            methods=methods,
            attributes=attributes,
            base_classes=base_classes,
            decorators=decorators,
            docstring=docstring,
            code=code,
        )

    def _extract_function_calls(self, node: ast.AST) -> Set[str]:
        calls: Set[str] = set()
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.add(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.add(child.func.attr)
        return calls

    def _calculate_complexity(self, node: ast.AST) -> int:
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(child, ast.BoolOp) and isinstance(child.op, (ast.And, ast.Or)):
                complexity += len(child.values) - 1
        return complexity

    def _is_top_level_function(self, node: FunctionNode) -> bool:
        for parent in ast.walk(self.tree):
            if isinstance(parent, ast.ClassDef):
                if node in parent.body:
                    return False
        return True

    # --- ×–×™×”×•×™ ×›×•×ª×¨×•×ª/×¡×¢×™×¤×™× ×‘×§×•×‘×¥ (×œ××©×œ "# 1) USER MANAGEMENT") ---
    def _extract_sections(self) -> None:
        """
        ×××ª×¨ ×›×•×ª×¨×•×ª ×¡×¢×™×¤×™× ×‘×§×•×‘×¥ ×›×“×™ ×œ×©××¨ ×§×•×”×–×™×” ×¡×× ×˜×™×ª ×‘×¤×™×¦×•×œ.
        ×ª×•××š ×‘×ª×‘× ×™×•×ª ×›×’×•×Ÿ:
        "# 1) USER MANAGEMENT"
        "# 2) PAYMENTS + SUBSCRIPTIONS"
        ×•×›×Ÿ ×•×¨×™××¦×™×•×ª ×¢× ×¨×™×‘×•×™ ×ª×•×•×™ '#'.
        """
        self._sections_by_line = {}
        lines = self.code.splitlines()
        last_section: Optional[str] = None
        for idx, raw in enumerate(lines, start=1):
            line = raw.strip()
            # ×“×œ×’ ×¢×œ ××¤×¨×™×“×™ "#####"
            if not line.startswith("#"):
                continue
            # ×”×¡×¨ ×ª×•×•×™ '#'
            text = line.lstrip("#").strip()
            if not text:
                continue
            # × ×ª××•×š ×‘×ª×‘× ×™×ª "N) " ×‘×ª×—×™×œ×ª ×”×§×• ××š ×œ× × ×“×¨×•×© ×–××ª
            m = re.match(r"^(\d+\)\s*)?(.+)$", text)
            if not m:
                continue
            title = m.group(2).strip()
            slug = self._section_to_slug(title)
            if slug:
                last_section = slug
                self._sections_by_line[idx] = slug

    def _section_to_slug(self, title: str) -> Optional[str]:
        t = title.lower()
        mapping = [
            (("user management", "users"), "users"),
            (("payments", "subscriptions", "billing", "finance"), "finance"),
            (("file system", "files", "storage"), "files"),
            (("inventory", "products"), "inventory"),
            (("network", "api clients", "api"), "api_clients"),
            (("analytics", "reports", "report"), "analytics"),
            (("notifications", "email"), "notifications"),
            (("permissions", "auth"), "permissions"),
            (("workflow", "pipelines"), "workflows"),
            (("debug", "temp", "mixed"), "debug"),
            (("random utilities", "utilities", "utils"), "utils"),
            (("application boot", "main"), "main"),
        ]
        for keys, slug in mapping:
            if any(k in t for k in keys):
                return slug
        return None

    def _get_section_for_line(self, line_no: int) -> Optional[str]:
        if not self._sections_by_line:
            return None
        keys = sorted(self._sections_by_line.keys())
        prev = None
        for k in keys:
            if k <= line_no:
                prev = k
            else:
                break
        return self._sections_by_line.get(prev) if prev is not None else None

    def _calculate_dependencies(self) -> None:
        func_names = {f.name for f in self.functions}
        for func in self.functions:
            for call in func.calls:
                if call in func_names:
                    for other_func in self.functions:
                        if other_func.name == call:
                            other_func.called_by.add(func.name)

    def find_large_functions(self, min_lines: int = 50) -> List[FunctionInfo]:
        large: List[FunctionInfo] = []
        for func in self.functions:
            lines_count = func.end_line - func.start_line + 1
            if lines_count >= min_lines or func.complexity >= 10:
                large.append(func)
        return large

    def find_large_classes(self, min_methods: int = 10) -> List[ClassInfo]:
        return [cls for cls in self.classes if len(cls.methods) >= min_methods]


class RefactoringEngine:
    """×× ×•×¢ ×¨×¤×§×˜×•×¨×™× ×’"""

    def __init__(self) -> None:
        self.analyzer: Optional[CodeAnalyzer] = None
        # ×ª×¦×•×¨×”: ×©×œ×™×˜×” ×‘××¡×¤×¨ ×§×‘×•×¦×•×ª/××•×“×•×œ×™× ×›×“×™ ×œ×× ×•×¢ Oversplitting
        self.preferred_min_groups: int = 3
        self.preferred_max_groups: int = 5
        self.min_functions_per_group: int = 2
        self.absolute_max_groups: int = 8

    def propose_refactoring(
        self,
        code: str,
        filename: str,
        refactor_type: RefactorType,
        layered_mode: Optional[bool] = None,
    ) -> RefactorResult:
        """×”×¦×¢×ª ×¨×¤×§×˜×•×¨×™× ×’"""
        self.analyzer = CodeAnalyzer(code, filename)
        if not self.analyzer.analyze():
            return RefactorResult(
                success=False,
                proposal=None,
                error="×›×©×œ ×‘× ×™×ª×•×— ×”×§×•×“ - ×™×™×ª×›×Ÿ ×©×’×™××ª ×ª×—×‘×™×¨",
            )
        # ×‘×§×©×ª ×©×›×‘×•×ª ×œ×¤×™-×§×¨×™××” (×œ×œ× ×–×™×”×•× ENV ×’×œ×•×‘×œ×™)
        if layered_mode is not None:
            setattr(self, "_layered_mode_override", bool(layered_mode))
        try:
            if refactor_type == RefactorType.SPLIT_FUNCTIONS:
                proposal = self._split_functions()
            elif refactor_type == RefactorType.EXTRACT_FUNCTIONS:
                proposal = self._extract_functions()
            elif refactor_type == RefactorType.MERGE_SIMILAR:
                proposal = self._merge_similar()
            elif refactor_type == RefactorType.CONVERT_TO_CLASSES:
                proposal = self._convert_to_classes()
            elif refactor_type == RefactorType.DEPENDENCY_INJECTION:
                proposal = self._add_dependency_injection()
            else:
                return RefactorResult(success=False, proposal=None, error=f"×¡×•×’ ×¨×¤×§×˜×•×¨×™× ×’ ×œ× × ×ª××š: {refactor_type}")
            validated = self._validate_proposal(proposal)
            return RefactorResult(success=True, proposal=proposal, validation_passed=validated)
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘×¨×¤×§×˜×•×¨×™× ×’: {e}", exc_info=True)
            return RefactorResult(success=False, proposal=None, error=f"×©×’×™××”: {str(e)}")
        finally:
            if hasattr(self, "_layered_mode_override"):
                try:
                    delattr(self, "_layered_mode_override")
                except Exception:
                    pass

    def _split_functions(self) -> RefactorProposal:
        # ××¦×‘ ××™×•×—×“: ×§×•×‘×¥ ××•×“×œ×™× ××•× ×•×œ×™×ª×™ (classes ×‘×œ×‘×“) â€” Safe Decomposition ×œ×“×•××™×™× ×™× ×‘×ª×•×š ×—×‘×™×œ×ª models/
        if self.analyzer and not self.analyzer.functions and self.analyzer.classes:
            filename_stem = Path(self.analyzer.filename).stem
            if filename_stem == "models":
                return self._split_models_monolith()
        groups = self._group_related_functions()
        if len(groups) <= 1:
            raise ValueError("×œ× × ××¦××• ×§×‘×•×¦×•×ª ×¤×•× ×§×¦×™×•×ª × ×¤×¨×“×•×ª. ×”×§×•×“ ×›×‘×¨ ×××•×¨×’×Ÿ ×”×™×˜×‘.")
        new_files: Dict[str, str] = {}
        changes: List[str] = []
        imports_needed: Dict[str, List[str]] = {}
        per_file_filtered_imports: Dict[str, List[str]] = {}
        base_name = Path(self.analyzer.filename).stem
        # ×¢×“×™×¤×•×ª ×œ×¢×§×™×¤×ª ×©×›×‘×•×ª ×œ×¤×™-×§×¨×™××”; ××—×¨×ª â€“ ENV
        override = getattr(self, "_layered_mode_override", None)
        layered_mode = bool(override) if override is not None else (
            str(os.getenv("REFACTOR_LAYERED_MODE", "")).strip().lower() in ("1", "true", "yes", "on")
        )
        # ×”×§×¦××ª ××—×œ×§×•×ª ×œ×§×‘×•×¦×•×ª (Collocation)
        classes_by_group = self._assign_classes_to_groups(groups)
        # ×‘××¦×‘ ×©×›×‘×•×ª (Layered) â€“ ×“×—×•×£ ××ª ×›×œ ×”××—×œ×§×•×ª ×œ×§×•×‘×¥ Leaf ×™×—×™×“
        classes_filename: Optional[str] = None
        if layered_mode and (self.analyzer and self.analyzer.classes):
            classes_filename, classes_file_content = self._build_classes_file("models")
            # × ×©×ª××© ×‘×§×•×‘×¥ ×¡×˜× ×“×¨×˜×™ ×‘×©× models.py ×•×œ× ×‘×©× ××‘×•×¡×¡ ×§×œ×˜
            classes_filename = "models.py"
            new_files[classes_filename] = classes_file_content
            changes.append(f"ğŸ“¦ {classes_filename}: ×¨×™×›×•×– ×™×©×•×™×•×ª (Leaf)")
        # ×‘× ×™×™×ª ×§×‘×¦×™ ×“×•××™×™×Ÿ
        module_stem_by_group: Dict[str, str] = {}
        for group_name, functions in groups.items():
            new_filename = self._choose_filename_for_group(base_name, group_name)
            module_stem_by_group[group_name] = Path(new_filename).stem
            group_classes = [] if layered_mode else classes_by_group.get(group_name, [])
            # ×¡×™× ×•×Ÿ imports ×œ×¤×™ ×©×™××•×© ×××™×ª×™ ×‘×§×•×“ ×”××©×•×œ×‘
            combined_body_parts: List[str] = []
            for c in group_classes:
                combined_body_parts.append(c.code)
            for f in functions:
                combined_body_parts.append(f.code)
            group_code_body = "\n\n".join(combined_body_parts)
            filtered_imports = self._filter_imports_for_code(self.analyzer.imports, group_code_body)
            per_file_filtered_imports[new_filename] = filtered_imports
            file_content = self._build_file_content(functions, imports=filtered_imports, classes=group_classes)
            new_files[new_filename] = file_content
            changes.append(f"ğŸ“¦ {new_filename}: {len(group_classes)} ××—×œ×§×•×ª, {len(functions)} ×¤×•× ×§×¦×™×•×ª")
            # ×©××™×¨×” ×œ××—×•×¨: imports_needed ××›×™×œ ××ª ×”-imports ×”××§×•×¨×™×™× (×œ×¦×¨×›×™ ×ª××™××•×ª ×‘×“×•×—×•×ª)
            imports_needed[new_filename] = self.analyzer.imports.copy()
        init_content = self._build_init_file(list(new_files.keys()))
        new_files["__init__.py"] = init_content
        changes.append("ğŸ“¦ __init__.py: ××™×™×¦× ××ª ×›×œ ×”-API")

        # ××™×—×•×“ imports ××©×•×ª×¤×™× ×œ×§×•×‘×¥ shared ×•×”×—×œ×¤×ª× ×‘-import ×™×—×™×“
        original_keys = set(new_files.keys())
        new_files = self._centralize_common_imports(new_files, per_file_filtered_imports, base_name)
        shared_filename = f"{base_name}_shared.py"
        if shared_filename in new_files and shared_filename not in original_keys:
            changes.append(f"ğŸ“¦ {shared_filename}: ×™×™×‘×•× ××©×•×ª×£ ××¨×•×›×–")

        # ××™×Ÿ ×™×•×ª×¨ ×§×•×‘×¥ ××—×œ×§×•×ª ××¨×›×–×™ â€“ ××—×œ×§×•×ª ××¨×•×›×–×•×ª ×œ×¤×™ ×“×•××™×™×Ÿ (Collocation)

        # ×”×–×¨×§×ª ×™×‘×•× ×œ×¤×•× ×§×¦×™×•×ª ×‘×™×Ÿ-××•×“×•×œ×™×•×ª (Cross-module function imports)
        func_to_module: Dict[str, str] = {}
        for group_name, functions in groups.items():
            module_stem = module_stem_by_group.get(group_name, f"{base_name}_{group_name}")
            for f in functions:
                func_to_module[f.name] = module_stem
        new_files = self._inject_function_imports(new_files, func_to_module)
        # ×”×–×¨×§×ª ×™×‘×•× ×œ××—×œ×§×•×ª ×‘×™×Ÿ-××•×“×•×œ×™×•×ª (Cross-module class imports)
        if layered_mode and classes_filename:
            # ×‘××¦×‘ ×©×›×‘×•×ª â€“ ×›×œ ×”××—×œ×§×•×ª ×‘-models.py, ×”×–×¨×§ import ××× ×•
            new_files = self._inject_class_imports(new_files, classes_filename)
        else:
            class_to_module: Dict[str, str] = {}
            for group_name, classes in classes_by_group.items():
                module_stem = module_stem_by_group.get(group_name, f"{base_name}_{group_name}")
                for c in classes:
                    class_to_module[c.name] = module_stem
            if class_to_module:
                new_files = self._inject_cross_module_class_imports(new_files, class_to_module)

        # DRY-RUN: ×–×™×”×•×™ ×•×× ×™×¢×ª ×ª×œ×•×ª ××¢×’×œ×™×ª ×‘×™×Ÿ ×”××•×“×•×œ×™× ×©× ×•×¦×¨×•
        cycle_warnings: List[str] = []
        new_files, merged_pairs = self._resolve_circular_imports(new_files)
        if merged_pairs:
            for a, b in merged_pairs:
                cycle_warnings.append(f"â™»ï¸ ×¤×•×¨×§×” ×ª×œ×•×ª ××¢×’×œ×™×ª ×‘×××¦×¢×•×ª ××™×–×•×’ ×”××•×“×•×œ×™×: {a} â‡„ {b}")
            # ×¢×“×›×•×Ÿ __init__.py ×œ××—×¨ ××™×–×•×’×™×
            module_file_names = [fn for fn in new_files.keys() if fn.endswith('.py') and fn != '__init__.py']
            new_files['__init__.py'] = self._build_init_file(module_file_names)

        # × ×™×§×•×™ ×¤×•×¡×˜-×¤×™×¦×•×œ: ×”×¡×¨×ª imports ××™×•×ª×¨×™× ×‘×¨××ª ×§×•×‘×¥
        new_files = self.post_refactor_cleanup(new_files)
        description = (
            f"ğŸ—ï¸ ××¦××ª×™ {len(self.analyzer.classes)} ××—×œ×§×•×ª ×•-{len(self.analyzer.functions)} ×¤×•× ×§×¦×™×•×ª.\n\n"
            f"×”×¦×¢×ª ×¤×™×¦×•×œ ×œ×¤×™ ×“×•××™×™×Ÿ (Collocation):\n"
            f"ğŸ“¦ {self.analyzer.filename} â†’\n"
        )
        for fname in new_files.keys():
            description += f"   â”œâ”€â”€ {fname}\n"
        description += "\nâœ… ××—×œ×§×•×ª ×•×¤×•× ×§×¦×™×•×ª ××¨×•×›×–×•×ª ×™×—×“ ×‘×§×‘×¦×™ ×“×•××™×™×Ÿ; ××™×Ÿ ×§×•×‘×¥ ××—×œ×§×•×ª ×’× ×¨×™"
        warnings: List[str] = []
        if len(self.analyzer.global_vars) > 0:
            warnings.append(
                f"âš ï¸ ×™×© {len(self.analyzer.global_vars)} ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× - ×¢×œ×•×œ ×œ×”×™×•×ª ×¦×•×¨×š ×‘×”×ª×××” ×™×“× ×™×ª"
            )
        warnings.extend(cycle_warnings)
        return RefactorProposal(
            refactor_type=RefactorType.SPLIT_FUNCTIONS,
            original_file=self.analyzer.filename,
            new_files=new_files,
            description=description,
            changes_summary=changes,
            warnings=warnings,
            imports_needed=imports_needed,
        )

    def _group_related_functions(self) -> Dict[str, List[FunctionInfo]]:
        """
        ×§×™×‘×•×¥ ×¤×•× ×§×¦×™×•×ª ×œ×¤×™ ×§×•×”×–×™×” ×¡×× ×˜×™×ª:
        1) ×¡×¢×™×£ (Section) ×× ×–×•×”×” ××ª×•×š ×›×•×ª×¨×•×ª ×”×§×•×‘×¥
        2) ×“×•××™×™×Ÿ (IO/Compute/Helpers)
        3) Prefix ×•×ª×œ×•×™×•×ª
        ×ª×•×š ×©××™×¨×” ×¢×œ ×˜×•×•×— ×§×‘×•×¦×•×ª ××•×¢×“×£ ×•×”×™×× ×¢×•×ª ××¤×™×¦×•×œ ×™×ª×¨.
        """
        if not self.analyzer:
            return {}
        functions = list(self.analyzer.functions)
        if len(functions) <= 1:
            section_groups = self._scaffold_groups_from_sections(functions)
            if section_groups:
                return section_groups
            return {"module": functions}
        if len(functions) == 2:
            return {
                f"group_{idx+1}_{func.name}": [func]
                for idx, func in enumerate(functions)
            }

        # 1) ×§×™×‘×•×¥ ×œ×¤×™ ×¡×¢×™×£ (×× ×§×™×™×). ×× ×™×© ×¤×•× ×§×¦×™×•×ª ×œ×œ× ×¡×¢×™×£ â€” × ×©××¨ ××•×ª×Ÿ ×¢"×™ ×§×™×‘×•×¥ ×“×•××™×™× ×™ ×•×”×•×¡×¤×”.
        section_groups = self._group_by_section(functions)
        if section_groups:
            leftovers = [f for f in functions if not f.section]
            if leftovers:
                domain_for_leftovers = self._group_by_domain(leftovers)
                for k, v in domain_for_leftovers.items():
                    section_groups.setdefault(k, []).extend(v)
        else:
            # ××™×Ÿ ×›×•×ª×¨×•×ª ×›×œ×œ â€” ×§×™×‘×•×¥ ×“×•××™×™× ×™ ××œ×
            section_groups = self._group_by_domain(functions)

        # 2) ×§×™×‘×•×¥ × ×•×¡×£ ×œ×¤×™ prefix ×‘×ª×•×š ×›×œ ×“×•××™×™×Ÿ, ×œ×©××™×¨×ª ×§×¨×‘×” ×¡×× ×˜×™×ª
        refined: Dict[str, List[FunctionInfo]] = {}
        for domain, funcs in section_groups.items():
            sub = self._group_by_prefix(funcs)
            large_sub = {f"{domain}_{k}": v for k, v in sub.items() if len(v) >= self.min_functions_per_group}
            if large_sub:
                refined.update(large_sub)
                leftovers = [f for k, v in sub.items() if len(v) < self.min_functions_per_group for f in v]
                if leftovers:
                    refined.setdefault(domain, []).extend(leftovers)
            else:
                refined[domain] = funcs

        # 3) ××™×–×•×’ ×œ×¤×™ ×ª×œ×•×ª (affinity) ×›×“×™ ×œ××—×“ ×§×‘×•×¦×•×ª ×§×¨×•×‘×•×ª
        refined = self._merge_by_dependency_affinity(refined)

        # 4) ××™×–×•×’ ×§×‘×•×¦×•×ª ×§×˜× ×•×ª ××“×™
        refined = self._merge_small_groups(refined)

        # 5) ×”×’×‘×œ×ª ××¡×¤×¨ ×§×‘×•×¦×•×ª ×œ×˜×•×•×— ×”××•×¢×“×£/××§×¡×™××œ×™
        refined = self._limit_group_count(refined)

        # ×”×‘×˜×—×ª ××™× ×™××•× ×©×ª×™ ×§×‘×•×¦×•×ª ×›×©×™×© ××¡×¤×™×§ ×¤×•× ×§×¦×™×•×ª
        if len(refined) < 2 and len(functions) >= 4:
            fallback = self._group_by_prefix(functions)
            sorted_groups = sorted(fallback.items(), key=lambda kv: -len(kv[1]))
            if len(sorted_groups) >= 2:
                # ×§×— ×©×ª×™ ×§×‘×•×¦×•×ª ××•×‘×™×œ×•×ª ×•××– ××™×–×•×’ ×©×œ ×™×ª×¨ ×”×§×‘×•×¦×•×ª â€” ×œ× ×–×•×¨×§×™× ×¤×•× ×§×¦×™×•×ª!
                refined = {
                    sorted_groups[0][0]: list(sorted_groups[0][1]),
                    sorted_groups[1][0]: list(sorted_groups[1][1]),
                }
                for name, funcs in sorted_groups[2:]:
                    # ×‘×—×¨ ×™×¢×“ ××™×–×•×’ ×œ×¤×™ affinity ××œ ××—×ª ×”×§×‘×•×¦×•×ª ×”×§×™×™××•×ª
                    best_target = None
                    best_score = -1.0
                    for target_name, target_funcs in refined.items():
                        score = self._group_affinity(funcs, target_funcs)
                        if score > best_score:
                            best_score = score
                            best_target = target_name
                    if best_target is None:
                        # ×’×™×‘×•×™: ××–×’ ×œ×§×‘×•×¦×” ×”×§×˜× ×” ×™×•×ª×¨ ×›×“×™ ×œ××–×Ÿ
                        best_target = min(refined.keys(), key=lambda k: len(refined[k]))
                    refined[best_target].extend(funcs)
            else:
                refined = {"module": functions}

        # ×™×™×¦×•×‘ ×©××•×ª ×§×‘×•×¦×•×ª
        stable: Dict[str, List[FunctionInfo]] = {}
        seen: Set[str] = set()
        for name, funcs in refined.items():
            base = re.sub(r"[^a-z0-9_]", "_", name.lower())
            if base in seen:
                idx = 2
                while f"{base}{idx}" in seen:
                    idx += 1
                base = f"{base}{idx}"
            seen.add(base)
            stable[base] = funcs
        return stable

    def _scaffold_groups_from_sections(self, functions: List[FunctionInfo]) -> Optional[Dict[str, List[FunctionInfo]]]:
        """
        ×›××©×¨ ×™×© ××¢×˜ ×××•×“ ×¤×•× ×§×¦×™×•×ª ××š ×§×™×™××•×ª ××—×œ×§×•×ª ×‘×¡×¢×™×¤×™× ×©×•× ×™×, × ×‘× ×” ×§×‘×•×¦×•×ª ×œ×¤×™ ×”×¡×¢×™×¤×™×
        ×›×“×™ ×œ××¤×©×¨ ×¤×™×¦×•×œ ×©×¢×“×™×™×Ÿ ×™×©××¨ ××ª ×”×”×§×©×¨×™× ×”×¡×× ×˜×™×™× (×œ××©×œ Users ×œ×¢×•××ª Analytics).
        """
        if not self.analyzer:
            return None
        entries: Dict[str, Dict[str, Any]] = {}

        def _ensure_entry(section: str, start_line: int) -> Dict[str, Any]:
            data = entries.setdefault(section, {"start": start_line, "funcs": []})
            data["start"] = min(data["start"], start_line)
            return data

        for func in functions:
            if func.section:
                entry = _ensure_entry(func.section, func.start_line)
                entry["funcs"].append(func)
        for cls in self.analyzer.classes:
            if cls.section:
                _ensure_entry(cls.section, cls.start_line)
        if len(entries) < 2:
            return None
        ordered_sections = sorted(entries.items(), key=lambda kv: kv[1]["start"])
        groups: Dict[str, List[FunctionInfo]] = {
            section: list(entries[section]["funcs"]) for section, _ in ordered_sections
        }
        leftovers = [f for f in functions if not f.section]
        if leftovers:
            first_section = ordered_sections[0][0]
            groups.setdefault(first_section, []).extend(leftovers)
        return groups

    def _group_by_section(self, functions: List[FunctionInfo]) -> Dict[str, List[FunctionInfo]]:
        """×§×™×‘×•×¥ ×¤×•× ×§×¦×™×•×ª ×œ×¤×™ ×¡×¢×™×£ (Section) ×× ×§×™×™×."""
        groups: Dict[str, List[FunctionInfo]] = {}
        for f in functions:
            if f.section:
                groups.setdefault(f.section, []).append(f)
        return groups

    def _group_by_dependencies(self) -> Dict[str, List[FunctionInfo]]:
        groups: Dict[str, List[FunctionInfo]] = {}
        visited: Set[str] = set()
        for func in self.analyzer.functions:
            if func.name in visited:
                continue
            group_name = func.name.replace('_', '')[:15]
            group = [func]
            visited.add(func.name)
            for other_func in self.analyzer.functions:
                if other_func.name in visited:
                    continue
                if (
                    func.name in other_func.calls
                    or other_func.name in func.calls
                    or func.name in other_func.called_by
                    or other_func.name in func.called_by
                ):
                    group.append(other_func)
                    visited.add(other_func.name)
            if len(group) >= 2:
                groups[group_name] = group
        return groups

    def _split_by_size(self) -> Dict[str, List[FunctionInfo]]:
        max_funcs_per_file = 5
        groups: Dict[str, List[FunctionInfo]] = {}
        for i in range(0, len(self.analyzer.functions), max_funcs_per_file):
            group_name = f"module{i // max_funcs_per_file + 1}"
            groups[group_name] = self.analyzer.functions[i : i + max_funcs_per_file]
        return groups

    # ==== ×§×™×‘×•×¥ ××©×•×¤×¨: ×“×•××™×™×Ÿ/Prefix/×ª×œ×•×ª ====

    def _tokenize_name(self, name: str) -> List[str]:
        parts = re.split(r'[_]|(?=[A-Z])', name)
        return [p.lower() for p in parts if p]

    def _classify_function_domain(self, func: FunctionInfo) -> str:
        """×¡×™×•×•×’ ×¤×•× ×§×¦×™×” ×œ-domain ×‘×¡×™×¡×™: io / compute / helpers / other."""
        tokens = self._tokenize_name(func.name)
        calls = {c.lower() for c in func.calls}
        io_keywords = {"load", "save", "fetch", "read", "write", "open", "close", "connect", "request",
                       "download", "upload", "send", "receive", "persist", "store", "serialize", "deserialize"}
        helper_keywords = {"helper", "util", "utils", "format", "convert", "parse", "normalize", "validate", "cleanup"}
        if any(tok in io_keywords for tok in tokens):
            return "io"
        if any(tok in helper_keywords for tok in tokens):
            return "helpers"
        io_calls = {"open", "requests", "httpx", "urllib", "json", "yaml", "pickle", "asyncio", "sqlite3", "psycopg2"}
        if calls & io_calls:
            return "io"
        return "compute"

    def _group_by_domain(self, functions: List[FunctionInfo]) -> Dict[str, List[FunctionInfo]]:
        groups: Dict[str, List[FunctionInfo]] = {"io": [], "compute": [], "helpers": []}
        for f in functions:
            domain = self._classify_function_domain(f)
            groups.setdefault(domain, []).append(f)
        return {k: v for k, v in groups.items() if v}

    def _group_by_prefix(self, functions: List[FunctionInfo]) -> Dict[str, List[FunctionInfo]]:
        groups: Dict[str, List[FunctionInfo]] = {}
        for func in functions:
            tokens = self._tokenize_name(func.name)
            prefix = tokens[0] if tokens else "module"
            groups.setdefault(prefix, []).append(func)
        return groups

    def _name_similarity(self, a: str, b: str) -> float:
        ta = set(self._tokenize_name(a))
        tb = set(self._tokenize_name(b))
        if not ta or not tb:
            return 0.0
        inter = len(ta & tb)
        union = len(ta | tb)
        return inter / union if union else 0.0

    def _group_affinity(self, g1: List[FunctionInfo], g2: List[FunctionInfo]) -> float:
        if not g1 or not g2:
            return 0.0
        name_score = 0.0
        dep_score = 0.0
        pairs = 0
        for f1 in g1:
            for f2 in g2:
                pairs += 1
                name_score += self._name_similarity(f1.name, f2.name)
                if (f1.name in f2.calls) or (f2.name in f1.calls) or (f1.name in f2.called_by) or (f2.name in f1.called_by):
                    dep_score += 1.0
        if pairs == 0:
            return 0.0
        return 0.6 * (name_score / pairs) + 0.4 * (dep_score / pairs)

    def _merge_by_dependency_affinity(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        items = list(groups.items())
        if len(items) <= 1:
            return groups
        changed = True
        while changed:
            changed = False
            smallest = min(items, key=lambda kv: len(kv[1]))
            if len(smallest[1]) >= self.min_functions_per_group or len(items) <= self.preferred_min_groups:
                break
            best_idx = None
            best_score = -1.0
            for i, (name, funcs) in enumerate(items):
                if name == smallest[0]:
                    continue
                score = self._group_affinity(smallest[1], funcs)
                if score > best_score:
                    best_score = score
                    best_idx = i
            if best_idx is not None and best_score >= 0.1:
                target_name, target_funcs = items[best_idx]
                target_funcs.extend(smallest[1])
                items = [(n, fs) for (n, fs) in items if n != smallest[0]]
                changed = True
        return dict(items)

    def _merge_small_groups(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        items = list(groups.items())
        # ×× ×™×© ×¨×§ 2 ×§×‘×•×¦×•×ª â€” × ×©××™×¨ ××•×ª×Ÿ ×›×“×™ ×œ×× ×•×¢ ×”×ª××–×’×•×ª ×œ××•×“×•×œ ×™×—×™×“
        if len(items) <= 2:
            return groups
        large: List[Tuple[str, List[FunctionInfo]]] = [(n, fs) for n, fs in items if len(fs) >= self.min_functions_per_group]
        small: List[Tuple[str, List[FunctionInfo]]] = [(n, fs) for n, fs in items if len(fs) < self.min_functions_per_group]
        if not small:
            return groups
        if not large:
            return groups
        for sname, sfuncs in small:
            best_name = None
            best_score = -1.0
            for lname, lfuncs in large:
                score = self._group_affinity(sfuncs, lfuncs)
                if score > best_score:
                    best_score = score
                    best_name = lname
            if best_name is None:
                best_name = large[0][0]
            for i, (lname, lfuncs) in enumerate(large):
                if lname == best_name:
                    lfuncs.extend(sfuncs)
                    large[i] = (lname, lfuncs)
                    break
        return dict(large)

    def _limit_group_count(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        items = list(groups.items())
        while len(items) > self.absolute_max_groups:
            best_pair = None
            best_score = -1.0
            for i in range(len(items)):
                for j in range(i + 1, len(items)):
                    score = self._group_affinity(items[i][1], items[j][1])
                    if score > best_score:
                        best_score = score
                        best_pair = (i, j)
            if best_pair is None:
                break
            i, j = best_pair
            items[i] = (items[i][0], items[i][1] + items[j][1])
            del items[j]
        if len(items) > self.preferred_max_groups:
            while len(items) > self.preferred_max_groups:
                smallest_idx = min(range(len(items)), key=lambda k: len(items[k][1]))
                best_idx = None
                best_score = -1.0
                for i in range(len(items)):
                    if i == smallest_idx:
                        continue
                    score = self._group_affinity(items[smallest_idx][1], items[i][1])
                    if score > best_score:
                        best_score = score
                        best_idx = i
                if best_idx is None:
                    break
                items[best_idx] = (items[best_idx][0], items[best_idx][1] + items[smallest_idx][1])
                del items[smallest_idx]
        return dict(items)

    def _merge_singletons_for_oop(self, groups: Dict[str, List[FunctionInfo]]) -> Dict[str, List[FunctionInfo]]:
        """
        ×××–×’ ×§×‘×•×¦×•×ª ×§×˜× ×•×ª (×¤×—×•×ª ×-min_functions_per_group) ××œ ×§×‘×•×¦×” ×“×•××”,
        ×›×“×™ ×©×œ× ×™××‘×“×• ×¤×•× ×§×¦×™×•×ª ×›××©×¨ ××—×œ×§×•×ª × ×•×¦×¨×•×ª ×¨×§ ×¢×‘×•×¨ ×§×‘×•×¦×•×ª ×¢× 2+ ×¤×•× ×§×¦×™×•×ª.
        """
        if len(groups) < 2:
            return groups
        # ×¢×‘×•×“×” ×¢×œ ×”×¢×ª×§ ×›×“×™ ×œ×©× ×•×ª ×‘×‘×˜×—×”
        names = list(groups.keys())
        for name in names:
            funcs = groups.get(name, [])
            if len(funcs) >= self.min_functions_per_group:
                continue
            # ××¦× ×™×¢×“ ××™×–×•×’
            best_name = None
            best_score = -1.0
            for target_name, target_funcs in groups.items():
                if target_name == name:
                    continue
                score = self._group_affinity(funcs, target_funcs)
                if score > best_score:
                    best_score = score
                    best_name = target_name
            if best_name is None:
                # ×’×™×‘×•×™: ×‘×—×¨ ××ª ×”×§×‘×•×¦×” ×”×’×“×•×œ×” ×‘×™×•×ª×¨
                best_name = max((k for k in groups.keys() if k != name), key=lambda k: len(groups[k]))
            groups[best_name].extend(funcs)
            if name in groups:
                del groups[name]
        return groups
    # ==== Safe Decomposition for models.py (classes-only) ====
    def _classify_class_domain(self, cls: ClassInfo) -> str:
        """
        ××¡×•×•×’ ××—×œ×§×” ×œ×“×•××™×™×Ÿ ×‘×¡×™×¡×™ ×›××©×¨ ××™×Ÿ ×¤×•× ×§×¦×™×•×ª ×˜×•×¤-×œ×‘×œ:
        core / billing / inventory / network / workflows.
        ×‘×¨×™×¨×ª ××—×“×œ: core.
        """
        name_l = cls.name.lower()
        # ×¡×™×× ×™× ×—×–×§×™×
        if any(k in name_l for k in ("subscription", "payment", "billing", "gateway")):
            return "billing"
        if any(k in name_l for k in ("product", "inventory", "stock")):
            return "inventory"
        if any(k in name_l for k in ("api", "client", "http", "network")):
            return "network"
        if any(k in name_l for k in ("workflow", "pipeline")):
            return "workflows"
        # ×œ×™×‘×”/××©×ª××©×™×/×”×¨×©××•×ª/××™××™×™×œ
        if any(k in name_l for k in ("user", "permission", "email", "manager")):
            return "core"
        return "core"

    def _extract_module_global_assignments(self) -> Tuple[List[str], str]:
        """
        ××—×–×™×¨ (×©××•×ª ×’×œ×•×‘×œ×™×™× ×œ×¤×™ ×¡×“×¨ ×”×•×¤×¢×”, ×§×•×“ ×”×”×§×¦××•×ª) ××ª×•×š ×”×§×•×‘×¥ ×”××§×•×¨×™ ×‘×¨××ª ××•×“×•×œ.
        ××©××© ×‘×©×œ×‘ Safe Decomposition ×œ-models.py ×›×“×™ ×œ×©××¨ ×§×‘×•×¢×™×/××©×ª× ×™× ×’×œ×•×‘×œ×™×™×.
        """
        if not self.analyzer or not getattr(self.analyzer, "tree", None):
            return [], ""
        names_in_order: List[str] = []
        seen: Set[str] = set()
        code_blocks: List[str] = []
        lines = self.analyzer.code.splitlines()
        for node in getattr(self.analyzer.tree, "body", []):  # type: ignore[attr-defined]
            if isinstance(node, (ast.Assign, ast.AnnAssign)):
                # ×©×—×–×•×¨ ×§×•×“ ××§×•×¨ ×©×œ ×”×”×§×¦××”
                start = max(1, getattr(node, "lineno", 1))
                end = getattr(node, "end_lineno", start)
                snippet = "\n".join(lines[start - 1 : end])
                code_blocks.append(snippet)
                # ××™×¡×•×£ ×©××•×ª
                targets: List[ast.AST] = []
                if isinstance(node, ast.Assign):
                    targets = list(node.targets)
                else:
                    targets = [node.target]  # type: ignore[attr-defined]
                for t in targets:
                    for n in ast.walk(t):
                        if isinstance(n, ast.Name) and n.id not in seen:
                            names_in_order.append(n.id)
                            seen.add(n.id)
        return names_in_order, ("\n".join(code_blocks).strip() + ("\n" if code_blocks else ""))

    def _split_models_monolith(self) -> RefactorProposal:
        """
        ×¤×™×¦×•×œ ×‘×˜×•×— ×©×œ ×§×•×‘×¥ models.py ××•× ×•×œ×™×ª×™ ×œ×ª×ª-××•×“×•×œ×™× ×“×•××™×™× ×™×™× ×ª×—×ª models/.
        - core.py: User, UserManager, PermissionSystem, EmailService ×•×›×•'
        - billing.py: SubscriptionManager, PaymentGateway ×•×›×•'
        - inventory.py: Product, Inventory ×•×›×•'
        - network.py/workflows.py ×œ×¤×™ ×”×¦×•×¨×š
        """
        assert self.analyzer is not None
        classes = list(self.analyzer.classes or [])
        if not classes:
            raise ValueError("×œ× × ××¦××• ××—×œ×§×•×ª ×œ×¤×™×¦×•×œ ×‘×ª×•×š models.py")
        # ×§×™×‘×•×¥ ×œ×¤×™ ×“×•××™×™×Ÿ
        domain_to_classes: Dict[str, List[ClassInfo]] = {}
        for c in classes:
            domain = self._classify_class_domain(c)
            domain_to_classes.setdefault(domain, []).append(c)
        # ×¡×“×¨ ×™×¦×™×‘ ×œ×”×¦×’×”
        preferred_order = ["core", "billing", "inventory", "network", "workflows"]
        ordered_domains = [d for d in preferred_order if d in domain_to_classes] + [
            d for d in domain_to_classes.keys() if d not in preferred_order
        ]
        new_files: Dict[str, str] = {}
        changes: List[str] = []
        # ×—×™×œ×•×¥ ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× ×‘×¨××ª ×”××•×“×•×œ ×›×“×™ ×œ×©××¨× ×‘×¤×™×¦×•×œ (×œ×× ×™×¢×ª NameError)
        global_names, globals_code = self._extract_module_global_assignments()
        # ×‘× ×™×™×ª ×§×‘×¦×™× ×ª×—×ª models/
        for domain in ordered_domains:
            cls_list = domain_to_classes.get(domain, [])
            if not cls_list:
                continue
            # ×¡×™× ×•×Ÿ imports ×œ×¤×™ ×©×™××•×© ×‘××—×œ×§×•×ª ×”×“×•××™×™×Ÿ
            code_body = "\n\n".join(c.code for c in cls_list)
            filtered_imports = self._filter_imports_for_code(self.analyzer.imports, code_body)
            # ×‘×§×•×‘×¥ core × ×–×¨×™×§ ××ª ×”×”×§×¦××•×ª ×”×’×œ×•×‘×œ×™×•×ª ××—×¨×™ imports ×•×œ×¤× ×™ ×”××—×œ×§×•×ª
            if domain == "core" and globals_code.strip():
                title = "××—×œ×§×•×ª: " + ", ".join(c.name for c in cls_list)
                parts: List[str] = []
                parts.append(f'"""\n××•×“×•×œ ×¢×‘×•×¨: {title}\n"""\n')
                parts.extend(filtered_imports)
                parts.append("")
                parts.append(globals_code.rstrip())
                parts.append("")
                for c in cls_list:
                    parts.append(c.code)
                    parts.append("\n")
                content = "\n".join(parts)
            else:
                content = self._build_file_content(functions=[], imports=filtered_imports, classes=cls_list)
            filename = f"models/{domain}.py"
            new_files[filename] = content
            changes.append(f"ğŸ“¦ {filename}: {len(cls_list)} ××—×œ×§×•×ª")
        # __init__ ×œ×—×‘×™×œ×ª models/
        models_module_files = [fn for fn in new_files.keys() if fn.startswith("models/") and fn.endswith(".py")]
        new_files["models/__init__.py"] = self._build_init_file(models_module_files)
        changes.append("ğŸ“¦ models/__init__.py: ××™×™×¦× ××ª ×›×œ ×”×™×©×•×™×•×ª")
        # ×”×–×¨×§×ª ×™×‘×•× ×‘×™×Ÿ-××•×“×•×œ×™ ×œ××—×œ×§×•×ª (×œ××©×œ billing â†’ core)
        class_to_module: Dict[str, str] = {}
        for domain, cls_list in domain_to_classes.items():
            for c in cls_list:
                class_to_module[c.name] = domain
        new_files = self._inject_cross_module_class_imports(new_files, class_to_module)
        # ×”×–×¨×§×ª ×™×‘×•× ×œ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× ×©× ×©××¨×• ×‘-core ××œ ××•×“×•×œ×™× ××—×¨×™× ×”×¦×•×¨×›×™× ××•×ª×
        if global_names:
            new_files = self._inject_global_imports(new_files, set(global_names), source_module_stem="core")
        # DRY-RUN: ×–×™×”×•×™/×¤×™×¨×•×§ ××¢×’×œ×™×•×ª ×‘×ª×•×š models/ ×‘×œ×‘×“
        subset = {k: v for k, v in new_files.items() if k.startswith("models/")}
        subset, merged_pairs = self._resolve_circular_imports(subset)
        if merged_pairs:
            # ×¢×“×›×•×Ÿ __init__.py ×©×œ models/ ×œ××—×¨ ××™×–×•×’
            module_file_names = [fn for fn in subset.keys() if fn.endswith(".py") and not fn.endswith("__init__.py")]
            subset["models/__init__.py"] = self._build_init_file(module_file_names)
        # ××™×–×•×’ ×—×–×¨×” ××œ ×”××¤×” ×”×›×•×œ×œ×ª
        for k in list(new_files.keys()):
            if k.startswith("models/"):
                del new_files[k]
        new_files.update(subset)
        # × ×™×§×•×™ imports ××™×•×ª×¨×™×
        new_files = self.post_refactor_cleanup(new_files)
        description = "ğŸ—ï¸ ×¤×™×¦×•×œ ×‘×˜×•×— ×©×œ models.py ×œ×ª×ª-××•×“×•×œ×™× ×“×•××™×™× ×™×™× ×ª×—×ª models/:\n"
        for fn in sorted(new_files.keys()):
            if fn.startswith("models/") and fn.endswith(".py"):
                description += f"   â”œâ”€â”€ {fn}\n"
        # ××–×”×¨×•×ª
        warnings: List[str] = []
        if global_names:
            warnings.append(f"â„¹ï¸ × ×©××¨×• {len(global_names)} ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× ××ª×•×š models.py ×‘×ª×•×š models/core.py.")
        return RefactorProposal(
            refactor_type=RefactorType.SPLIT_FUNCTIONS,
            original_file=self.analyzer.filename,
            new_files=new_files,
            description=description,
            changes_summary=changes,
            warnings=warnings,
            imports_needed={},
        )
    def _build_file_content(
        self,
        functions: List[FunctionInfo],
        imports: Optional[List[str]] = None,
        classes: Optional[List[ClassInfo]] = None,
    ) -> str:
        """×‘×•× ×” ×ª×•×›×Ÿ ×§×•×‘×¥ ×—×“×© ×¢×‘×•×¨ ×§×‘×•×¦×ª ×“×•××™×™×Ÿ ×¢× ××—×œ×§×•×ª ×•×¤×•× ×§×¦×™×•×ª ×•×‘×œ×•×§ imports ××¡×•× ×Ÿ."""
        content_parts: List[str] = []
        func_names = ", ".join(f.name for f in functions) if functions else ""
        class_names = ", ".join(c.name for c in (classes or [])) if classes else ""
        title_parts: List[str] = []
        if class_names:
            title_parts.append(f"××—×œ×§×•×ª: {class_names}")
        if func_names:
            title_parts.append(f"×¤×•× ×§×¦×™×•×ª: {func_names}")
        title = " | ".join(title_parts) if title_parts else "×“×•××™×™×Ÿ"
        # ×ª××™××•×ª ×œ××—×•×¨: ×©××•×¨ ××ª ×”×¡××Ÿ "××•×“×•×œ ×¢×‘×•×¨" ×©× ×˜×¢×Ÿ ×‘×˜×¡×˜×™×
        content_parts.append(f'"""\n××•×“×•×œ ×¢×‘×•×¨: {title}\n"""\n')
        imports_list = list(imports or self.analyzer.imports)
        content_parts.extend(imports_list)
        content_parts.append("\n")
        # ×¡×“×¨: ××—×œ×§×•×ª ×ª×—×™×œ×”, ××—×¨ ×›×š ×¤×•× ×§×¦×™×•×ª â€“ ××¤×—×™×ª ×¦×•×¨×š ×‘-import ×¤× ×™××™
        for cls in (classes or []):
            content_parts.append(cls.code)
            content_parts.append("\n\n")
        for func in functions:
            content_parts.append(func.code)
            content_parts.append("\n\n")
        return "\n".join(content_parts)

    def _build_init_file(self, filenames: List[str]) -> str:
        content = '"""\n××™× ×“×§×¡ ××¨×›×–×™ ×œ×›×œ ×”×¤×•× ×§×¦×™×•×ª\n"""\n\n'
        for fname in filenames:
            if os.path.basename(fname) == "__init__.py":
                continue
            module_name = Path(fname).stem
            content += f"from .{module_name} import *\n"
        return content

    def _extract_functions(self) -> RefactorProposal:
        duplicates: List[Dict[str, Any]] = self._find_code_duplication()
        if not duplicates:
            raise ValueError("×œ× × ××¦× ×§×•×“ ×—×•×–×¨ ××¡×¤×™×§ ××©××¢×•×ª×™ ×œ×—×™×œ×•×¥")
        new_files: Dict[str, str] = {}
        changes: List[str] = []
        utils_content = self._build_utils_from_duplicates(duplicates)
        new_files["utils.py"] = utils_content
        changes.append(f"ğŸ“¦ utils.py: {len(duplicates)} ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×—×“×©×•×ª")
        updated_original = self._replace_duplicates_with_calls(duplicates)
        new_files[self.analyzer.filename] = updated_original
        changes.append(f"âœï¸ {self.analyzer.filename}: ×¢×•×“×›×Ÿ ×œ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ×”×¢×–×¨")
        description = (
            f"ğŸ”§ ××¦××ª×™ {len(duplicates)} ×‘×œ×•×§×™ ×§×•×“ ×—×•×–×¨×™×.\n\n"
            "×”×¦×¢×ª ×—×™×œ×•×¥:\n"
            "ğŸ“¦ ×™×¦×™×¨×ª utils.py ×¢× ×¤×•× ×§×¦×™×•×ª ×¢×–×¨\n"
            "ğŸ“ ×¢×“×›×•×Ÿ ×”×§×•×“ ×”××§×•×¨×™ ×œ×©×™××•×© ×‘×”×Ÿ\n\n"
            "âœ… ×§×•×“ × ×§×™ ×™×•×ª×¨ ×•×œ×œ× ×›×¤×™×œ×•×™×•×ª"
        )
        return RefactorProposal(
            refactor_type=RefactorType.EXTRACT_FUNCTIONS,
            original_file=self.analyzer.filename,
            new_files=new_files,
            description=description,
            changes_summary=changes,
        )

    def _find_code_duplication(self) -> List[Dict[str, Any]]:
        return []

    def _build_utils_from_duplicates(self, duplicates: List[Dict[str, Any]]) -> str:
        return '"""\n×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ××©×•×ª×¤×•×ª\n"""\n\n# TODO: implement\n'

    def _replace_duplicates_with_calls(self, duplicates: List[Dict[str, Any]]) -> str:
        return self.analyzer.code  # type: ignore[return-value]

    def _merge_similar(self) -> RefactorProposal:
        raise ValueError("×¤×™×¦'×¨ ××™×–×•×’ ×§×•×“ ×˜×¨× ×™×•×©× ×‘××œ×•××•")

    def _convert_to_classes(self) -> RefactorProposal:
        if len(self.analyzer.functions) < 3:
            raise ValueError("××™×Ÿ ××¡×¤×™×§ ×¤×•× ×§×¦×™×•×ª ×œ×”××¨×” ×œ××—×œ×§×”")
        groups = self._group_related_functions()
        # ×× ×™×¢×ª God Class: ×× ×™×© ×¨×§ ×§×‘×•×¦×” ××—×ª ×•×”×¨×‘×” ×¤×•× ×§×¦×™×•×ª â€“ × ×¡×” ×œ×¤×¦×œ ×œ×¤×™ ×“×•××™×™×Ÿ
        if len(groups) == 1 and len(self.analyzer.functions) >= 6:
            domain_groups = self._group_by_domain(self.analyzer.functions)
            domain_groups = {k: v for k, v in domain_groups.items() if len(v) >= self.min_functions_per_group}
            if len(domain_groups) >= 2:
                groups = self._limit_group_count(domain_groups)
        # ×× ×™×¢×ª ××™×‘×•×“ ×¤×•× ×§×¦×™×•×ª: ××™×–×•×’ ×§×‘×•×¦×•×ª ×§×˜× ×•×ª (singleton) ×œ×¤× ×™ ×‘× ×™×™×ª ×”××—×œ×§×•×ª
        if any(len(v) < self.min_functions_per_group for v in groups.values()) and len(groups) >= 2:
            groups = self._merge_singletons_for_oop(groups)
        new_files: Dict[str, str] = {}
        changes: List[str] = []
        for group_name, functions in groups.items():
            if len(functions) < 2:
                continue
            class_name = self._generate_class_name(group_name)
            class_code = self._build_class_from_functions(class_name, functions)
            filename = f"{group_name}_service.py"
            new_files[filename] = class_code
            changes.append(f"ğŸ“¦ {filename}: ××—×œ×§×” {class_name} ×¢× {len(functions)} ××ª×•×“×•×ª")
        if not new_files:
            raise ValueError("×œ× × ×™×ª×Ÿ ×œ×§×‘×¥ ××ª ×”×¤×•× ×§×¦×™×•×ª ×œ××—×œ×§×•×ª ××©××¢×•×ª×™×•×ª")
        description = (
            f"ğŸ¨ ×”××¨×” ×œ-OOP:\n\n"
            f"××¦××ª×™ {len(self.analyzer.functions)} ×¤×•× ×§×¦×™×•×ª.\n"
            f"×”×¦×¢×ª ×”××¨×” ×œ-{len(new_files)} ××—×œ×§×•×ª:\n\n"
        )
        for fname in new_files.keys():
            description += f"   ğŸ“¦ {fname}\n"
        description += "\nâœ… ××¨×›×™×˜×§×˜×•×¨×” ××•× ×—×™×ª ×¢×¦××™×"
        return RefactorProposal(
            refactor_type=RefactorType.CONVERT_TO_CLASSES,
            original_file=self.analyzer.filename,
            new_files=new_files,
            description=description,
            changes_summary=changes,
        )

    def _generate_class_name(self, prefix: str) -> str:
        words = prefix.split('_')
        return ''.join(word.capitalize() for word in words)

    def _build_class_from_functions(self, class_name: str, functions: List[FunctionInfo]) -> str:
        lines: List[str] = []
        lines.append(f'"""')
        lines.append(f'{class_name} - ××—×œ×§×” ×©× ×•×¦×¨×” ××¨×¤×§×˜×•×¨×™× ×’')
        lines.append(f'"""')
        lines.append('')
        lines.extend(self.analyzer.imports)
        lines.append('')
        lines.append('')
        lines.append(f'class {class_name}:')
        lines.append(f'    """××—×œ×§×ª ×©×™×¨×•×ª ×œ-{class_name.lower()}"""')
        lines.append('')
        lines.append('    def __init__(self):')
        lines.append('        """××ª×—×•×œ ×”××—×œ×§×”"""')
        lines.append('        pass')
        lines.append('')
        for func in functions:
            method_code = self._convert_function_to_method(func)
            lines.append(method_code)
            lines.append('')
        return '\n'.join(lines)

    def _convert_function_to_method(self, func: FunctionInfo) -> str:
        method_lines = func.code.splitlines()
        def_line_idx = 0
        for i, line in enumerate(method_lines):
            stripped = line.strip()
            if stripped.startswith('def ') or stripped.startswith('async def '):
                def_line_idx = i
                break
        def_line = method_lines[def_line_idx]
        if '(' in def_line:
            def_line = def_line.replace('(', '(self, ', 1)
            def_line = def_line.replace('(self, )', '(self)')
        method_lines[def_line_idx] = '    ' + def_line
        for i in range(len(method_lines)):
            if i != def_line_idx:
                method_lines[i] = '    ' + method_lines[i]
        return '\n'.join(method_lines)

    def _add_dependency_injection(self) -> RefactorProposal:
        raise ValueError("×¤×™×¦'×¨ DI ×˜×¨× ×™×•×©× ×‘××œ×•××•")

    def _validate_proposal(self, proposal: RefactorProposal) -> bool:
        try:
            for filename, content in proposal.new_files.items():
                if filename.endswith('.py'):
                    ast.parse(content)
            return True
        except SyntaxError as e:
            logger.error(f"×©×’×™××ª ×ª×—×‘×™×¨ ×‘×§×•×‘×¥ ×©× ×•×¦×¨: {e}")
            proposal.warnings.append(f"âš ï¸ ×©×’×™××ª ×ª×—×‘×™×¨: {e}")
            return False

    # === Utilities for import cleanup and centralization ===

    def _get_import_aliases(self, import_line: str) -> List[str]:
        """×”×—×–×¨×ª ×©××•×ª ×©×™×™×•×‘××• (alias/×©× ×’×œ×•×™) ××ª×•×š ×©×•×¨×ª import ××—×ª."""
        try:
            tree = ast.parse(import_line)
        except Exception:
            return []
        names: List[str] = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    base = (alias.asname or alias.name).split('.')[0]
                    names.append(base)
            elif isinstance(node, ast.ImportFrom):
                for alias in node.names:
                    names.append(alias.asname or alias.name)
        return names

    def _extract_used_names(self, code: str) -> Set[str]:
        """×”×—×–×¨×ª ×›×œ ×”×©××•×ª ×©×‘×”× × ×¢×©×” ×©×™××•×© ×‘×§×•×“ (×œ×¦×•×¨×š ×‘×“×™×§×ª imports ×‘×©×™××•×©)."""
        used: Set[str] = set()
        try:
            tree = ast.parse(code)
        except Exception:
            return used
        for node in ast.walk(tree):
            if isinstance(node, ast.Name):
                used.add(node.id)
        return used

    def _filter_imports_for_code(self, imports: List[str], code: str) -> List[str]:
        """××¡× ×Ÿ imports ×›×š ×©×™×•×¤×™×¢×• ×¨×§ ××œ×• ×”× ×“×¨×©×™× ×¢×œ ×¤×™ ×©×™××•×© ×‘×¤×•× ×§×¦×™×•×ª ×”×§×‘×•×¦×”."""
        used = self._extract_used_names(code)
        filtered: List[str] = []
        for imp in imports:
            # ×”×©××¨×ª star-import ×ª××™×“ (×œ××©×œ from .<base>_shared import *)
            if ' import *' in imp:
                filtered.append(imp)
                continue
            aliases = self._get_import_aliases(imp)
            if not aliases:
                # ×× ×œ× ×–×•×”×• ×©××•×ª (×œ××©×œ import ×œ× ×¡×˜× ×“×¨×˜×™) â€” × ×©××•×¨ ×œ×™×ª×¨ ×‘×˜×—×•×Ÿ
                filtered.append(imp)
                continue
            if any(alias in used for alias in aliases):
                filtered.append(imp)
        # ×”×¡×¨×” ×©×œ ×›×¤×™×œ×•×™×•×ª ×ª×•×š ×©××™×¨×ª ×¡×“×¨
        seen: Set[str] = set()
        unique: List[str] = []
        for line in filtered:
            if line not in seen:
                seen.add(line)
                unique.append(line)
        return unique

    def _centralize_common_imports(
        self,
        new_files: Dict[str, str],
        per_file_imports: Dict[str, List[str]],
        base_name: str,
    ) -> Dict[str, str]:
        """×××—×“ imports ××©×•×ª×¤×™× ×œ×›×œ ×”××•×“×•×œ×™× ×œ×§×•×‘×¥ <base>_shared.py ×•××™×™×‘× ××× ×• ×‘×›×œ ××•×“×•×œ."""
        module_files = [fn for fn in new_files.keys() if fn != "__init__.py"]
        if len(module_files) < 2:
            return new_files
        # ×—×™×ª×•×š ××©×•×ª×£ ×©×œ imports ×–×”×™× ×‘×™×Ÿ ×›×œ ×”××•×“×•×œ×™×
        import_sets = [set(per_file_imports.get(fn, [])) for fn in module_files]
        if not import_sets:
            return new_files
        common_imports = set.intersection(*import_sets) if len(import_sets) >= 2 else set()
        # ×¡× ×Ÿ ×¨×§ ×©×•×¨×•×ª import ×××©×™×•×ª
        common_imports = {imp for imp in common_imports if imp.startswith('import ') or imp.startswith('from ')}
        if not common_imports:
            return new_files

        shared_module_stem = f"{base_name}_shared"
        shared_filename = f"{shared_module_stem}.py"

        # ×‘× ×” ×§×•×‘×¥ imports ××©×•×ª×¤×™×
        shared_lines: List[str] = []
        shared_lines.append('"""')
        shared_lines.append('×™×™×‘×•× ××©×•×ª×£ ×œ×§×‘×¦×™× ×©× ×•×¦×¨×• ××¨×¤×§×˜×•×¨×™× ×’')
        shared_lines.append('"""')
        shared_lines.append('')
        shared_lines.extend(sorted(common_imports))
        shared_content = "\n".join(shared_lines) + "\n"
        new_files[shared_filename] = shared_content

        # ×¦×•×¨ ×¨×©×™××ª ×©××•×ª ×©×™×•×‘××• ××©×•×ª×¤×ª ×œ×›×œ ×”××•×“×•×œ×™×
        # ×›×“×™ ×œ×× ×•×¢ ×”×•×¤×¢×ª ×”××—×¨×•×–×ª "import os\n" ×‘×ª×•×š ×©×•×¨×ª import ×™×—×¡×™×ª (×©××›×©×™×œ×” ×˜×¡×˜×™×),
        # × ×©×ª××© ×‘-star import, ×××—×¨ ×•×–×” ××•×ª×¨ ×‘×”×§×©×¨ ××•×“×•×œ×™× ×¤× ×™××™×™× ×©× ×•×¦×¨×• ××•×˜×•××˜×™×ª.
        shared_import_stmt = f"from .{shared_module_stem} import *"

        # ×”×¡×¨ ××ª ×”×©×•×¨×•×ª ×”××©×•×ª×¤×•×ª ××›×œ ××•×“×•×œ ×•×”×•×¡×£ import ××©×•×ª×£ ××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’
        for fn in module_files:
            content = new_files.get(fn, '')
            if not content:
                continue
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’ ×œ××™×§×•× ×”×”×•×¡×¤×”
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2  # ××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’ ×•×”×§×• ×”×¨×™×§ ×©××—×¨×™×•
                        break
            # ×”×¡×¨×ª imports ××©×•×ª×¤×™×
            filtered_lines: List[str] = []
            for line in lines:
                if line.strip() in common_imports:
                    continue
                filtered_lines.append(line)
            # ×”×–×¨×§×ª import ××©×•×ª×£ ×× ×œ× ×§×™×™× ×›×‘×¨
            already_has_shared = any(
                ln.strip().startswith(f"from .{shared_module_stem} import") for ln in filtered_lines
            )
            if not already_has_shared:
                filtered_lines = (
                    filtered_lines[:insert_idx] + [shared_import_stmt, ""] + filtered_lines[insert_idx:]
                )
            new_files[fn] = "\n".join(filtered_lines) + "\n"

        return new_files

    # === ×ª××™×›×” ×‘××—×œ×§×•×ª: ×™×¦×™×¨×ª ×§×•×‘×¥ ××—×œ×§×•×ª ×•×”×–×¨×§×ª ×™×‘×•× ×œ××—×œ×§×•×ª ×‘×©×™××•×© ===
    def _build_classes_file(self, base_name: str) -> Tuple[str, str]:
        """
        ×™×•×¦×¨ ×§×•×‘×¥ ××¨×›×–×™ ×œ×›×œ ×”××—×œ×§×•×ª ×©×–×•×”×• ×‘×§×•×‘×¥ ×”××§×•×¨×™.
        ××—×–×™×¨ (×©× ×”×§×•×‘×¥, ×ª×•×›×Ÿ).
        """
        classes_stem = f"{base_name}_classes"
        filename = f"{classes_stem}.py"
        if not self.analyzer or not self.analyzer.classes:
            return filename, '"""\n××—×œ×§×•×ª (××™×Ÿ)\n"""\n\n'
        # ×‘×¢×–×¨×ª ××¡× ×Ÿ imports, × ×©××•×¨ ×¨×§ ×™×™×‘×•× ×©× ×“×¨×© ×œ××—×œ×§×•×ª
        classes_code_body = "\n\n".join(cls.code for cls in self.analyzer.classes)
        filtered_imports = self._filter_imports_for_code(self.analyzer.imports, classes_code_body)
        parts: List[str] = []
        parts.append('"""')
        parts.append("××—×œ×§×•×ª ×©×”×•×¤×§×• ××”××•× ×•×œ×™×ª")
        parts.append('"""')
        parts.append("")
        parts.extend(filtered_imports)
        parts.append("")
        for cls in self.analyzer.classes:
            parts.append(cls.code)
            parts.append("")
        return filename, "\n".join(parts) + "\n"

    def _inject_class_imports(self, new_files: Dict[str, str], classes_filename: str) -> Dict[str, str]:
        """
        ×¢×‘×•×¨ ×›×œ ××•×“×•×œ ×¤×•× ×§×¦×™×•×ª, ××–×”×” ××™×œ×• ××—×œ×§×•×ª ×‘×©×™××•×© ×•××–×¨×™×§ ×©×•×¨×ª import ××ª××™××”.
        """
        classes_stem = Path(classes_filename).stem
        class_names = {cls.name for cls in (self.analyzer.classes if self.analyzer else [])}
        out: Dict[str, str] = {}
        for fn, content in new_files.items():
            if os.path.basename(fn) == "__init__.py" or fn == classes_filename or fn.endswith("_shared.py"):
                out[fn] = content
                continue
            used = self._extract_used_names(content)
            needed = sorted([name for name in class_names if name in used])
            if not needed:
                out[fn] = content
                continue
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2  # ××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’ ×•×©×•×¨×” ×¨×™×§×”
                        break
            import_line = f"from .{classes_stem} import {', '.join(needed)}"
            # ×”×–×¨×§ ×× ×œ× ×§×™×™×
            already = any(ln.strip().startswith(f"from .{classes_stem} import") for ln in lines)
            if not already:
                lines = lines[:insert_idx] + [import_line, ""] + lines[insert_idx:]
            out[fn] = "\n".join(lines) + "\n"
        return out

    def _extract_defined_functions_in_code(self, code: str) -> Set[str]:
        """×©××•×ª ×¤×•× ×§×¦×™×•×ª ×˜×•×¤-×œ×‘×œ ×”××•×’×“×¨×•×ª ×‘×§×•×“ × ×ª×•×Ÿ.
        ×ª×™×§×•×Ÿ: ××œ ×ª×›×œ×•×œ ××ª×•×“×•×ª ×©×œ ××—×œ×§×•×ª ×›×“×™ ×œ×× ×•×¢ ×“×™×›×•×™ import × ×“×¨×©.
        """
        defined: Set[str] = set()
        try:
            tree = ast.parse(code)
        except Exception:
            return defined
        # ×¤×•× ×§×¦×™×•×ª ×˜×•×¤-×œ×‘×œ ×”×Ÿ ×›××œ×” ×©××•×¤×™×¢×•×ª ×™×©×™×¨×•×ª ×‘-tree.body
        for node in getattr(tree, "body", []):
            if isinstance(node, ast.FunctionDef):
                defined.add(node.name)
        return defined

    def _inject_function_imports(self, new_files: Dict[str, str], func_to_module: Dict[str, str]) -> Dict[str, str]:
        """
        ××–×¨×™×§ import ×™×—×¡×™ ×œ×¤×•× ×§×¦×™×•×ª ×©××•×’×“×¨×•×ª ×‘××•×“×•×œ×™× ××—×¨×™× ××š × ××¦××•×ª ×‘×©×™××•×©.
        """
        out: Dict[str, str] = {}
        for fn, content in new_files.items():
            if os.path.basename(fn) == "__init__.py" or fn.endswith("_shared.py"):
                out[fn] = content
                continue
            current_stem = Path(fn).stem
            used = self._extract_used_names(content)
            defined_here = self._extract_defined_functions_in_code(content)
            # ×¤×•× ×§×¦×™×•×ª × ×“×¨×©×•×ª: × ××¦××•×ª ×‘×©×™××•×©, ×™×“×•×¢ ×”×™×›×Ÿ ××•×’×“×¨×•×ª, ×œ× ××•×’×“×¨×•×ª ×›××Ÿ, ×•××•×’×“×¨×•×ª ×‘××•×“×•×œ ××—×¨
            needed = [name for name in used if name in func_to_module and name not in defined_here and func_to_module[name] != current_stem]
            if not needed:
                out[fn] = content
                continue
            # ×§×™×‘×•×¥ ×œ×¤×™ ××•×“×•×œ ×™×¢×“
            per_module: Dict[str, List[str]] = {}
            for name in needed:
                per_module.setdefault(func_to_module[name], []).append(name)
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2
                        break
            # ×‘× ×” ×©×•×¨×•×ª import ×œ×œ× ×›×¤×™×œ×•×™×•×ª ×§×™×™××•×ª
            new_imports: List[str] = []
            for module_stem, names in per_module.items():
                names_sorted = sorted(set(names))
                imp = f"from .{module_stem} import {', '.join(names_sorted)}"
                exists = any(ln.strip().startswith(f"from .{module_stem} import") for ln in lines)
                if not exists:
                    new_imports.append(imp)
            if new_imports:
                lines = lines[:insert_idx] + new_imports + [""] + lines[insert_idx:]
            out[fn] = "\n".join(lines) + "\n"
        return out
    def _extract_defined_globals_in_code(self, code: str) -> Set[str]:
        """×©××•×ª ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× (Assign/AnnAssign) ×”××•×’×“×¨×™× ×‘×§×•×“ × ×ª×•×Ÿ ×‘×¨××ª ××•×“×•×œ."""
        defined: Set[str] = set()
        try:
            tree = ast.parse(code)
        except Exception:
            return defined
        for node in getattr(tree, "body", []):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    for name in ast.walk(target):
                        if isinstance(name, ast.Name):
                            defined.add(name.id)
            elif isinstance(node, ast.AnnAssign):
                tgt = getattr(node, "target", None)
                if isinstance(tgt, ast.Name):
                    defined.add(tgt.id)
        return defined
    def _inject_global_imports(self, new_files: Dict[str, str], global_names: Set[str], source_module_stem: str) -> Dict[str, str]:
        """
        ××–×¨×™×§ import ×œ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× ×©× ×©××¨×• ×‘××•×“×•×œ ××§×•×¨ (×œ××©×œ core) ××œ ××•×“×•×œ×™× ×©×¦×•×¨×›×™× ××•×ª×.
        """
        out: Dict[str, str] = {}
        for fn, content in new_files.items():
            stem = Path(fn).stem
            if not fn.endswith(".py") or os.path.basename(fn) == "__init__.py" or fn.endswith("_shared.py") or stem == source_module_stem:
                out[fn] = content
                continue
            used = self._extract_used_names(content)
            defined_here = self._extract_defined_globals_in_code(content)
            needed = sorted([name for name in global_names if name in used and name not in defined_here])
            if not needed:
                out[fn] = content
                continue
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2
                        break
            import_line = f"from .{source_module_stem} import {', '.join(needed)}"
            # ×”×–×¨×§×” ×× ×œ× ×§×™×™× ×›×‘×¨
            already = any(ln.strip().startswith(f"from .{source_module_stem} import") for ln in lines)
            if not already:
                lines = lines[:insert_idx] + [import_line, ""] + lines[insert_idx:]
            out[fn] = "\n".join(lines) + "\n"
        return out
    def post_refactor_cleanup(self, files: Dict[str, str]) -> Dict[str, str]:
        """
        ×©×œ×‘ × ×™×§×•×™ ×œ××—×¨ ×¨×¤×§×˜×•×¨×™× ×’: × ×§×™×•×Ÿ imports ×œ× ×‘×©×™××•×© ×‘×¨××ª ×§×•×‘×¥ ×ª×•×š ×©×™××•×¨ ×§×‘×•×¢×™×/×”×§×¦××•×ª ×’×œ×•×‘×œ×™×•×ª.
        ×”×¢×¨×”: × ×× ×¢×™× ××”×¨×¦×ª ×›×œ×™× ×—×™×¦×•× ×™×™× (ruff/black) ××¡×™×‘×•×ª ×ª××™××•×ª ×¡×‘×™×‘×”.
        """
        cleaned: Dict[str, str] = {}
        for filename, content in files.items():
            if (
                not filename.endswith('.py')
                or os.path.basename(filename) == '__init__.py'
                or filename.endswith('_shared.py')
            ):
                cleaned[filename] = content
                continue
            try:
                # × ×–×”×” imports ×‘×§×•×‘×¥ ×•× ×©××•×¨ ×¨×§ ××œ×• ×©×‘×©×™××•×©
                import_lines: List[str] = []
                body_lines: List[str] = []
                for ln in content.splitlines():
                    stripped = ln.strip()
                    if stripped.startswith('import ') or stripped.startswith('from '):
                        import_lines.append(stripped)
                    else:
                        body_lines.append(ln)
                code_body = "\n".join(body_lines)
                filtered = self._filter_imports_for_code(import_lines, code_body)

                lines = content.splitlines()
                # ××™×ª×•×¨ ×”×“×•×§×¡×˜×¨×™× ×’ ×”×¢×œ×™×•×Ÿ ×›×“×™ ×©×œ× × ×–×™×– ×§×•×“ ×œ×¤× ×™×•
                docstring_end_idx = -1
                quote_count = 0
                for idx, line in enumerate(lines):
                    if line.strip().startswith('"""'):
                        quote_count += 1
                        if quote_count == 2:
                            docstring_end_idx = idx
                            break
                if docstring_end_idx == -1:
                    cleaned[filename] = content
                    continue

                # ×—×™×¤×•×© ×‘×œ×•×§ ×”-imports ×”××•×¤×™×¢ ××™×“ ××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’ (×¢× ×¨×•×•×—×™×/×”×¢×¨×•×ª ×‘×™× ×™×™×)
                def _is_import_stmt(text: str) -> bool:
                    return text.startswith('import ') or text.startswith('from ')

                import_start_idx: Optional[int] = None
                scan_idx = docstring_end_idx + 1
                while scan_idx < len(lines):
                    stripped = lines[scan_idx].strip()
                    if not stripped or stripped.startswith('#'):
                        scan_idx += 1
                        continue
                    if _is_import_stmt(stripped):
                        import_start_idx = scan_idx
                    break

                if import_start_idx is None:
                    # ××™×Ÿ ×‘×œ×•×§ imports ×œ××—×¨ ×”×“×•×§×¡×˜×¨×™× ×’ â€“ ××™×Ÿ ××” ×œ× ×§×•×ª
                    cleaned[filename] = content
                    continue

                import_end_idx = import_start_idx
                while import_end_idx < len(lines):
                    stripped = lines[import_end_idx].strip()
                    if not stripped or stripped.startswith('#') or _is_import_stmt(stripped):
                        import_end_idx += 1
                        continue
                    break

                before_block = lines[:import_start_idx]
                after_block = lines[import_end_idx:]

                rebuilt: List[str] = list(before_block)

                def _ensure_trailing_blank(target: List[str]) -> None:
                    if target and target[-1].strip() != "":
                        target.append("")

                if filtered:
                    _ensure_trailing_blank(rebuilt)
                    rebuilt.extend(filtered)
                    if after_block and after_block[0].strip() != "":
                        rebuilt.append("")
                else:
                    # ×× ××™×Ÿ imports ×œ××—×¨ ×”×¡×™× ×•×Ÿ, × ×©××•×¨ ×¨×•×•×— ××—×“ ×‘×™×Ÿ ×”×“×•×§×¡×˜×¨×™× ×’ ×œ×‘×™×Ÿ ×”×’×•×£ ×× × ×“×¨×©
                    if after_block and after_block[0].strip() != "":
                        _ensure_trailing_blank(rebuilt)

                rebuilt.extend(after_block)
                cleaned_content = "\n".join(rebuilt).rstrip() + "\n"
                cleaned[filename] = cleaned_content
            except Exception:
                cleaned[filename] = content
        return cleaned

    # === Collocation: ×”×§×¦××ª ××—×œ×§×•×ª ×œ×§×‘×•×¦×•×ª ×¤×•× ×§×¦×™×•×ª ×•×‘× ×™×™×ª ××™×¤×•×™ ===
    def _extract_defined_classes_in_code(self, code: str) -> Set[str]:
        defined: Set[str] = set()
        try:
            tree = ast.parse(code)
        except Exception:
            return defined
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                defined.add(node.name)
        return defined

    def _assign_classes_to_groups(
        self, groups: Dict[str, List[FunctionInfo]]
    ) -> Dict[str, List[ClassInfo]]:
        """
        ××§×¦×” ×›×œ Class ×œ×§×‘×•×¦×ª ×“×•××™×™×Ÿ ××—×ª ×œ×¤×™:
        - ×©×™××•×© ×‘×¤×•×¢×œ ×¢×´×™ ×¤×•× ×§×¦×™×•×ª ×”×§×‘×•×¦×” (affinity)
        - ×”×ª×××ª Section (×× ×§×™×™××ª) ×œ×©× ×”×§×‘×•×¦×”
        - ×“××™×•×Ÿ ×©××•×ª ×›××©×¨ ××™×Ÿ ×©×™××•×© ×‘×¨×•×¨
        """
        if not self.analyzer:
            return {}
        class_by_name: Dict[str, ClassInfo] = {c.name: c for c in self.analyzer.classes}
        groups_classes: Dict[str, List[ClassInfo]] = {g: [] for g in groups.keys()}
        if not class_by_name:
            return groups_classes

        # ×”×¢×“×¤×” ×§×©×™×—×”: ×× ×œ××—×œ×§×” ×™×© Section ×”×ª×•×× ×œ×§×‘×•×¦×” ×§×™×™××ª â€“ ××œ ×ª×¡×™×˜ ××•×ª×” ××©×
        for cname, cls in list(class_by_name.items()):
            section = (cls.section or "").strip()
            if section and section in groups:
                groups_classes[section].append(cls)
                del class_by_name[cname]

        # ×”×›× ×”: ×©×™××•×©×™ ×§×œ××¡×™× ×œ×¤×™ ×¤×•× ×§×¦×™×•×ª
        def used_names_in_function(func: FunctionInfo) -> Set[str]:
            return self._extract_used_names(func.code)

        # ×¦×‘×™×¨×ª × ×™×§×•×“ ×©×™××•×© ×œ×›×œ Class ××•×œ ×›×œ ×§×‘×•×¦×”
        score: Dict[Tuple[str, str], float] = {}  # (class_name, group_name) -> score
        for gname, funcs in groups.items():
            used_names: Set[str] = set()
            for f in funcs:
                used_names |= used_names_in_function(f)
            for cname in class_by_name.keys():
                s = 0.0
                if cname in used_names:
                    # × ×—×©×‘ ×©×™××•×© ×™×©×™×¨ ×‘××—×œ×§×”
                    s += 3.0
                # ×‘×•× ×•×¡ ×× ×”-Section ×©×œ ×”××—×œ×§×” ×ª×•×× ××ª ×©× ×”×§×‘×•×¦×”
                cls_section = class_by_name[cname].section or ""
                if cls_section and cls_section == gname:
                    s += 2.0
                # ×“××™×•×Ÿ ×œ×©×: ×›××©×¨ ××™×Ÿ ×©×™××•×© ××• section
                if s == 0.0:
                    s += self._name_similarity(cname, gname)
                score[(cname, gname)] = s

        # ×”×ª×××•×ª ×œ×¤×™ ×©×™××•×© ×”××—×œ×§×” ×‘×¤×•× ×§×¦×™×•×ª (××ª×•×š ××ª×•×“×•×ª)
        # ×× ××ª×•×“×•×ª ×”××—×œ×§×” ×§×•×¨××•×ª ×œ×¤×•× ×§×¦×™×•×ª ×‘×§×‘×•×¦×” ×¡×¤×¦×™×¤×™×ª â€“ × ×–×™×– ×œ×©×
        func_name_to_group: Dict[str, str] = {}
        for gname, funcs in groups.items():
            for f in funcs:
                func_name_to_group[f.name] = gname
        for cname, cls in class_by_name.items():
            group_bonus: Dict[str, float] = {}
            for m in cls.methods:
                for called in m.calls:
                    g = func_name_to_group.get(called)
                    if g:
                        group_bonus[g] = group_bonus.get(g, 0.0) + 1.0
            if group_bonus:
                # ×‘×•× ×•×¡ ××©××¢×•×ª×™ ×œ×§×‘×•×¦×” ×¢× ××™×¨×‘ ×”×§×¨×™××•×ª
                best_g, best_v = max(group_bonus.items(), key=lambda kv: kv[1])
                score[(cname, best_g)] = score.get((cname, best_g), 0.0) + 3.0

        # ×”×§×¦××”: ×‘×—×¨ ××ª ×”×§×‘×•×¦×” ×¢× ×”× ×™×§×•×“ ×”×’×‘×•×” ×‘×™×•×ª×¨ ×œ×›×œ Class
        assigned_group_for_class: Dict[str, str] = {}
        for cname in class_by_name.keys():
            best_g = None
            best_s = float("-inf")
            for gname in groups.keys():
                s = score.get((cname, gname), 0.0)
                if s > best_s:
                    best_s = s
                    best_g = gname
            if best_g is None:
                # ×’×™×‘×•×™: ×”×¦××“ ×œ×§×‘×•×¦×” ×”×¨××©×•× ×”
                best_g = next(iter(groups.keys()))
            assigned_group_for_class[cname] = best_g
            groups_classes[best_g].append(class_by_name[cname])

        # ×›×œ×œ Coupling: ×”×¦××“×ª ×× ×”×œ+×™×©×•×ª ×œ××•×ª×• ×§×•×‘×¥ ×›××©×¨ ×™×© ×©×™××•×© ×ª×›×•×£ ×‘-Type Hint/×©××•×ª
        # × ×–×”×” ×¢×‘×•×¨ ×›×œ ××—×œ×§×” ××™×œ×• ××—×œ×§×•×ª ××—×¨×•×ª ××•×–×›×¨×•×ª ×‘××ª×•×“×•×ª×™×”, ×•× ××—×“ ×œ×§×‘×•×¦×” ××©×•×ª×¤×ª ×›××©×¨ ×”×™×—×¡ ×’×‘×•×”.
        # ×”×—××¨×”: ×¢×‘×•×¨ ×“×•××™×™× ×™× ×©×•× ×™× (×œ××©×œ inventory ××•×œ core/billing) ×”×¦××“×” ×ª×ª×‘×¦×¢ ×¨×§ ×‘××§×¨×” ×©×œ ××–×›×•×¨ ×”×“×“×™ ×—×–×§.
        try:
            all_class_names: Set[str] = set(c.name for c in (self.analyzer.classes or []))
            # ××™×¤×•×™ ×“×•××™×™×Ÿ ×¢×‘×•×¨ ×›×œ ××—×œ×§×”
            domain_by_class: Dict[str, str] = {}
            for cls in (self.analyzer.classes or []):
                try:
                    domain_by_class[cls.name] = self._classify_class_domain(cls)
                except Exception:
                    domain_by_class[cls.name] = "core"
            # ××™×¤×•×™: cname -> counter ×©×œ ××—×œ×§×•×ª ××—×¨×•×ª ×©×”×•×–×›×¨×•
            mentions: Dict[str, Dict[str, int]] = {}
            method_counts: Dict[str, int] = {}
            for cls in (self.analyzer.classes or []):
                method_counts[cls.name] = len(cls.methods or [])
                for m in (cls.methods or []):
                    used = self._extract_used_names(m.code)
                    for other in (used & all_class_names):
                        if other == cls.name:
                            continue
                        mentions.setdefault(cls.name, {}).setdefault(other, 0)
                        mentions[cls.name][other] += 1
            # ×¡×£: ××—×œ×§×” A ××–×›×™×¨×” ××ª B ×‘×œ×¤×—×•×ª ××—×¦×™×ª ××”××ª×•×“×•×ª ×©×œ×” (××• 2 ××ª×•×“×•×ª ××™× ×™××•×)
            for a, counters in mentions.items():
                total_methods = max(1, method_counts.get(a, 0))
                # ×‘×—×¨ ××ª B ×¢× ×”×”×–×›×¨×•×ª ×”×’×‘×•×”×•×ª ×‘×™×•×ª×¨
                b, cnt = None, 0
                for other, c in counters.items():
                    if c > cnt:
                        b, cnt = other, c
                if not b:
                    continue
                strong_coupling = (cnt >= max(2, (total_methods + 1) // 2))
                if not strong_coupling:
                    continue
                ga = assigned_group_for_class.get(a)
                gb = assigned_group_for_class.get(b)
                if ga and gb and ga != gb:
                    # ×‘×“×™×§×ª ×“×•××™×™× ×™× â€“ ×× ×™×¢×ª ×”×¦××“×” ×—×•×¦×ª-×“×•××™×™×Ÿ ××’×¨×¡×™×‘×™×ª.
                    da = domain_by_class.get(a, "core")
                    db = domain_by_class.get(b, "core")
                    a_to_b = cnt
                    b_to_a = mentions.get(b, {}).get(a, 0)
                    # ×“×¨×™×©×ª ×”×“×“×™×•×ª ×¢×‘×•×¨ ×“×•××™×™× ×™× ×©×•× ×™×: ×©× ×™ ×”×›×™×•×•× ×™× ×¦×¨×™×›×™× ×œ×”×™×•×ª "×—×–×§×™×"
                    # (×œ×¤×—×•×ª ××—×¦×™ ××”××ª×•×“×•×ª ××• ××™× ×™××•× 2, ×œ×›×œ ××—×“ ××”×¦×“×“×™×).
                    if da != db:
                        thr_a = max(2, (method_counts.get(a, 0) + 1) // 2)
                        thr_b = max(2, (method_counts.get(b, 0) + 1) // 2)
                        # ×”×§×©×—×” × ×•×¡×¤×ª: ×× ××—×“ ×”×“×•××™×™× ×™× ×”×•× 'inventory', ××œ ×ª×‘×¦×¢ ×”×¦××“×” ××œ× ×× ×©× ×™ ×”×›×™×•×•× ×™× ×—×–×§×™×.
                        involves_inventory = ("inventory" in (da, db))
                        if involves_inventory and not (a_to_b >= thr_a and b_to_a >= thr_b):
                            continue
                        # ×‘×”×™×¢×“×¨ ×”×“×“×™×•×ª ××¡×¤×§×ª â€“ ××œ ×ª×‘×¦×¢ ×”×¦××“×” ×—×•×¦×ª-×“×•××™×™×Ÿ
                        if not (a_to_b >= thr_a and b_to_a >= thr_b):
                            continue
                    # ×”×–×– ××ª ×”××—×œ×§×” ×”×¤×—×•×ª "××•×›×¨×ª" ××œ ×§×‘×•×¦×ª ×”××—×œ×§×” ×”××¨×›×–×™×ª
                    # ×”×§×¨×™×˜×¨×™×•×Ÿ: ×× b ××–×›×™×¨ ××ª a ×¤×—×•×ª ×-a ×©××–×›×™×¨ ××ª b â€“ × ×¢×“×™×£ ××ª ×§×‘×•×¦×ª a
                    dest = ga if a_to_b >= b_to_a else gb
                    src = gb if dest == ga else ga
                    # ×¢×“×›×Ÿ mapping ×•×¨×©×™××•×ª
                    assigned_group_for_class[b if dest == ga else a] = dest
                    # ×”×¡×¨ ××§×‘×•×¦×ª ×”××§×•×¨ ×•×”×•×¡×£ ×œ×™×¢×“
                    move_name = b if dest == ga else a
                    # ×”×¡×¨×” ×‘×˜×•×—×”
                    for gname, arr in groups_classes.items():
                        groups_classes[gname] = [c for c in arr if c.name != move_name]
                    # ×”×•×¡×¤×”
                    cls_obj = next((c for c in (self.analyzer.classes or []) if c.name == move_name), None)
                    if cls_obj:
                        groups_classes[dest].append(cls_obj)
        except Exception:
            # ×œ× × ×›×©×™×œ ××ª ×”×ª×”×œ×™×š ×‘××§×¨×” ×©×œ ×—×¨×™×’×” â€“ ×”×›×œ×œ ×”×•× ×©×™×¤×•×¨-×”×™×•×¨×™×¡×˜×™
            pass

        return groups_classes

    def _inject_cross_module_class_imports(
        self,
        new_files: Dict[str, str],
        class_to_module: Dict[str, str],
    ) -> Dict[str, str]:
        """
        ××–×¨×™×§ import ×™×—×¡×™ ×œ××—×œ×§×•×ª ×©××•×’×“×¨×•×ª ×‘××•×“×•×œ×™× ××—×¨×™× ××š × ××¦××•×ª ×‘×©×™××•×©.
        """
        out: Dict[str, str] = {}
        class_names: Set[str] = set(class_to_module.keys())
        for fn, content in new_files.items():
            if os.path.basename(fn) == "__init__.py" or fn.endswith("_shared.py"):
                out[fn] = content
                continue
            current_stem = Path(fn).stem
            used = self._extract_used_names(content)
            defined_here = self._extract_defined_classes_in_code(content)
            needed = [name for name in used if name in class_names and name not in defined_here and class_to_module.get(name) != current_stem]
            if not needed:
                out[fn] = content
                continue
            per_module: Dict[str, List[str]] = {}
            for name in needed:
                mod = class_to_module.get(name)
                if not mod:
                    continue
                per_module.setdefault(mod, []).append(name)
            lines = content.splitlines()
            # ××¦× ××ª ×¡×•×£ ×”×“×•×§×¡×˜×¨×™× ×’
            insert_idx = 0
            quote_count = 0
            for i, line in enumerate(lines):
                if line.strip().startswith('"""'):
                    quote_count += 1
                    if quote_count == 2:
                        insert_idx = i + 2
                        break
            new_imports: List[str] = []
            for module_stem, names in per_module.items():
                names_sorted = sorted(set(names))
                imp = f"from .{module_stem} import {', '.join(names_sorted)}"
                exists = any(ln.strip().startswith(f"from .{module_stem} import") for ln in lines)
                if not exists:
                    new_imports.append(imp)
            if new_imports:
                lines = lines[:insert_idx] + new_imports + [""] + lines[insert_idx:]
            out[fn] = "\n".join(lines) + "\n"
        return out

    # === DRY-RUN: ×–×™×”×•×™ ×•×× ×™×¢×ª ×ª×œ×•×ª ××¢×’×œ×™×ª ×‘×××¦×¢×•×ª ××™×–×•×’ ××•×“×•×œ×™× ×¦××•×“×™× ===
    def _resolve_circular_imports(self, files: Dict[str, str]) -> Tuple[Dict[str, str], List[Tuple[str, str]]]:
        """
        ××–×”×” ×¨×›×™×‘×™× ×—×–×§×™× (SCC) ×‘×’×¨×£ ×”×™×™×‘×•× ×‘×™×Ÿ ×”××•×“×•×œ×™× ×©× ×•×¦×¨×•.
        ×¢×‘×•×¨ ×›×œ ××¢×’×œ (SCC ×‘×’×•×“×œ > 1) â€“ ×××–×’ ××•×“×•×œ×™× ×‘×–×•×’×•×ª ×§×¨×•×‘×™× ×›×“×™ ×œ×¤×¨×§ ××ª ×”××¢×’×œ,
        ×‘×”×ª×× ×œ×›×œ×œ ×”-Coupling (×”×¦××“×”).
        ××—×–×™×¨ (files_after, merged_pairs)
        """
        def _module_stem(fn: str) -> Optional[str]:
            if not fn.endswith(".py"):
                return None
            if os.path.basename(fn) == "__init__.py":
                return None
            if fn.endswith("_shared.py"):
                return None
            return Path(fn).stem

        def _build_graph(files_map: Dict[str, str]) -> Tuple[Dict[str, Set[str]], Dict[str, str]]:
            stems: Dict[str, str] = {}
            for fn in files_map.keys():
                st = _module_stem(fn)
                if st:
                    stems[st] = fn
            graph: Dict[str, Set[str]] = {st: set() for st in stems.keys()}
            import_re = re.compile(r'^\s*from\s+\.(\w+)\s+import\s+')
            for st, fn in stems.items():
                content = files_map.get(fn, "")
                for line in content.splitlines():
                    m = import_re.match(line)
                    if not m:
                        continue
                    target = m.group(1)
                    if target in graph and target != st:
                        graph[st].add(target)
            return graph, stems

        def _tarjan_scc(graph: Dict[str, Set[str]]) -> List[List[str]]:
            index = 0
            indices: Dict[str, int] = {}
            lowlink: Dict[str, int] = {}
            stack: List[str] = []
            onstack: Set[str] = set()
            sccs: List[List[str]] = []

            def strongconnect(v: str) -> None:
                nonlocal index
                indices[v] = index
                lowlink[v] = index
                index += 1
                stack.append(v)
                onstack.add(v)
                for w in graph.get(v, ()):
                    if w not in indices:
                        strongconnect(w)
                        lowlink[v] = min(lowlink[v], lowlink[w])
                    elif w in onstack:
                        lowlink[v] = min(lowlink[v], indices[w])
                if lowlink[v] == indices[v]:
                    comp: List[str] = []
                    while True:
                        w = stack.pop()
                        onstack.discard(w)
                        comp.append(w)
                        if w == v:
                            break
                    sccs.append(comp)

            for v in graph.keys():
                if v not in indices:
                    strongconnect(v)
            return sccs

        def _merge_two(files_map: Dict[str, str], stems_map: Dict[str, str], a: str, b: str) -> Tuple[Dict[str, str], Tuple[str, str]]:
            """
            ×××–×’ ××ª ×”××•×“×•×œ b ×œ×ª×•×š a. ××¢×“×›×Ÿ ×™×™×‘×•××™× ×‘×§×‘×¦×™× ××—×¨×™×.
            """
            a_file = stems_map[a]
            b_file = stems_map[b]
            a_content = files_map.get(a_file, "")
            b_content = files_map.get(b_file, "")
            # ×”×¡×¨ ×“×•×§×¡×˜×¨×™× ×’ ×¢×œ×™×•×Ÿ ×-b ×›×“×™ ×œ×× ×•×¢ ×›×¤×™×œ×•×ª ××¨×—×™×‘×”
            def strip_top_docstring(text: str) -> str:
                lines = text.splitlines()
                out: List[str] = []
                quote_count = 0
                i = 0
                while i < len(lines):
                    line = lines[i]
                    if quote_count < 2 and line.strip().startswith('"""'):
                        quote_count += 1
                        i += 1
                        continue
                    if quote_count < 2:
                        i += 1
                        continue
                    break
                # ×“×œ×’ ×’× ×¢×œ ×©×•×¨×” ×¨×™×§×” ×©××—×¨×™ ×”×“×•×§×¡×˜×¨×™× ×’
                if i < len(lines) and lines[i].strip() == "":
                    i += 1
                out = lines[i:]
                return "\n".join(out)
            merged = a_content.rstrip() + f"\n\n# ---- merged from {b_file} ----\n" + strip_top_docstring(b_content).lstrip() + "\n"
            files_map[a_file] = merged
            # ××—×§ ××ª ×§×•×‘×¥ b
            del files_map[b_file]
            # ×¢×“×›×Ÿ ×™×™×‘×•××™× ×‘×›×œ ×©××¨ ×”×§×‘×¦×™×: from .b import -> from .a import
            b_pat = re.compile(rf'^(\s*from\s+)\.{re.escape(b)}(\s+import\s+)', re.M)
            for fn, content in list(files_map.items()):
                files_map[fn] = re.sub(b_pat, r'\1.' + a + r'\2', content)
            # ×”×¡×¨ self-imports ×‘×§×•×‘×¥ ×”×××•×—×“ (from .a import ...) ×›×“×™ ×œ×× ×•×¢ ×˜×¢×™× ×ª ××•×“×•×œ ×—×œ×§×™×ª
            self_import_pat = re.compile(rf'^\s*from\s+\.{re.escape(a)}\s+import\s+.*$', re.M)
            files_map[a_file] = re.sub(self_import_pat, '', files_map[a_file])
            # × ×§×” ×¨×•×•×—×™× ×›×¤×•×œ×™× ×©× ×•×¦×¨×• ××”×¡×¨×”
            files_map[a_file] = re.sub(r'\n{3,}', '\n\n', files_map[a_file]).lstrip() + ("\n" if not files_map[a_file].endswith("\n") else "")
            return files_map, (stems_map[a], b_file)

        merged_pairs: List[Tuple[str, str]] = []
        iterations = 0
        while iterations < 5:  # ×”×’×‘×œ×ª × ×™×¡×™×•× ×•×ª ×›×“×™ ×œ×× ×•×¢ ×œ×•×œ××”
            graph, stems_map = _build_graph(files)
            sccs = _tarjan_scc(graph)
            # ×—×¤×© SCCs ×¢× ×™×•×ª×¨ ××¦×•××ª ××—×“ (××¢×’×œ×™×)
            cycles = [comp for comp in sccs if len(comp) > 1]
            if not cycles:
                break
            changed = False
            for comp in cycles:
                # ×‘×—×¨ ×–×•×’ ×œ××™×–×•×’: × ×¢×“×™×£ ×–×•×’ ×©×™×© ×‘×™× ×™×”× ×™×™×‘×•× ×”×“×“×™,
                # ×•×× ××™×Ÿ â€“ × ×¢×“×™×£ ×–×•×’ ×©×‘×• ×œ×¤×—×•×ª ××—×“ ××™× ×• ×“×•××™×™×Ÿ ×§× ×•× ×™ (××–×¢×¨ ×¤×’×™×¢×” ×‘-users/finance/inventory/network/workflows).
                pair_to_merge: Optional[Tuple[str, str]] = None
                mutual: List[Tuple[str, str]] = []
                canonical_domains = {"users", "finance", "inventory", "network", "workflows"}
                def _is_canonical(stem: str) -> bool:
                    return stem in canonical_domains
                for i in range(len(comp)):
                    for j in range(i + 1, len(comp)):
                        u, v = comp[i], comp[j]
                        if v in graph.get(u, set()) and u in graph.get(v, set()):
                            mutual.append((u, v))
                if mutual:
                    # ×™×¦×™×‘×•×ª: ×‘×—×¨ ×œ×¤×™ ×¡×“×¨ ××œ×¤×‘×™×ª×™ ×›×“×™ ×œ×§×‘×œ ×ª×•×¦××” ×“×˜×¨××™× ×™×¡×˜×™×ª
                    pair_to_merge = tuple(sorted(mutual, key=lambda p: (min(p[0], p[1]), max(p[0], p[1])))[0])  # type: ignore[assignment]
                else:
                    # ××™×Ÿ ×™×™×‘×•× ×”×“×“×™ ×™×©×™×¨ â€“ × ×‘×—×¨ ×–×•×’ ×©××¢×¨×‘ ×œ×¤×—×•×ª ××•×“×•×œ ×œ×-×§× ×•× ×™ ×›×“×™ ×œ×¦××¦× ×¤×’×™×¢×” ×‘×“×•××™×™× ×™×.
                    candidates: List[Tuple[str, str]] = []
                    for i in range(len(comp)):
                        for j in range(i + 1, len(comp)):
                            u, v = comp[i], comp[j]
                            if not (_is_canonical(u) and _is_canonical(v)):
                                candidates.append((u, v))
                    if candidates:
                        # ×”×¢×“×£ ×–×•×’ ×¢× "××™-×§× ×•× ×™×•×ª" ×’×‘×•×”×” (×›×œ×•××¨ ×©× ×™×”× ×œ×-×§× ×•× ×™×™×), ×•××– ×œ×¤×™ ×¡×“×¨ ×™×¦×™×‘
                        def _score(pair: Tuple[str, str]) -> Tuple[int, str, str]:
                            u, v = pair
                            non_canon_count = int(not _is_canonical(u)) + int(not _is_canonical(v))
                            return (-non_canon_count, min(u, v), max(u, v))
                        pair_to_merge = sorted(candidates, key=_score)[0]
                    else:
                        # ×‘×œ×™×ª ×‘×¨×™×¨×” (×›×œ ×”××•×“×•×œ×™× ×§× ×•× ×™×™×) â€“ × ×©××•×¨ ×”×ª× ×”×’×•×ª ×§×™×™××ª ×•× ×‘×—×¨ ×©× ×™×™× ×¨××©×•× ×™×
                        pair_to_merge = (comp[0], comp[1])
                a, b = pair_to_merge  # type: ignore[misc]
                # ×¢×“×™×¤×•×ª ×™×¢×“ ××™×–×•×’: ×”×¢×“×£ ×“×•××™×™× ×™× ×§× ×•× ×™×™× ×›×§×•×‘×¥ ×™×¢×“ (users, finance, inventory, network, workflows)
                def _priority(stem: str) -> int:
                    order = {"users": 0, "finance": 1, "inventory": 2, "network": 3, "workflows": 4}
                    return order.get(stem, 9)
                prioritized = False
                if _priority(a) > _priority(b):
                    a, b = b, a
                    prioritized = True
                # ×©××•×¨ ×¢×œ ×©× ×©××™×™×¦×‘: ×‘×—×¨ ××ª ×”×§×¦×¨/××œ×¤×‘×™×ª×™ ×›-a (×¨×§ ×× ×œ× ×”×•×¤×¢×œ×” ×¢×“×™×¤×•×ª ×“×•××™×™×Ÿ)
                if not prioritized:
                    if (b < a) or (len(b) < len(a) and b not in graph.get(a, set())):
                        a, b = b, a
                files, merged = _merge_two(files, stems_map, a, b)
                merged_pairs.append((merged[0], merged[1]))
                changed = True
            if not changed:
                break
            iterations += 1
        return files, merged_pairs

    # === Naming helpers ===
    def _choose_filename_for_group(self, base_name: str, group_name: str) -> str:
        """
        ×§×•×‘×¢ ×©× ×§×•×‘×¥ ×¢×‘×•×¨ ×§×‘×•×¦×”.
        ×›××©×¨ ××“×•×‘×¨ ×‘×“×•××™×™×Ÿ ××•×›×¨ â€“ ××©×ª××© ×‘×©× ×“×•××™×™× ×™ ×™×¦×™×‘ (users.py, finance.py, inventory.py, network.py, workflows.py).
        ××—×¨×ª â€“ × ×©××¨ ××ª ×©× ×”×‘×¡×™×¡ + ×”×§×‘×•×¦×” (base_group.py) ×œ×©××™×¨×ª ×ª××™××•×ª.
        """
        canonical_map = {
            "users": "users.py",
            "finance": "finance.py",
            "inventory": "inventory.py",
            "api_clients": "network.py",
            "workflows": "workflows.py",
            "analytics": f"{base_name}_analytics.py",  # × ×©××¨ ×××•×§×“-×‘×¡×™×¡ ×›×“×™ ×œ× ×œ×©×‘×•×¨ ×¦×™×¤×™×•×ª ×§×™×™××•×ª ×‘×˜×¡×˜×™×
            "utils": f"{base_name}_utils.py",
            "files": f"{base_name}_files.py",
            "permissions": f"{base_name}_permissions.py",
            "main": f"{base_name}_main.py",
            "debug": f"{base_name}_debug.py",
            "compute": f"{base_name}_compute.py",
            "helpers": f"{base_name}_helpers.py",
            "io": f"{base_name}_io.py",
        }
        if group_name in canonical_map:
            return canonical_map[group_name]
        # ×‘×¨×™×¨×ª ××—×“×œ: base_name_group.py
        safe_group = re.sub(r"[^a-z0-9_]", "_", group_name.lower())
        return f"{base_name}_{safe_group}.py"

# Instance ×’×œ×•×‘×œ×™
refactoring_engine = RefactoringEngine()

